% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames*,x11names*}{xcolor}
%
\documentclass[
]{krantz}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Data Science: A First Introduction},
  pdfauthor={Tiffany Timbers, Trevor Campbell and Melissa Lee},
  colorlinks=true,
  linkcolor=Maroon,
  filecolor=Maroon,
  citecolor=Blue,
  urlcolor=Blue,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.33,0.33,0.33}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.61,0.61,0.61}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.14,0.14,0.14}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.06,0.06,0.06}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.27,0.27,0.27}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.43,0.43,0.43}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0,0,0}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.5,0.5,0.5}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[bf,singlelinecheck=off]{caption}
\usepackage[scale=.8]{sourcecodepro}

\usepackage{framed,color}
\definecolor{shadecolor}{RGB}{248,248,248}

\renewcommand{\textfraction}{0.05}
\renewcommand{\topfraction}{0.8}
\renewcommand{\bottomfraction}{0.8}
\renewcommand{\floatpagefraction}{0.75}

\renewenvironment{quote}{\begin{VF}}{\end{VF}}
\let\oldhref\href
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}

\makeatletter
\newenvironment{kframe}{%
\medskip{}
\setlength{\fboxsep}{.8em}
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\renewenvironment{Shaded}{\begin{kframe}}{\end{kframe}}

\usepackage{makeidx}
\makeindex

\urlstyle{tt}

\usepackage{amsthm}
\makeatletter
\def\thm@space@setup{%
  \thm@preskip=8pt plus 2pt minus 4pt
  \thm@postskip=\thm@preskip
}
\makeatother

\frontmatter
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Data Science: A First Introduction}
\author{Tiffany Timbers, Trevor Campbell and Melissa Lee}
\date{2020-12-16}

\begin{document}
\maketitle

% you may need to leave a few empty pages before the dedication page

%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage\thispagestyle{empty}\null
%\cleardoublepage\newpage
\thispagestyle{empty}

\begin{center}
Dedication to be completed ...
%\includegraphics{images/dedication.pdf}
\end{center}

\setlength{\abovedisplayskip}{-5pt}
\setlength{\abovedisplayshortskip}{-5pt}

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoftables
\listoffigures
\hypertarget{preface}{%
\chapter*{Preface}\label{preface}}


This is an open source textbook aimed at introducing undergraduate students to data science. It was originally written for the University of British Columbia's \href{https://ubc-dsci.github.io/dsci-100/}{DSCI 100 - Introduction to Data Science} course. In this book, we define data science as the study and development of reproducible, auditable processes to obtain value (i.e., insight) from data.

\hypertarget{why-read-this-book}{%
\section*{Why read this book}\label{why-read-this-book}}


This book will provide readers unfamiliar with data science an introduction to
the field, advice on best practices for performing data analysis, as well as
practical skills to get up and going doing their own data analyses.

\hypertarget{structure-of-the-book}{%
\section*{Structure of the book}\label{structure-of-the-book}}


The book is structured so that learners spend the first four chapters learning how to use the R programming language and Jupyter notebooks to load, wrangle/clean, and visualize data, while answering descriptive and exploratory data analysis questions. The remaining chapters illustrate how to solve four common problems in data science, which are useful for answering predictive and inferential data analysis questions.

\hypertarget{software-information-and-conventions}{%
\section*{Software information and conventions}\label{software-information-and-conventions}}


I used the \textbf{knitr}\index{knitr} package \citep{xie2015} and the \textbf{bookdown}\index{bookdown} package \citep{R-bookdown} to compile my book. My R session information is shown below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{xfun}\SpecialCharTok{::}\FunctionTok{session\_info}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## R version 4.0.3 (2020-10-10)
## Platform: x86_64-apple-darwin17.0 (64-bit)
## Running under: macOS Catalina 10.15.7, RStudio 1.4.953
## 
## Locale: en_CA.UTF-8 / en_CA.UTF-8 / en_CA.UTF-8 / C / en_CA.UTF-8 / en_CA.UTF-8
## 
## Package version:
##   askpass_1.1          assertthat_0.2.1    
##   backports_1.1.10     base64enc_0.1.3     
##   BH_1.72.0.3          bit_4.0.4           
##   bit64_4.0.5          blob_1.2.1          
##   bookdown_0.21        brew_1.0.6          
##   broom_0.7.2          callr_3.5.1         
##   cancensus_0.3.2      canlang_0.0.1       
##   caret_6.0-86         cellranger_1.1.0    
##   class_7.3-17         cli_2.2.0           
##   clipr_0.7.1          codetools_0.2-16    
##   colorspace_1.4-1     commonmark_1.7      
##   compiler_4.0.3       covr_3.5.1          
##   cpp11_0.2.3          crayon_1.3.4        
##   crosstalk_1.1.0.1    curl_4.3            
##   data.table_1.13.2    DBI_1.1.0           
##   dbplyr_1.4.4         desc_1.2.0          
##   devtools_2.3.2       dials_0.0.9         
##   DiceDesign_1.8-1     digest_0.6.26       
##   dplyr_1.0.2          DT_0.16             
##   ellipsis_0.3.1       evaluate_0.14       
##   fansi_0.4.1          farver_2.0.3        
##   forcats_0.5.0        foreach_1.5.1       
##   fs_1.5.0             furrr_0.2.1         
##   future_1.21.0        gdtools_0.2.2       
##   generics_0.1.0       GGally_2.0.0        
##   ggplot2_3.3.2        gh_1.1.0            
##   git2r_0.27.1         globals_0.14.0      
##   glue_1.4.2           gower_0.2.2         
##   GPfit_1.0-8          graphics_4.0.3      
##   grDevices_4.0.3      grid_4.0.3          
##   gridExtra_2.3        gtable_0.3.0        
##   hardhat_0.1.5        haven_2.3.1         
##   highr_0.8            hms_0.5.3           
##   htmltools_0.5.0      htmlwidgets_1.5.2   
##   httr_1.4.2           igraph_1.2.6        
##   infer_0.5.3          ini_0.3.1           
##   ipred_0.9-9          isoband_0.2.2       
##   iterators_1.0.13     jsonlite_1.7.1      
##   KernSmooth_2.23.17   kknn_1.3.1          
##   knitr_1.30           labeling_0.4.2      
##   later_1.1.0.1        lattice_0.20-41     
##   lava_1.6.8.1         lazyeval_0.2.2      
##   lhs_1.1.1            lifecycle_0.2.0     
##   listenv_0.8.0        lubridate_1.7.9     
##   magick_2.5.2         magrittr_2.0.1      
##   markdown_1.1         MASS_7.3-53         
##   Matrix_1.2-18        memoise_1.1.0       
##   methods_4.0.3        mgcv_1.8-33         
##   mime_0.9             modeldata_0.1.0     
##   ModelMetrics_1.2.2.2 modelr_0.1.8        
##   munsell_0.5.0        nlme_3.1-149        
##   nnet_7.3-14          numDeriv_2016.8.1.1 
##   openssl_1.4.3        parallel_4.0.3      
##   parallelly_1.22.0    parsnip_0.1.4       
##   pillar_1.4.6         pkgbuild_1.1.0      
##   pkgconfig_2.0.3      pkgload_1.1.0       
##   plogr_0.2.0          plyr_1.8.6          
##   png_0.1-7            praise_1.0.0        
##   prettyunits_1.1.1    pROC_1.16.2         
##   processx_3.4.4       prodlim_2019.11.13  
##   progress_1.2.2       promises_1.1.1      
##   ps_1.4.0             purrr_0.3.4         
##   R6_2.4.1             rcmdcheck_1.3.3     
##   RColorBrewer_1.1-2   Rcpp_1.0.5          
##   readr_1.4.0          readxl_1.3.1        
##   recipes_0.1.15       rematch_1.0.1       
##   rematch2_2.1.2       remotes_2.2.0       
##   reprex_0.3.0         reshape_0.8.8       
##   reshape2_1.4.4       rex_1.2.0           
##   rlang_0.4.8          rmarkdown_2.5       
##   roxygen2_7.1.1       rpart_4.1-15        
##   rprojroot_2.0.2      rsample_0.0.8       
##   RSQLite_2.2.1        rstudioapi_0.13     
##   rversions_2.0.2      rvest_0.3.6         
##   scales_1.1.1         selectr_0.4-2       
##   sessioninfo_1.1.1    slider_0.1.5        
##   splines_4.0.3        SQUAREM_2020.5      
##   stats_4.0.3          stats4_4.0.3        
##   stringi_1.5.3        stringr_1.4.0       
##   survival_3.2-7       svglite_1.2.3.2     
##   sys_3.4              systemfonts_0.3.2   
##   testthat_2.3.2       tibble_3.0.4        
##   tidymodels_0.1.2     tidyr_1.1.2         
##   tidyselect_1.1.0     tidyverse_1.3.0     
##   timeDate_3043.102    tinytex_0.26        
##   tools_4.0.3          tune_0.1.2          
##   usethis_1.6.3        utf8_1.1.4          
##   utils_4.0.3          vctrs_0.3.4         
##   viridisLite_0.3.0    warp_0.2.0          
##   whisker_0.4          withr_2.3.0         
##   workflows_0.2.1      xfun_0.18           
##   xml2_1.3.2           xopen_1.0.0         
##   yaml_2.2.1           yardstick_0.0.7
\end{verbatim}

Package names are in bold text (e.g., \textbf{rmarkdown}), and inline code and filenames are formatted in a typewriter font (e.g., \texttt{knitr::knit(\textquotesingle{}foo.Rmd\textquotesingle{})}). Function names are followed by parentheses (e.g., \texttt{bookdown::render\_book()}).

\hypertarget{acknowledgments}{%
\section*{Acknowledgments}\label{acknowledgments}}


A lot of people helped me when I was writing the book.

\begin{flushright}
The DSCI 100 teaching assistant team and Matías Salibián-Barrera
\end{flushright}

\hypertarget{about-the-authors}{%
\chapter*{About the Authors}\label{about-the-authors}}


Tiffany Timbers is an Assistant Professor of Teaching in the Department of Statistics and an Co-Director for the Master of Data Science program (Vancouver Option) at the University of British Columbia. In these roles she teaches and develops curriculum around the responsible application of Data Science to solve real-world problems. One of her favourite courses she teaches is a graduate course on collaborative software development, which focuses on teaching how to create R and Python packages using modern tools and workflows.

Trevor Campbell is an Assistant Professor of Statistics at the University of British Columbia. His research focuses on automated, scalable Bayesian inference algorithms, Bayesian nonparametrics, streaming data, and Bayesian theory. He was previously a postdoctoral associate advised by Tamara Broderick in the Computer Science and Artificial Intelligence Laboratory (CSAIL) and Institute for Data, Systems, and Society (IDSS) at MIT, a Ph.D.~candidate under Jonathan How in the Laboratory for Information and Decision Systems (LIDS) at MIT, and before that he was in the Engineering Science program at the University of Toronto.

Melissa Lee is an Assistant Professor of Teaching in the Department of Statistics at the University of British Columbia. She enjoys teaching introductory statistics and data science courses. She is dedicated to improving statistics and data science education for instructors and learners alike through developing content grounded in evidence-based teaching practices.

\mainmatter

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

This is an open source textbook aimed at introducing the field of data science. In this book, we define data science as the study and development of reproducible, auditable processes to obtain value (i.e., insight) from data.

The book is structured so that learners spend the first four chapters learning how to use the R programming language and Jupyter notebooks to load, wrangle/clean, and visualize data, while answering descriptive and exploratory data analysis questions. The remaining chapters illustrate how to solve four common problems in data science, which are useful for answering predictive and inferential data analysis questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Predicting a class/category for a new observation/measurement (e.g., cancerous or benign tumour)
\item
  Predicting a value for a new observation/measurement (e.g., 10 km race time for 20 year old females with a BMI of 25).
\item
  Finding previously unknown/unlabelled subgroups in your data (e.g., products commonly bought together on Amazon)
\item
  Estimating an average or a proportion from a representative sample (group of people or units) and using that estimate to generalize to the broader population (e.g., the proportion of undergraduate students that own an iphone)
\end{enumerate}

For each of these problems, we map them to the type of data analysis question being asked and discuss what kinds of data are needed to answer such questions \citep{leek2015question, peng2015art}. More advanced (e.g., causal or mechanistic) data analysis questions are beyond the scope of this text.

\textbf{Types of data analysis questions}

\begin{longtable}[]{@{}lll@{}}
\toprule
\begin{minipage}[b]{0.37\columnwidth}\raggedright
Question type\strut
\end{minipage} & \begin{minipage}[b]{0.32\columnwidth}\raggedright
Description\strut
\end{minipage} & \begin{minipage}[b]{0.22\columnwidth}\raggedright
Example\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.37\columnwidth}\raggedright
Descriptive\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright
A question which asks about summarized characteristics of a data set without interpretation (i.e., report a fact).\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
How many people live in each province or territory in Canada?\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.37\columnwidth}\raggedright
Exploratory\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright
A question asks if there are patterns, trends, or relationships within a single data set. Often used to propose hypotheses for future study.\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
Does political party voting change with indicators of wealth in a set of data collected on 2,000 people living in Canada?\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.37\columnwidth}\raggedright
Inferential\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright
A question that looks for patterns, trends, or relationships in a single data set \textbf{and} also asks for quantification of how applicable these findings are to the wider population.\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
Does political party voting change with indicators of wealth for all people living in Canada?\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.37\columnwidth}\raggedright
Predictive\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright
A question that asks about predicting measurements or labels for individuals (people or things). The focus is on what things predict some outcome, but not what causes the outcome.\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
What political party will someone vote for in the next Canadian election?\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.37\columnwidth}\raggedright
Causal\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright
A question that asks about whether changing one factor will lead to a change in another factor, on average, in the wider population.\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
Does wealth lead to voting for a certain political party in Canadian elections?\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.37\columnwidth}\raggedright
Mechanistic\strut
\end{minipage} & \begin{minipage}[t]{0.32\columnwidth}\raggedright
A question that asks about the underlying mechanism of the observed patterns, trends, or relationship (i.e., how does it happen?)\strut
\end{minipage} & \begin{minipage}[t]{0.22\columnwidth}\raggedright
How does wealth lead to voting for a certain political party in Canadian elections?\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Source: \href{https://science.sciencemag.org/content/347/6228/1314}{What is the question?} by Jeffery T. Leek, Roger D. Peng \& \href{https://leanpub.com/artofdatascience}{The Art of Data Science} by Roger Peng \& Elizabeth Matsui

\hypertarget{chapter-learning-objectives}{%
\section{Chapter learning objectives}\label{chapter-learning-objectives}}

By the end of the chapter, students will be able to:

\begin{itemize}
\tightlist
\item
  use a Jupyter notebook to execute provided R code
\item
  edit code and markdown cells in a Jupyter notebook
\item
  create new code and markdown cells in a Jupyter notebook
\item
  load the \texttt{tidyverse} package into R
\item
  create new variables and objects in R using the assignment symbol
\item
  use the help and documentation tools in R
\item
  match the names of the following functions from the \texttt{tidyverse} package to their documentation descriptions:

  \begin{itemize}
  \tightlist
  \item
    \texttt{read\_csv}
  \item
    \texttt{select}
  \item
    \texttt{filter}
  \item
    \texttt{mutate}
  \item
    \texttt{ggplot}
  \item
    \texttt{aes}
  \end{itemize}
\end{itemize}

\hypertarget{jupyter-notebooks}{%
\section{Jupyter notebooks}\label{jupyter-notebooks}}

Jupyter notebooks are documents that contain a mix of computer code (and its output) and formattable text. Given that they are able to combine these two in a single document---code is not separate from the output or written report---notebooks are one of the leading tools to create \emph{reproducible data analyses}. A reproducible data analysis is one where you can reliably and easily recreate the same results when analyzing the same data. Although this sounds like something that should always be true of any data analysis, in reality this is not often the case; one needs to make a conscious effort to perform data analysis in a reproducible manner.

The name Jupyter came from combining the names of the three programming language that it was initially targeted for (Julia, Python, and R), and now many other languages can be used with Jupyter notebooks.

A notebook looks like this:

\begin{figure}
\includegraphics[width=1\linewidth]{img/jupyter} \caption{Jupyter Notebook}\label{fig:img-jupyter}
\end{figure}

We have included a short demo video here to help you get started and to introduce you to R and Jupyter.
However, the best way to learn how to write and run code and formattable text in a Jupyter notebook is to do it yourself! \href{https://github.com/UBC-DSCI/dsci-100-assets/blob/master/2019-fall/materials/worksheet_01/worksheet_01.ipynb}{Here is a worksheet} that provides a step-by-step guide through the basics.

\hypertarget{loading-a-spreadsheet-like-dataset}{%
\section{Loading a spreadsheet-like dataset}\label{loading-a-spreadsheet-like-dataset}}

Often, the first thing we need to do in data analysis is to load a dataset into R. When we bring spreadsheet-like (think Microsoft Excel tables) data, generally shaped like a rectangle, into R it is represented as what we call a \emph{data frame} object. It is very similar to a spreadsheet where the rows are the collected observations and the columns are the variables.

\begin{figure}
\includegraphics[width=1\linewidth]{img/spreadsheet_vs_dataframe} \caption{A spreadsheet versus a data frame in R}\label{fig:img-spreadsheet-vs-dataframe}
\end{figure}

The first kind of data we will learn how to load into R (as a data frame) is the
spreadsheet-like \emph{comma-separated values} format (\texttt{.csv} for short).
These files have names ending in \texttt{.csv}, and can be opened open and saved from common spreadsheet programs like Microsoft Excel and Google Sheets.
For example, a \texttt{.csv} file named \texttt{can\_lang.csv} \href{https://github.com/UBC-DSCI/introduction-to-datascience/blob/master/data/can_lang.csv}{is included with the code for this book}.
This file--- originally from \href{https://ttimbers.github.io/canlang/}{\{canlang\} R data package}---has language data collected in the 2016 Canadian census \citep{cancensus2016}.
If we were to open this data in a plain text editor, we would see each row on its own line, and each entry in the table separated by a comma:

\begin{verbatim}
category,language,mother_tongue,most_at_home,most_at_work,lang_known
Aboriginal languages,"Aboriginal languages, n.o.s.",590,235,30,665
Non-Official & Non-Aboriginal languages,Afrikaans,10260,4785,85,23415
Non-Official & Non-Aboriginal languages,"Afro-Asiatic languages, n.i.e.",1150,445,10,2775
Non-Official & Non-Aboriginal languages,Akan (Twi),13460,5985,25,22150
Non-Official & Non-Aboriginal languages,Albanian,26895,13135,345,31930
Aboriginal languages,"Algonquian languages, n.i.e.",45,10,0,120
Aboriginal languages,Algonquin,1260,370,40,2480
Non-Official & Non-Aboriginal languages,American Sign Language,2685,3020,1145,21930
Non-Official & Non-Aboriginal languages,Amharic,22465,12785,200,33670
\end{verbatim}

To load this data into R, and then to do anything else with it afterwards, we will need to use something called a \emph{function.}
A function is a special word in R that takes in instructions (we call these \emph{arguments}) and does something. The function we will
use to read a \texttt{.csv} file into R is called \texttt{read\_csv}.

In its most basic use-case, \texttt{read\_csv} expects that the data file:

\begin{itemize}
\tightlist
\item
  has column names (or \emph{headers}),
\item
  uses a comma (\texttt{,}) to separate the columns, and
\item
  does not have row names.
\end{itemize}

Below you'll see the code used to load the data into R using the \texttt{read\_csv} function. But there is one extra step we need to do first. Since \texttt{read\_csv} is not included in the base installation of R,
to be able to use it we have to load it from somewhere else: a collection of useful functions known as a \emph{package}. The \texttt{read\_csv} function in particular
is in the \texttt{tidyverse} package (more on this later), which we load using the \texttt{library} function.

Next, we call the \texttt{read\_csv} function and pass it a single argument: the name of the file, \texttt{"can\_lang.csv"}. We have to put quotes around filenames and other letters and words that we
use in our code to distinguish it from the special words that make up R programming language. This is the only argument we need to provide for this file, because our file satifies everthing else
the \texttt{read\_csv} function expects in the default use-case (which we just discussed). Later in the course, we'll learn more about how to deal with more complicated files where the default arguments are not
appropriate. For example, files that use spaces or tabs to separate the columns, or with no column names.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/can\_lang.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 6
##    category language mother_tongue most_at_home
##    <chr>    <chr>            <dbl>        <dbl>
##  1 Aborigi~ Aborigi~           590          235
##  2 Non-Off~ Afrikaa~         10260         4785
##  3 Non-Off~ Afro-As~          1150          445
##  4 Non-Off~ Akan (T~         13460         5985
##  5 Non-Off~ Albanian         26895        13135
##  6 Aborigi~ Algonqu~            45           10
##  7 Aborigi~ Algonqu~          1260          370
##  8 Non-Off~ America~          2685         3020
##  9 Non-Off~ Amharic          22465        12785
## 10 Non-Off~ Arabic          419890       223535
## # ... with 204 more rows, and 2 more variables:
## #   most_at_work <dbl>, lang_known <dbl>
\end{verbatim}

Above you can also see something neat that Jupyter does to help us understand our code: it colours text depending on its meaning in R. For example,
you'll note that functions get bold green text, while letters and words surrounded by quotations like filenames get blue text.

\begin{quote}
\textbf{In case you want to know more (optional):}
We use the \texttt{read\_csv} function from the \texttt{tidyverse} instead of the base R function \texttt{read.csv} because it's faster and it creates a nicer variant of the base R data frame called a \emph{tibble}.
This has several benefits that we'll discuss in further detail later in the course.
\end{quote}

\hypertarget{assigning-value-to-a-data-frame}{%
\section{Assigning value to a data frame}\label{assigning-value-to-a-data-frame}}

When we loaded the language data collected in the 2016 Canadian census in R above using \texttt{read\_csv}, we did not give this data frame a name, so it was
just printed to the screen and we cannot do anything else with it. That isn't very useful; what we would like to do is give a name to the data frame that \texttt{read\_csv} outputs
so that we can use it later for analysis and visualization.

To assign name to something in R, there are two possible ways---using either the assignment symbol (\texttt{\textless{}-}) or the equals symbol (\texttt{=}). From a style perspective,
the assignment symbol is preferred and is what we will use in this course. When we name something in R using the assignment symbol, \texttt{\textless{}-}, we do not need to surround
it with quotes like the filename. This is because we are formally telling R about this word and giving it a value. Only characters and words that act as values need
to be surrounded by quotes.

Let's now use the assignment symbol to give the name \texttt{can\_lang} to the language data collected in the 2016 Canadian census data frame that we get from \texttt{read\_csv}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{can\_lang }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/can\_lang.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Wait a minute! Nothing happened this time! Or at least it looks like that. But actually, something did happen: the data was read in and now has the name \texttt{can\_lang} associated with it.
And we can use that name to access the data frame and do things with it. First we will type the name of the data frame to print it to the screen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{can\_lang}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 6
##    category language mother_tongue most_at_home
##    <chr>    <chr>            <dbl>        <dbl>
##  1 Aborigi~ Aborigi~           590          235
##  2 Non-Off~ Afrikaa~         10260         4785
##  3 Non-Off~ Afro-As~          1150          445
##  4 Non-Off~ Akan (T~         13460         5985
##  5 Non-Off~ Albanian         26895        13135
##  6 Aborigi~ Algonqu~            45           10
##  7 Aborigi~ Algonqu~          1260          370
##  8 Non-Off~ America~          2685         3020
##  9 Non-Off~ Amharic          22465        12785
## 10 Non-Off~ Arabic          419890       223535
## # ... with 204 more rows, and 2 more variables:
## #   most_at_work <dbl>, lang_known <dbl>
\end{verbatim}

\hypertarget{creating-subsets-of-data-frames-with-select-filter}{%
\section{\texorpdfstring{Creating subsets of data frames with \texttt{select} \& \texttt{filter}}{Creating subsets of data frames with select \& filter}}\label{creating-subsets-of-data-frames-with-select-filter}}

Now, we are going to learn how to obtain subsets of data from a data frame in R using two other \texttt{tidyverse} functions: \texttt{select} and \texttt{filter}.
The \texttt{select} function allows you to create a subset of the columns of a data frame, while the \texttt{filter} function allows you to obtain a subset of the rows with specific values.

Before we start using \texttt{select} and \texttt{filter}, let's take a look at the language data collected in the 2016 Canadian census again to familiarize ourselves with it.
We will do this by printing the data we loaded earlier in the chapter to the screen.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{can\_lang}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 6
##    category language mother_tongue most_at_home
##    <chr>    <chr>            <dbl>        <dbl>
##  1 Aborigi~ Aborigi~           590          235
##  2 Non-Off~ Afrikaa~         10260         4785
##  3 Non-Off~ Afro-As~          1150          445
##  4 Non-Off~ Akan (T~         13460         5985
##  5 Non-Off~ Albanian         26895        13135
##  6 Aborigi~ Algonqu~            45           10
##  7 Aborigi~ Algonqu~          1260          370
##  8 Non-Off~ America~          2685         3020
##  9 Non-Off~ Amharic          22465        12785
## 10 Non-Off~ Arabic          419890       223535
## # ... with 204 more rows, and 2 more variables:
## #   most_at_work <dbl>, lang_known <dbl>
\end{verbatim}

In this data frame there are 214 rows rows (corresponding to the 214 languages recorded on the 2016 Canadian census)
and 6 columns:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{category}: Higher level language category (describing whether the language is an Official Canadian language, an Aboriginal language, or a Non-Official and Non-Aboriginal language).
\item
  \texttt{language}: Language queried about on the Canadian Census.
\item
  \texttt{mother\_tongue}: Total count of Canadians from the Census who reported the language as their mother tongue. Mother tongue is generally defined as the language someone was exposed to since birth.
\item
  \texttt{most\_at\_home}: Total count of Canadians from the Census who reported the language as spoken most often at home.
\item
  \texttt{most\_at\_work}: Total count of Canadians from the Census who reported the language as used most often at work for the population.
\item
  \texttt{lang\_known}: Total count of Canadians from the Census who reported knowledge of language for the population in private households.
\end{enumerate}

Now let's use \texttt{select} to extract the language column from this data frame. To do this, we need to provide the \texttt{select} function with two arguments. The first argument is the
name of the data frame object, which in this example is \texttt{can\_lang}. The second argument is the column name that we want to select, here \texttt{language}. After passing these two arguments,
the \texttt{select} function returns a single column (the \texttt{language} column that we asked for) as a data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{language\_column }\OtherTok{\textless{}{-}} \FunctionTok{select}\NormalTok{(can\_lang, language)}
\NormalTok{language\_column}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 1
##    language                      
##    <chr>                         
##  1 Aboriginal languages, n.o.s.  
##  2 Afrikaans                     
##  3 Afro-Asiatic languages, n.i.e.
##  4 Akan (Twi)                    
##  5 Albanian                      
##  6 Algonquian languages, n.i.e.  
##  7 Algonquin                     
##  8 American Sign Language        
##  9 Amharic                       
## 10 Arabic                        
## # ... with 204 more rows
\end{verbatim}

\hypertarget{using-select-to-extract-multiple-columns}{%
\subsection{\texorpdfstring{Using \texttt{select} to extract multiple columns}{Using select to extract multiple columns}}\label{using-select-to-extract-multiple-columns}}

We can also use \texttt{select} to obtain a subset of the data frame with multiple columns. Again, the first argument is the name of the data frame.
Then we list all the columns we want as arguments separated by commas. Here we create a subset of three columns: language, mother tongue, and language spoken most often at home.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{three\_columns }\OtherTok{\textless{}{-}} \FunctionTok{select}\NormalTok{(can\_lang, language, mother\_tongue, most\_at\_home)}
\NormalTok{three\_columns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 3
##    language                  mother_tongue most_at_home
##    <chr>                             <dbl>        <dbl>
##  1 Aboriginal languages, n.~           590          235
##  2 Afrikaans                         10260         4785
##  3 Afro-Asiatic languages, ~          1150          445
##  4 Akan (Twi)                        13460         5985
##  5 Albanian                          26895        13135
##  6 Algonquian languages, n.~            45           10
##  7 Algonquin                          1260          370
##  8 American Sign Language             2685         3020
##  9 Amharic                           22465        12785
## 10 Arabic                           419890       223535
## # ... with 204 more rows
\end{verbatim}

\hypertarget{using-select-to-extract-a-range-of-columns}{%
\subsection{\texorpdfstring{Using \texttt{select} to extract a range of columns}{Using select to extract a range of columns}}\label{using-select-to-extract-a-range-of-columns}}

We can also use \texttt{select} to obtain a subset of the data frame constructed from a range of columns. To do this we use the colon (\texttt{:}) operator to denote the range.
For example, to get all the columns in the data frame from \texttt{language} to \texttt{most\_at\_home} we pass \texttt{language:most\_at\_home} as the second argument to the \texttt{select} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{column\_range }\OtherTok{\textless{}{-}} \FunctionTok{select}\NormalTok{(can\_lang, language}\SpecialCharTok{:}\NormalTok{most\_at\_home)}
\NormalTok{column\_range}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 3
##    language                  mother_tongue most_at_home
##    <chr>                             <dbl>        <dbl>
##  1 Aboriginal languages, n.~           590          235
##  2 Afrikaans                         10260         4785
##  3 Afro-Asiatic languages, ~          1150          445
##  4 Akan (Twi)                        13460         5985
##  5 Albanian                          26895        13135
##  6 Algonquian languages, n.~            45           10
##  7 Algonquin                          1260          370
##  8 American Sign Language             2685         3020
##  9 Amharic                           22465        12785
## 10 Arabic                           419890       223535
## # ... with 204 more rows
\end{verbatim}

\hypertarget{using-filter-to-extract-a-single-row}{%
\subsection{\texorpdfstring{Using \texttt{filter} to extract a single row}{Using filter to extract a single row}}\label{using-filter-to-extract-a-single-row}}

We can use the \texttt{filter} function to obtain the subset of rows with desired values from a data frame. Again, our first argument is the name of the data frame object, \texttt{can\_lang}.
The second argument is a logical statement to use when filtering the rows. Here, for example, we'll say that we are interested in rows where the language is Mandarin. To make
this comparison, we use the \emph{equivalency operator} \texttt{==} to compare the values of the \texttt{language} column with the value \texttt{"Mandarin"}. Similar to when we loaded the data file and put quotes around the filename,
here we need to put quotes around \texttt{"Mandarin"} to tell R that this is a character value and not one of the special words that make up R programming language, nor one of the names
we have given to data frames in the code we have already written.

With these arguments, \texttt{filter} returns a data frame that has all the columns of the input data frame but only the rows we asked for in our logical filter statement.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mandarin }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(can\_lang, language }\SpecialCharTok{==} \StringTok{"Mandarin"}\NormalTok{)}
\NormalTok{mandarin}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 6
##   category language mother_tongue most_at_home
##   <chr>    <chr>            <dbl>        <dbl>
## 1 Non-Off~ Mandarin        592040       462890
## # ... with 2 more variables: most_at_work <dbl>,
## #   lang_known <dbl>
\end{verbatim}

\hypertarget{using-filter-to-extract-rows-with-values-above-a-threshold}{%
\subsection{\texorpdfstring{Using \texttt{filter} to extract rows with values above a threshold}{Using filter to extract rows with values above a threshold}}\label{using-filter-to-extract-rows-with-values-above-a-threshold}}

If we are interested in finding information about the languages who have a higher number of people who primarily speak it at home compared to Mandarin---which is reported to have 462890 people speaking it as the primary language they speak in their home---then we can create a filter
to obtain rows where the value of \texttt{most\_at\_home} is greater than 462890.
In this case, we see that \texttt{filter} returns a data frame with 2 rows; this indicates that there are two languages that are spoken more often at
home compared to Mandarin.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{spoke\_often\_at\_home }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(can\_lang, most\_at\_home }\SpecialCharTok{\textgreater{}} \DecValTok{462890}\NormalTok{)}
\NormalTok{spoke\_often\_at\_home}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 6
##   category language mother_tongue most_at_home
##   <chr>    <chr>            <dbl>        <dbl>
## 1 Officia~ English       19460850     22162865
## 2 Officia~ French         7166700      6943800
## # ... with 2 more variables: most_at_work <dbl>,
## #   lang_known <dbl>
\end{verbatim}

\hypertarget{exploring-data-with-visualizations}{%
\section{Exploring data with visualizations}\label{exploring-data-with-visualizations}}

Creating effective data visualizations is an essential piece to any data analysis. For the remainder of Chapter 1, we will learn how to use
functions from the \texttt{tidyverse} to make visualizations that let us explore relationships in data. In particular, we'll develop a visualization
of the language data collected in the 2016 Canadian census we've been working with that will help us understand two potential relationships in the data:
first, the relationship between the number of people who speak a language as their mother tongue and the number of people who speak that language as their primary spoken language at home, and second, whether there is a pattern in the strength of this relationship in the higher level language categories (Official languages, Aboriginal languages, or non-official and non-Aboriginal languages). This is an example of an exploratory data analysis
question: we are looking for relationships and patterns within the data set we have, but are not trying to generalize what we find beyond this data set.

\hypertarget{using-ggplot-to-create-a-scatter-plot}{%
\subsection{\texorpdfstring{Using \texttt{ggplot} to create a scatter plot}{Using ggplot to create a scatter plot}}\label{using-ggplot-to-create-a-scatter-plot}}

Taking another look at our data set below, we can immediately see that the three columns (or variables) we are interested in visualizing---mother tongue, language spoken most at home, and higher level language category---are all in separate columns. In addition, there is a single row (or observation) for each language.
The data are therefore in what we call a \emph{tidy data} format.
Tidy data is particularly important concept and will be a major focus in the remainder of this course: many of the functions from \texttt{tidyverse} require tidy data,
including the \texttt{ggplot} function that we will use shortly for our visualization. We will formally introduce this concept in chapter 3.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{can\_lang}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 6
##    category language mother_tongue most_at_home
##    <chr>    <chr>            <dbl>        <dbl>
##  1 Aborigi~ Aborigi~           590          235
##  2 Non-Off~ Afrikaa~         10260         4785
##  3 Non-Off~ Afro-As~          1150          445
##  4 Non-Off~ Akan (T~         13460         5985
##  5 Non-Off~ Albanian         26895        13135
##  6 Aborigi~ Algonqu~            45           10
##  7 Aborigi~ Algonqu~          1260          370
##  8 Non-Off~ America~          2685         3020
##  9 Non-Off~ Amharic          22465        12785
## 10 Non-Off~ Arabic          419890       223535
## # ... with 204 more rows, and 2 more variables:
## #   most_at_work <dbl>, lang_known <dbl>
\end{verbatim}

We will begin with a scatter plot of the \texttt{mother\_tongue} and \texttt{most\_at\_home} columns from our data frame.
To create a scatter plot of these two variables using the \texttt{ggplot} function, we do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  call the \texttt{ggplot} function
\item
  provide the name of the data frame as the first argument
\item
  call the aesthetic function, \texttt{aes}, to specify which column will correspond to the x-axis and which will correspond to the y-axis
\item
  add a \texttt{+} symbol at the end of the \texttt{ggplot} call to add a layer to the plot
\item
  call the \texttt{geom\_point} function to tell R that we want to represent the data points as dots/points to create a scatter plot.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(can\_lang, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ most\_at\_home, }\AttributeTok{y =}\NormalTok{ mother\_tongue)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/mother-tongue-vs-most-at-home-1.pdf}
\caption{\label{fig:mother-tongue-vs-most-at-home}Scatter plot of number of Canadians reporting a language as their mother tongue vs the primary language at home}
\end{figure}

\begin{quote}
\textbf{In case you have used R before and are curious:}
There are a small number of situations in which you can have a single R expression span multiple lines.
Here, the \texttt{+} symbol at the end of the first line tells R that the expression isn't done yet and to
continue reading on the next line. While not strictly necessary, this sort of pattern will appear a
lot when using \texttt{ggplot} as it keeps things more readable.
\end{quote}

\hypertarget{formatting-ggplot-objects}{%
\subsection{Formatting ggplot objects}\label{formatting-ggplot-objects}}

It is motivating and exciting that we have already been able to visualize our
data to help answer our question, but we are not done yet! There is more we can
(and should) do to improve the interpretability of the data visualization that
we created. For example, by default, R uses the column names as the axis labels,
however, usually these column names do not have enough information about
the variable in the column. We really should replace this default with a more
informative label. For the example above, the column name \texttt{mother\_tongue} is
used as the label for the y-axis, but most people will not know what that is.
And even if they did, they will not know how we are measuring mother tongue, nor
which group of people the measurements were taken. An axis label that
read ``Mother tongue (number of Canadian residents)'' would be much more
informative.

Adding additional layers to our visualizations that we create in \texttt{ggplot} is one
common and easy way to improve and refine our data visualizations. New layers
are added to \texttt{ggplot} objects using the \texttt{+} symbol. For example, we can use the
\texttt{xlab} and \texttt{ylab} functions to add layers where we specify meaningful and
informative labels for the x and y axes. Again, since we are specifying words
(e.g.~\texttt{"Mother\ tongue\ (number\ of\ Canadian\ residents)"}) as arguments to \texttt{xlab}
and \texttt{ylab}, we surround them with double quotes. There are many more layers we
can add to format the plot further, and we will explore these in later chapters.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(can\_lang, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ most\_at\_home, }\AttributeTok{y =}\NormalTok{ mother\_tongue)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Language spoken most at home (number of Canadian residents)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Mother tongue (number of Canadian residents)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/mother-tongue-vs-most-at-home-labs-1.pdf}
\caption{\label{fig:mother-tongue-vs-most-at-home-labs}Scatter plot of number of Canadians reporting a language as their mother tongue vs the primary language at home with x and y labels}
\end{figure}

Most of the data points from the 214 observations in this data set are bunched up in the lower left-handside of this visualization. This is because many many more people in Canada speak the two official languages (English and French). Thus to answer our question, we will need to adjust the scale of the x and y axes so that they are on a log scale. We can again add additional layers to the plot object using the \texttt{+} symbol to do this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(can\_lang, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ most\_at\_home, }\AttributeTok{y =}\NormalTok{ mother\_tongue)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Language spoken most at home (number of Canadian residents)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Mother tongue (number of Canadian residents)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\NormalTok{comma) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_log10}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\NormalTok{comma)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/mother-tongue-vs-most-at-home-scale-1.pdf}
\caption{\label{fig:mother-tongue-vs-most-at-home-scale}Scatter plot of number of Canadians reporting a language as their mother tongue vs the primary language at home with log adjusted x and y axes}
\end{figure}

From this visualization we see that for the 214 languages in this data set, as the number of people who have a language as their mother tongue increases, so does the number of people who speak that language at home. When we see
two variables do this, we call this a \emph{positive relationship}. Because the points are fairly close together, we can say that the relationship is strong. Because drawing a straight line through these
points would fit the pattern we observe quite well, we say that it's linear.

Learning how to describe data visualizations is a very useful skill. We will provide descriptions for you in this course (as we did above) until we get to Chapter 4,
which focuses on data visualization. Then, we will explicitly teach you how to do this yourself, and how to not over-state or over-interpret the results
from a visualization.

\hypertarget{changing-the-units}{%
\subsection{Changing the units}\label{changing-the-units}}

What does it mean that 19,460,850
people reported that their mother tongue was English in the 2016 Canadian
census? To really understand this number, we need context. In particular, how
many people were in Canada when this data was collected? From the 2016 Canadian
census profile, we can see that the population was reported to be
35,151,728 people. The count of
the number of people who report that English is their mother tongue is much more
meaningful when we report it in this context. We can even go a step further and
transform this count to a relative frequency, or proportion, so that we can
represent this as a single meaningful number in our data visualizations. We can
do this by dividing the number of people reporting a given language as their
mother tongue by the number of people who live in Canada. For example, the
proportion of people who reported that their mother tongue was English in the
2016 Canadian census was
0.55.

We can use the \texttt{mutate} function in R to do this for all of the languages in the 2016 Canadian census data set. \texttt{mutate} is useful for creating new columns in a data frame, as well as transforming existing columns. It's general syntax is: \texttt{mutate(dataframe,\ column\_to\_create/transform\ =\ value/how\_to\_transform)}. Below we use mutate to calculate the proportion of people reporting a given language as their mother tongue for all the languages in the \texttt{can\_lang} data set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{can\_lang }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{(can\_lang, }\AttributeTok{mother\_tongue =}\NormalTok{ mother\_tongue }\SpecialCharTok{/} \DecValTok{35151728}\NormalTok{)}
\NormalTok{can\_lang}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 6
##    category language mother_tongue most_at_home
##    <chr>    <chr>            <dbl>        <dbl>
##  1 Aborigi~ Aborigi~    0.0000168           235
##  2 Non-Off~ Afrikaa~    0.000292           4785
##  3 Non-Off~ Afro-As~    0.0000327           445
##  4 Non-Off~ Akan (T~    0.000383           5985
##  5 Non-Off~ Albanian    0.000765          13135
##  6 Aborigi~ Algonqu~    0.00000128           10
##  7 Aborigi~ Algonqu~    0.0000358           370
##  8 Non-Off~ America~    0.0000764          3020
##  9 Non-Off~ Amharic     0.000639          12785
## 10 Non-Off~ Arabic      0.0119           223535
## # ... with 204 more rows, and 2 more variables:
## #   most_at_work <dbl>, lang_known <dbl>
\end{verbatim}

Let's also do this for the counts of the number of people who report that they speak a given language most often at home:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{can\_lang }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{(can\_lang, }\AttributeTok{most\_at\_home =}\NormalTok{ most\_at\_home }\SpecialCharTok{/} \DecValTok{35151728}\NormalTok{)}
\NormalTok{can\_lang}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 6
##    category language mother_tongue most_at_home
##    <chr>    <chr>            <dbl>        <dbl>
##  1 Aborigi~ Aborigi~    0.0000168   0.00000669 
##  2 Non-Off~ Afrikaa~    0.000292    0.000136   
##  3 Non-Off~ Afro-As~    0.0000327   0.0000127  
##  4 Non-Off~ Akan (T~    0.000383    0.000170   
##  5 Non-Off~ Albanian    0.000765    0.000374   
##  6 Aborigi~ Algonqu~    0.00000128  0.000000284
##  7 Aborigi~ Algonqu~    0.0000358   0.0000105  
##  8 Non-Off~ America~    0.0000764   0.0000859  
##  9 Non-Off~ Amharic     0.000639    0.000364   
## 10 Non-Off~ Arabic      0.0119      0.00636    
## # ... with 204 more rows, and 2 more variables:
## #   most_at_work <dbl>, lang_known <dbl>
\end{verbatim}

Finally, let's visualize the data now that we have represented it as proportions (and change our axis labels to reflect this change in units!):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(can\_lang, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ most\_at\_home, }\AttributeTok{y =}\NormalTok{ mother\_tongue)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Language spoken most at home (proportion of Canadian residents)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Mother tongue (proportion of Canadian residents)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\NormalTok{comma) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_log10}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\NormalTok{comma)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/mother-tongue-vs-most-at-home-scale-props-1.pdf}
\caption{\label{fig:mother-tongue-vs-most-at-home-scale-props}Scatter plot of proportion of Canadians reporting a language as their mother tongue vs the primary language at home}
\end{figure}

From the visualization above, we can now clearly see that not just a lot, but that the majority of Canadians reported English as their mother tongue and as the language they speak most often at home. Changing the units to include this context increases our understanding and allows us to interpret the numbers in our data set better.

\hypertarget{coloring-points-by-group}{%
\subsection{Coloring points by group}\label{coloring-points-by-group}}

Now we'll move onto the second part of our exploratory data analysis question: when considering the relationship between the number of people who have a language as their mother tongue and the number of people who speak that language at home, is there a pattern in the strength of this relationship in the higher-level language categories (Official languages, Aboriginal languages, or non-official and non-Aboriginal languages)? One common way to explore this is to colour the data points on the
scatter plot we have already created by group/category. For example,
given that we have the higher level language category for each language recorded in the 2016 Canadian census, we can colour the points in our previous
scatter plot to represent each language's higher-level language category.

To do this, we modify our scatter plot code above. Specifically, we will add an argument to the \texttt{aes} function, specifying that the points should be coloured by the \texttt{category} column:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(can\_lang, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ most\_at\_home, }\AttributeTok{y =}\NormalTok{ mother\_tongue, }\AttributeTok{color =}\NormalTok{ category)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Language spoken most at home (proportion of Canadian residents)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Mother tongue (proportion of Canadian residents)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\NormalTok{comma) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_log10}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\NormalTok{comma)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/scatter-colour-by-category-1.pdf}
\caption{\label{fig:scatter-colour-by-category}Scatter plot of proportion of Canadians reporting a language as their mother tongue vs the primary language at home coloured by language category}
\end{figure}

What do we see when considering the second part of our exploratory question? Do we see a difference in the pattern of the relationship between the number of people who speak a language as their mother tongue and the number of people who speak a language as their primary spoken language at home between higher-level language categories? Probably not!

For each higher-level language category there appears to be a positive relationship between the number of people who speak a language as their mother tongue and the number of people who speak a language as their primary spoken language at home. This relationship looks similar, regardless of the category.

Does this mean that this relationship is positive for all languages in the world? Can we use this data visualization on its own to predict how many people have a given language as their mother tongue if we know how many people speak it as their primary language at home?

The answer to both these questions is ``no.'' However, with this exploratory data analysis, we can create new hypotheses, ideas,
and questions (like the ones at the beginning of this paragraph). Answering those questions would likely involve gathering additional data and doing more complex analyses, which we will
see more of later in this course.

\hypertarget{putting-it-all-together}{%
\subsection{Putting it all together}\label{putting-it-all-together}}

Below, we put everything from this chapter together in one code chunk. We have
added a few more layers to make the data visualization even more effective.
Specifically we used have improved the visualizations accessibility by choosing
colours that are easier to distinguish and also mapped category to
shape, we handled the problem of overlapping data points by making them slightly
transparent, and we changed the background from grey to white to improve the
contrast. This demonstrates the power of R: in relatively few lines of code, we
are able to create an entire data science workflow with a highly effective
data visualization.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}

\NormalTok{can\_lang }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/can\_lang.csv"}\NormalTok{)}

\NormalTok{can\_lang }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{(can\_lang, }\AttributeTok{mother\_tongue =}\NormalTok{ mother\_tongue }\SpecialCharTok{/} \DecValTok{35151728}\NormalTok{)}
\NormalTok{can\_lang }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{(can\_lang, }\AttributeTok{most\_at\_home =}\NormalTok{ most\_at\_home }\SpecialCharTok{/} \DecValTok{35151728}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(can\_lang, }\FunctionTok{aes}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ most\_at\_home,}
  \AttributeTok{y =}\NormalTok{ mother\_tongue,}
  \AttributeTok{colour =}\NormalTok{ category,}
  \AttributeTok{shape =}\NormalTok{ category}
\NormalTok{)) }\SpecialCharTok{+} \CommentTok{\# map categories to different shapes}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.6}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\# set the transparency of the points}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"deepskyblue2"}\NormalTok{, }\StringTok{"firebrick1"}\NormalTok{, }\StringTok{"black"}\NormalTok{)) }\SpecialCharTok{+} \CommentTok{\# choose point colours}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Language spoken most at home (proportion of Canadian residents)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Mother tongue (proportion of Canadian residents)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_log10}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\NormalTok{comma) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_log10}\NormalTok{(}\AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\NormalTok{comma) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\CommentTok{\# use a theme to have a white background}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/nachos-to-cheesecake-1.pdf}
\caption{\label{fig:nachos-to-cheesecake}Putting it all together: Scatter plot of proportion of Canadians reporting a language as their mother tongue vs the primary language at home coloured by language category}
\end{figure}

\hypertarget{reading}{%
\chapter{Reading in data locally and from the web}\label{reading}}

\hypertarget{overview}{%
\section{Overview}\label{overview}}

In this chapter, you'll learn to read spreadsheet-like data of various formats into R from your local device and the web. ``Reading'' (or ``loading'') is the process of converting data (stored as plain text, a database, HTML, etc.) into an object (e.g., a data frame) that R can easily access and manipulate. Thus reading data is the gateway to any data analysis; you won't be able to analyze data unless you've loaded it first. And because there are many ways to store data, there are similarly many ways to read data into R. The more time you spend upfront matching the data reading method to the type of data you have, the less time you will have to devote to re-formatting, cleaning and wrangling your data (the second step to all data analyses). It's like making sure your shoelaces are tied well before going for a run so that you don't trip later on!

\hypertarget{chapter-learning-objectives-1}{%
\section{Chapter learning objectives}\label{chapter-learning-objectives-1}}

By the end of the chapter, students will be able to:

\begin{itemize}
\item
  define the following:

  \begin{itemize}
  \tightlist
  \item
    absolute file path
  \item
    relative file path
  \item
    url
  \end{itemize}
\item
  read data into R using a relative path and a url
\item
  compare and contrast the following functions:

  \begin{itemize}
  \tightlist
  \item
    \texttt{read\_csv}
  \item
    \texttt{read\_tsv}
  \item
    \texttt{read\_csv2}
  \item
    \texttt{read\_delim}
  \item
    \texttt{read\_excel}
  \end{itemize}
\item
  match the following \texttt{tidyverse} \texttt{read\_*} function arguments to their descriptions:

  \begin{itemize}
  \tightlist
  \item
    \texttt{file}
  \item
    \texttt{delim}
  \item
    \texttt{col\_names}
  \item
    \texttt{skip}
  \end{itemize}
\item
  choose the appropriate \texttt{tidyverse} \texttt{read\_*} function and function arguments to load a given plain text tabular data set into R
\item
  use \texttt{readxl} package's \texttt{read\_excel} function and arguments to load a sheet from an excel file into R
\item
  connect to a database using the \texttt{DBI} package's \texttt{dbConnect} function
\item
  list the tables in a database using the \texttt{DBI} package's \texttt{dbListTables} function
\item
  create a reference to a database table that is queriable using the \texttt{tbl} from the \texttt{dbplyr} package
\item
  retrieve data from a database query and bring it into R using the \texttt{collect} function from the \texttt{dbplyr} package
\item
  use \texttt{write\_csv} to save a data frame to a \texttt{.csv} file
\item
  (\emph{optional}) scrape data from the web

  \begin{itemize}
  \tightlist
  \item
    read/scrape data from an internet URL using the rvest \texttt{html\_nodes} and \texttt{html\_text} functions
  \item
    compare downloading tabular data from a plain text file (e.g.~\texttt{.csv}) from the web versus scraping data from a \texttt{.html} file
  \end{itemize}
\end{itemize}

\hypertarget{absolute-and-relative-file-paths}{%
\section{Absolute and relative file paths}\label{absolute-and-relative-file-paths}}

When you load a data set into R, you first need to tell R where those files live. The file could live on your
computer (\emph{local}) or somewhere on the internet (\emph{remote}). In this section, we will discuss the case where the file lives on your computer.

The place where the file lives on your computer is called the ``path''. You can think of the path as directions to the file. There are two kinds of
paths: relative paths and absolute paths. A relative path is where the file is with respect to where you currently are on the computer (e.g., where the
Jupyter notebook file that you're working in is). On the other hand, an absolute path is where the file is in respect to the base (or root) folder of
the computer's filesystem.

Suppose our computer's filesystem looks like the picture below, and we are working in the Jupyter notebook titled \texttt{worksheetk\_02.ipynb}. If we want to
read in the \texttt{.csv} file named \texttt{happiness\_report.csv} into our Jupyter notebook using R, we could do this using either a relative or an absolute path.
We show both choices below.

\begin{figure}
\includegraphics[width=1\linewidth]{img/file-system-for-export-to-intro-datascience} \caption{Example file system}\label{fig:file-system-for-export-to-intro-datascience}
\end{figure}

\textbf{Reading \texttt{happiness\_report.csv} using a relative path:}

\begin{verbatim}
happiness_data <- read_csv("data/happiness_report.csv")
\end{verbatim}

\textbf{Reading \texttt{happiness\_report.csv} using an absolute path:}

\begin{verbatim}
happiness_data <- read_csv("/home/jupyter/dsci-100/worksheet_02/data/happiness_report.csv")
\end{verbatim}

So which one should you use? Generally speaking, to ensure your code can be run
on a different computer, you should use relative paths. An added bonus is that
it's also less typing! This is because the absolute path of a file (the names of
folders between the computer's root \texttt{/} and the file) isn't usually the same
across different computers. For example, suppose Fatima and Jayden are working on a
project together on the \texttt{happiness\_report.csv} data. Fatima's file is stored at

\texttt{/home/Fatima/project/data/happiness\_report.csv},

while Jayden's is stored at

\texttt{/home/Jayden/project/data/happiness\_report.csv}.

Even though Fatima and Jayden stored their files in the same place on their computers (in their home folders), the absolute paths are different due to their different usernames.
If Jayden has code that loads the \texttt{happiness\_report.csv} data using an absolute path, the code won't work on Fatima's computer.
But the relative path from inside the \texttt{project} folder (\texttt{data/happiness\_report.csv}) is the same on both computers; any code that uses relative paths will work on both!

See this video for another explanation:

\emph{Source: \href{https://www.udacity.com/course/linux-command-line-basics--ud595}{Udacity course ``Linux Command Line Basics''}}

\hypertarget{reading-tabular-data-from-a-plain-text-file-into-r}{%
\section{Reading tabular data from a plain text file into R}\label{reading-tabular-data-from-a-plain-text-file-into-r}}

Now we will learn more about reading tabular data from a plain text file into R, as well as how to write tabular data to a file.
Last chapter, we learned about using the \texttt{tidyverse} \texttt{read\_csv} function when reading files that match that function's expected defaults
(column names are present, and commas are used as the delimiter/separator between columns). In this section, we will learn how to read
files that do not satisfy the default expectations of \texttt{read\_csv}.

Before we jump into the cases where the data aren't in the expected default format for \texttt{tidyverse} and \texttt{read\_csv}, let's revisit the more straightforward
case where the defaults hold, and the only argument we need to give to the function is the path to the file, \texttt{data/can\_lang.csv}. The \texttt{can\_lang} data set contains language data from the 2016 Canadian census. We put \texttt{data/} before the file's name when we are loading the data set because this data set is located in a sub-folder, named \texttt{data}, relative to where we are running our R code.

Here is what the file would look like in a plain text editor:

\begin{verbatim}
category,language,mother_tongue,most_at_home,most_at_work,lang_known
Aboriginal languages,"Aboriginal languages, n.o.s.",590,235,30,665
Non-Official & Non-Aboriginal languages,Afrikaans,10260,4785,85,23415
Non-Official & Non-Aboriginal languages,"Afro-Asiatic languages, n.i.e.",1150,445,10,2775
Non-Official & Non-Aboriginal languages,Akan (Twi),13460,5985,25,22150
Non-Official & Non-Aboriginal languages,Albanian,26895,13135,345,31930
Aboriginal languages,"Algonquian languages, n.i.e.",45,10,0,120
Aboriginal languages,Algonquin,1260,370,40,2480
Non-Official & Non-Aboriginal languages,American Sign Language,2685,3020,1145,21930
Non-Official & Non-Aboriginal languages,Amharic,22465,12785,200,33670
\end{verbatim}

And here is a review of how we can use \texttt{read\_csv} to load it into R. First we
load the \texttt{tidyverse} package to gain access to useful functions for reading the
data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Note: it is normal and expected that a message is printed out after
loading the \texttt{tidyverse} and some packages. Generally, this message let's you
know if functions from the different packages were loaded share the same name
(which is confusing to R), and if so, which one you can access using just it's
name (and which one you need to refer the package name and the function name to
refer to it, this is called masking). Additionally, the \texttt{tidyverse} is a special
R package - it is a meta-package that bundles together several
related and commonly used packages. Because of this it lists the packages it
does the job of loading. In future when we load this package in this book we
will silence these messages to help with readability of the book.
\end{quote}

Next we use \texttt{read\_csv} to load the data into R, and in that call we specify the
relative path to the file.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{canlang\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/can\_lang.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## -- Column specification -------------------------------
## cols(
##   category = col_character(),
##   language = col_character(),
##   mother_tongue = col_double(),
##   most_at_home = col_double(),
##   most_at_work = col_double(),
##   lang_known = col_double()
## )
\end{verbatim}

\begin{quote}
Note: it is also normal and expected that a message is printed out after using
the \texttt{read\_csv} and related functions. This message functions to let you know the
data types of each of the columns that R inferred while reading the data into R.
In future when we use this and related functions to load data in this book we
will silence these messages to help with readability of the book.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{canlang\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 6
##    category language mother_tongue most_at_home
##    <chr>    <chr>            <dbl>        <dbl>
##  1 Aborigi~ Aborigi~           590          235
##  2 Non-Off~ Afrikaa~         10260         4785
##  3 Non-Off~ Afro-As~          1150          445
##  4 Non-Off~ Akan (T~         13460         5985
##  5 Non-Off~ Albanian         26895        13135
##  6 Aborigi~ Algonqu~            45           10
##  7 Aborigi~ Algonqu~          1260          370
##  8 Non-Off~ America~          2685         3020
##  9 Non-Off~ Amharic          22465        12785
## 10 Non-Off~ Arabic          419890       223535
## # ... with 204 more rows, and 2 more variables:
## #   most_at_work <dbl>, lang_known <dbl>
\end{verbatim}

\hypertarget{skipping-rows-when-reading-in-data}{%
\subsection{Skipping rows when reading in data}\label{skipping-rows-when-reading-in-data}}

Often times information about how data was collected, or other relevant information, is included at the top of the data file. This information is usually written in sentence and paragraph form, with no delimiter because it is not organized into columns. An example of this is shown below. This
information gives the data scientist useful context and information about the data, however, it is not well formatted or intended to be read into
a data frame cell along with the tabular data that follows later in the file.

\begin{verbatim}
Data source: https://ttimbers.github.io/canlang/
Data originally published in: Statistics Canada Census of Population 2016.
Reproduced and distributed on an as is basis with the permission of Statistics Canada.
category,language,mother_tongue,most_at_home,most_at_work,lang_known
Aboriginal languages,"Aboriginal languages, n.o.s.",590,235,30,665
Non-Official & Non-Aboriginal languages,Afrikaans,10260,4785,85,23415
Non-Official & Non-Aboriginal languages,"Afro-Asiatic languages, n.i.e.",1150,445,10,2775
Non-Official & Non-Aboriginal languages,Akan (Twi),13460,5985,25,22150
Non-Official & Non-Aboriginal languages,Albanian,26895,13135,345,31930
Aboriginal languages,"Algonquian languages, n.i.e.",45,10,0,120
Aboriginal languages,Algonquin,1260,370,40,2480
Non-Official & Non-Aboriginal languages,American Sign Language,2685,3020,1145,21930
Non-Official & Non-Aboriginal languages,Amharic,22465,12785,200,33670
\end{verbatim}

With this extra information being present at the top of the file, using \texttt{read\_csv} as we did previously does not allow us to correctly load the data into R. In the case of this file we end up only reading in one column of the data set:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{canlang\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/can\_lang\_meta{-}data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Note: In contrast to the normal and expected messages above, this time R
printed out a warning for us indicating that there might be a problem with how
our data is being read in.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{canlang\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 217 x 1
##    `Data source: https://ttimbers.github.io/canlang/`  
##    <chr>                                               
##  1 Data originally published in: Statistics Canada Cen~
##  2 Reproduced and distributed on an as is basis with t~
##  3 category                                            
##  4 Aboriginal languages                                
##  5 Non-Official & Non-Aboriginal languages             
##  6 Non-Official & Non-Aboriginal languages             
##  7 Non-Official & Non-Aboriginal languages             
##  8 Non-Official & Non-Aboriginal languages             
##  9 Aboriginal languages                                
## 10 Aboriginal languages                                
## # ... with 207 more rows
\end{verbatim}

To successfully read data like this into R, the \texttt{skip} argument can be useful to tell R how many lines to skip before it should start reading in the data. In the example above, we would set this value to 3:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{canlang\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/can\_lang\_meta{-}data.csv"}\NormalTok{, }\AttributeTok{skip =} \DecValTok{3}\NormalTok{)}
\NormalTok{canlang\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 6
##    category language mother_tongue most_at_home
##    <chr>    <chr>            <dbl>        <dbl>
##  1 Aborigi~ Aborigi~           590          235
##  2 Non-Off~ Afrikaa~         10260         4785
##  3 Non-Off~ Afro-As~          1150          445
##  4 Non-Off~ Akan (T~         13460         5985
##  5 Non-Off~ Albanian         26895        13135
##  6 Aborigi~ Algonqu~            45           10
##  7 Aborigi~ Algonqu~          1260          370
##  8 Non-Off~ America~          2685         3020
##  9 Non-Off~ Amharic          22465        12785
## 10 Non-Off~ Arabic          419890       223535
## # ... with 204 more rows, and 2 more variables:
## #   most_at_work <dbl>, lang_known <dbl>
\end{verbatim}

\hypertarget{read_delim-as-a-more-flexible-method-to-get-tabular-data-into-r}{%
\subsection{\texorpdfstring{\texttt{read\_delim} as a more flexible method to get tabular data into R}{read\_delim as a more flexible method to get tabular data into R}}\label{read_delim-as-a-more-flexible-method-to-get-tabular-data-into-r}}

When our tabular data comes in a different format, we can use the \texttt{read\_delim} function instead. For example, a different version of this same data set has no column names and uses tabs as the delimiter instead of commas.

Here is how the file would look in a plain text editor:

\begin{verbatim}
Aboriginal languages    Aboriginal languages, n.o.s.    590 235 30  665
Non-Official & Non-Aboriginal languages Afrikaans   10260   4785    85  23415
Non-Official & Non-Aboriginal languages Afro-Asiatic languages, n.i.e.  1150    445 10  2775
Non-Official & Non-Aboriginal languages Akan (Twi)  13460   5985    25  22150
Non-Official & Non-Aboriginal languages Albanian    26895   13135   345 31930
Aboriginal languages    Algonquian languages, n.i.e.    45  10  0   120
Aboriginal languages    Algonquin   1260    370 40  2480
Non-Official & Non-Aboriginal languages American Sign Language  2685    3020    1145    21930
Non-Official & Non-Aboriginal languages Amharic 22465   12785   200 33670
Non-Official & Non-Aboriginal languages Arabic  419890  223535  5585    629055
\end{verbatim}

To get this into R using the \texttt{read\_delim()} function, we specify the first argument as the path to the file (as done with \texttt{read\_csv}), and then provide values to the \texttt{delim} argument (here a tab, which we represent by \texttt{"\textbackslash{}t"}) and the \texttt{col\_names} argument (here we specify that there are no column names to assign, and give it the value of \texttt{FALSE}). Both \texttt{read\_csv()} and \texttt{read\_delim()} have a \texttt{col\_names} argument and the default is \texttt{TRUE}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{canlang\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_delim}\NormalTok{(}\StringTok{"data/can\_lang.tsv"}\NormalTok{, }\AttributeTok{delim =} \StringTok{"}\SpecialCharTok{\textbackslash{}t}\StringTok{"}\NormalTok{, }\AttributeTok{col\_names =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{canlang\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 6
##    X1             X2             X3     X4    X5     X6
##    <chr>          <chr>       <dbl>  <dbl> <dbl>  <dbl>
##  1 Aboriginal la~ Aborigina~    590    235    30    665
##  2 Non-Official ~ Afrikaans   10260   4785    85  23415
##  3 Non-Official ~ Afro-Asia~   1150    445    10   2775
##  4 Non-Official ~ Akan (Twi)  13460   5985    25  22150
##  5 Non-Official ~ Albanian    26895  13135   345  31930
##  6 Aboriginal la~ Algonquia~     45     10     0    120
##  7 Aboriginal la~ Algonquin    1260    370    40   2480
##  8 Non-Official ~ American ~   2685   3020  1145  21930
##  9 Non-Official ~ Amharic     22465  12785   200  33670
## 10 Non-Official ~ Arabic     419890 223535  5585 629055
## # ... with 204 more rows
\end{verbatim}

Data frames in R need to have column names, thus if you read data into R as a data frame without column names then R assigns column names for them. If you used the \texttt{read\_*} functions to read the data into R, then R gives each column a name of X1, X2, \ldots, XN, where N is the number of columns in the data set.

\hypertarget{reading-tabular-data-directly-from-a-url}{%
\subsection{Reading tabular data directly from a URL}\label{reading-tabular-data-directly-from-a-url}}

We can also use \texttt{read\_csv()} or \texttt{read\_delim()} (and related functions) to read in tabular data directly from a url that contains tabular data. In this case, we provide the url to \texttt{read\_csv()} as the path to the file instead of a path to a local file on our computer. Similar to when we specify a path on our local computer, here we need to surround the url by quotes. All other arguments that we use are the same as when using these functions with a local file on our computer.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{canlang\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"https://raw.githubusercontent.com/UBC{-}DSCI/introduction{-}to{-}datascience/master/data/can\_lang.csv"}\NormalTok{)}
\NormalTok{canlang\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 6
##    category language mother_tongue most_at_home
##    <chr>    <chr>            <dbl>        <dbl>
##  1 Aborigi~ Aborigi~           590          235
##  2 Non-Off~ Afrikaa~         10260         4785
##  3 Non-Off~ Afro-As~          1150          445
##  4 Non-Off~ Akan (T~         13460         5985
##  5 Non-Off~ Albanian         26895        13135
##  6 Aborigi~ Algonqu~            45           10
##  7 Aborigi~ Algonqu~          1260          370
##  8 Non-Off~ America~          2685         3020
##  9 Non-Off~ Amharic          22465        12785
## 10 Non-Off~ Arabic          419890       223535
## # ... with 204 more rows, and 2 more variables:
## #   most_at_work <dbl>, lang_known <dbl>
\end{verbatim}

\hypertarget{previewing-a-data-file-before-reading-it-into-r}{%
\subsection{Previewing a data file before reading it into R}\label{previewing-a-data-file-before-reading-it-into-r}}

In all the examples above, we gave you previews of the data file before we read it into R. This is essential so that you can see whether or not there are column names, what the
delimiters are, and if there are lines you need to skip. You should do this yourself when trying to read in data files. In Jupyter, you preview data as a plain text file by right-clicking
on the file's name in the Jupyter home menu and selecting ``Open with'' and then selecting ``Editor''.

\begin{figure}
\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/open-data-w-editor-01-1} \caption{Opening data files with an editor in Jupyter}\label{fig:open-data-w-editor-01}
\end{figure}

\begin{figure}
\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/open-data-w-editor-02-1} \caption{A data file as viewed in an editor in Jupyter}\label{fig:open-data-w-editor-02}
\end{figure}

If you do not specify to open the data file with an editor, then Jupyter will
render a nice table for you and you will not be able to see the column
delimiters, and therefore you will not know which function to use, nor
which arguments to use and values to specify for them.

This is also demonstrated in the video below:

\hypertarget{reading-data-from-an-microsoft-excel-file}{%
\section{Reading data from an Microsoft Excel file}\label{reading-data-from-an-microsoft-excel-file}}

There are many other ways to store tabular data sets beyond plain text files, and similarly, many ways to load those data sets into R. For example, it is very common to encounter,
and need to load into R, data stored as a Microsoft Excel spreadsheet (with the filename extension \texttt{.xlsx}).
To be able to do this, a key thing to know is that even though \texttt{.csv} and \texttt{.xlsx} files look almost
identical when loaded into Excel, the data themselves are stored completely differently.
While \texttt{.csv} files are plain text files, where the characters you see when you open the file in a text editor are exactly the data they represent,
this is not the case for \texttt{.xlsx} files. Take a look at what a \texttt{.xlsx} file would look like in a text editor:

\begin{verbatim}
,?'O
    _rels/.rels???J1??>E?{7?
<?V????w8?'J???'QrJ???Tf?d??d?o?wZ'???@>?4'?|??hlIo??F
t                                                       8f??3wn
????t??u"/
          %~Ed2??<?w??
                       ?Pd(??J-?E???7?'t(?-GZ?????y???c~N?g[^_r?4
                                                                  yG?O
                                                                      ?K??G?RPX?<??,?'O[Content_Types].xml???n?0E%?J
                                                                                                                    ]TUEe??O??c[???????6q??s??d?m???\???H?^????3} ?rZY? ?:L60?^?????XTP+?|?3???"~?3T1W3???,?#p?R?!??w(??R???[S?D?kP?P!XS(?i?t?$?ei
X?a??4VT?,D?Jq
                D
                 ?????u?]??;??L?.8AhfNv}?hHF*??Jr?Q?%?g?U??CtX"8x>?.|????5j?/$???JE?c??~??4iw?????E;?+?S??w?cV+?:???2l???=?2nel???;|?V??????c'?????9?P&Bcj,?'OdocProps/app.xml??1
                                                     ?0???k????A?u?U?]??{#?:;/<?g?Cd????M+?=???Z?O??R+??u?P?X KV@??M$??a???d?_???4??5v?R????9D????t??Fk?Ú'P?=?,?'OdocProps/core.xml??MO?0
                                                             ??J?{???3j?h'??(q??U4J
                                                                                   ??=i?I'?b??[v?!??{gk?
                                                                                                         F2????v5yj??"J???,?d???J???C??l??4?-?`$?4t?K?.;?%c?J??G<?H????
                                                  X????z???6?????~q??X??????q^>??tH???*?D???M?g
??D?????????d?:g).?3.??j?P?F?'Oxl/_rels/workbook.xml.rels??Ak1??J?{7???R?^J?kk@Hf7??I?L???E]A?Þ?{a??`f?????b?6xUQ?@o?m}??o????X{???Q?????;?y?\?
                        O
?YY??4?L??S??k?252j??
??V ?C?g?C]???????
?
???E??TENyf6%
             ?Y????|??:%???}^ N?Q?N'????)??F?\??P?G??,?'O'xl/printerSettings/printerSettings1.bin?Wmn? 
                                                                                                        ??Sp>?G???q?#
                                                                                                                     ?I??5R'???q????(?L
??m??8F?5<  L`??`?A??2{dp??9R#?>7??Xu???/?X??HI?|?
                                                          ??r)???\?VA8?2dFfq???I]]o
5`????6A ?
\end{verbatim}

This type of file representation allows Excel files to store additional things that you cannot store in a \texttt{.csv} file, such as fonts, text formatting, graphics, multiple sheets
and more. And despite looking odd in a plain text editor, we can read Excel spreadsheets into R using the \texttt{readxl} package developed
specifically for this purpose.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readxl)}
\NormalTok{canlang\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_excel}\NormalTok{(}\StringTok{"data/can\_lang.xlsx"}\NormalTok{)}
\NormalTok{canlang\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 6
##    category language mother_tongue most_at_home
##    <chr>    <chr>            <dbl>        <dbl>
##  1 Aborigi~ Aborigi~           590          235
##  2 Non-Off~ Afrikaa~         10260         4785
##  3 Non-Off~ Afro-As~          1150          445
##  4 Non-Off~ Akan (T~         13460         5985
##  5 Non-Off~ Albanian         26895        13135
##  6 Aborigi~ Algonqu~            45           10
##  7 Aborigi~ Algonqu~          1260          370
##  8 Non-Off~ America~          2685         3020
##  9 Non-Off~ Amharic          22465        12785
## 10 Non-Off~ Arabic          419890       223535
## # ... with 204 more rows, and 2 more variables:
## #   most_at_work <dbl>, lang_known <dbl>
\end{verbatim}

If the \texttt{.xlsx} file has multiple sheets, you have to use the \texttt{sheet} argument to specify the sheet number or name. You can also specify cell ranges using the \texttt{range} argument. This functionality is useful when a single sheet contains multiple tables (a sad thing that happens to many Excel spreadsheets).

As with plain text files, you should always explore the data file before importing it into R. Exploring the data beforehand helps you decide which arguments you need to to load the data into R successfully. If you do not have the Excel program on your computer, you can use other programs to preview the file. Examples include Google Sheets and Libre Office.

\hypertarget{reading-data-from-a-database}{%
\section{Reading data from a database}\label{reading-data-from-a-database}}

Another very common form of data storage is the relational database. There are many relational database management systems, such as
\href{https://www.sqlite.org/index.html}{SQLite}, \href{https://www.mysql.com/}{MySQL}, \href{https://www.postgresql.org/}{PostgreSQL}, \href{https://www.oracle.com/ca-en/index.html}{Oracle}, and many more. These different relational database management systems each have their own advantages and limitations. Almost all employ SQL (\emph{structured query language}) to pull data from the database. Thankfully, you don't need to know SQL
to analyze data from a database;
several packages have been written
that allows R to connect to relational databases and use the R programming language as the front end (what the user types in) to pull data from them. These different relational database management systems have their own advantages, limitations, and excels in particular scenarios. In this book, we will
give examples of how to do this using R with SQLite and PostgreSQL databases.

\hypertarget{connecting-to-a-database}{%
\subsection{Connecting to a database}\label{connecting-to-a-database}}

\hypertarget{reading-data-from-a-sqlite-database}{%
\subsubsection{Reading data from a SQLite database}\label{reading-data-from-a-sqlite-database}}

SQLite is probably the simplest relational database that one can use in combination with R. SQLite databases are self-contained and usually stored and accessed locally on one computer. Data is usually stored in a file with a \texttt{.db} extension. Similar to Excel files, these are not plain text files and cannot be read in a plain text editor.

The first thing you need to do to read data into R from a database is to connect to the database. We do that using the \texttt{dbConnect} function from the \texttt{DBI} (database interface) package. This does not read in the data, but simply tells R where the database is and opens up a communication channel.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(DBI)}
\NormalTok{con\_lang\_data }\OtherTok{\textless{}{-}} \FunctionTok{dbConnect}\NormalTok{(RSQLite}\SpecialCharTok{::}\FunctionTok{SQLite}\NormalTok{(), }\StringTok{"data/can\_lang.db"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Often relational databases have many tables, and their power comes from the useful ways they can be joined. Thus anytime you want to access data from a relational database, you need to know the table names. You can get the names of all the tables in the database using the \texttt{dbListTables} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tables }\OtherTok{\textless{}{-}} \FunctionTok{dbListTables}\NormalTok{(con\_lang\_data)}
\NormalTok{tables}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "lang"
\end{verbatim}

We only get one table name returned from calling \texttt{dbListTables}, which tells us that there is only one table in this database. To reference a table in the database to do things like select columns and filter rows, we use the \texttt{tbl} function from the \texttt{dbplyr} package. The package \texttt{dbplyr} allows us to work with data stored in databases as if they were local data frames, which is useful because we can do a lot with big datasets without actually having to bring these vast amounts of data into your computer!

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dbplyr)}
\NormalTok{lang\_db }\OtherTok{\textless{}{-}} \FunctionTok{tbl}\NormalTok{(con\_lang\_data, }\StringTok{"lang"}\NormalTok{)}
\NormalTok{lang\_db}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # Source:   table<lang> [?? x 6]
## # Database: sqlite 3.33.0
## #   [/Users/tiffanytimbers/Documents/UBC-DSCI/introduction-to-datascience/data/can_lang.db]
##    category language mother_tongue most_at_home
##    <chr>    <chr>            <dbl>        <dbl>
##  1 Aborigi~ Aborigi~           590          235
##  2 Non-Off~ Afrikaa~         10260         4785
##  3 Non-Off~ Afro-As~          1150          445
##  4 Non-Off~ Akan (T~         13460         5985
##  5 Non-Off~ Albanian         26895        13135
##  6 Aborigi~ Algonqu~            45           10
##  7 Aborigi~ Algonqu~          1260          370
##  8 Non-Off~ America~          2685         3020
##  9 Non-Off~ Amharic          22465        12785
## 10 Non-Off~ Arabic          419890       223535
## # ... with more rows, and 2 more variables:
## #   most_at_work <dbl>, lang_known <dbl>
\end{verbatim}

Although it looks like we just got a data frame from the database, we didn't! It's a \emph{reference}, showing us data that is still in the SQLite database (note the first two lines of the output).
It does this because databases are often more efficient at selecting, filtering and joining large data sets than R. And typically, the database will not even be
stored on your computer, but rather a more powerful machine somewhere on the web. So R is lazy and waits to bring this data into memory until you explicitly tell
it to do so using the \texttt{collect} function from the \texttt{dbplyr} package.

Here we will filter for only rows in the Aboriginal languages category according to the 2016 Canada Census, and then use \texttt{collect} to finally bring this data into R as a data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aboriginal\_lang\_db }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(lang\_db, category }\SpecialCharTok{==} \StringTok{"Aboriginal languages"}\NormalTok{)}
\NormalTok{aboriginal\_lang\_db}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # Source:   lazy query [?? x 6]
## # Database: sqlite 3.33.0
## #   [/Users/tiffanytimbers/Documents/UBC-DSCI/introduction-to-datascience/data/can_lang.db]
##    category language mother_tongue most_at_home
##    <chr>    <chr>            <dbl>        <dbl>
##  1 Aborigi~ Aborigi~           590          235
##  2 Aborigi~ Algonqu~            45           10
##  3 Aborigi~ Algonqu~          1260          370
##  4 Aborigi~ Athabas~            50           10
##  5 Aborigi~ Atikame~          6150         5465
##  6 Aborigi~ Babine ~           110           20
##  7 Aborigi~ Beaver             190           50
##  8 Aborigi~ Blackfo~          2815         1110
##  9 Aborigi~ Carrier           1025          250
## 10 Aborigi~ Cayuga              45           10
## # ... with more rows, and 2 more variables:
## #   most_at_work <dbl>, lang_known <dbl>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{aboriginal\_lang\_data }\OtherTok{\textless{}{-}} \FunctionTok{collect}\NormalTok{(aboriginal\_lang\_db)}
\NormalTok{aboriginal\_lang\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 67 x 6
##    category language mother_tongue most_at_home
##    <chr>    <chr>            <dbl>        <dbl>
##  1 Aborigi~ Aborigi~           590          235
##  2 Aborigi~ Algonqu~            45           10
##  3 Aborigi~ Algonqu~          1260          370
##  4 Aborigi~ Athabas~            50           10
##  5 Aborigi~ Atikame~          6150         5465
##  6 Aborigi~ Babine ~           110           20
##  7 Aborigi~ Beaver             190           50
##  8 Aborigi~ Blackfo~          2815         1110
##  9 Aborigi~ Carrier           1025          250
## 10 Aborigi~ Cayuga              45           10
## # ... with 57 more rows, and 2 more variables:
## #   most_at_work <dbl>, lang_known <dbl>
\end{verbatim}

Why bother to use the \texttt{collect} function? The data looks pretty similar in both outputs shown above. And \texttt{dbplyr} provides lots of functions similar to \texttt{filter} that
you can use to directly feed the database reference (what \texttt{tbl} gives you) into downstream analysis functions (e.g., \texttt{ggplot2} for data visualization and \texttt{lm} for
linear regression modeling). However, this does not
work in \emph{every} case; look what happens when we try to use \texttt{nrow} to count rows in a data frame:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{nrow}\NormalTok{(aboriginal\_lang\_db)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] NA
\end{verbatim}

or \texttt{tail} to preview the last 6 rows of a data frame:

\begin{verbatim}
tail(aboriginal_lang_db)
\end{verbatim}

\begin{verbatim}
## Error: tail() is not supported by sql sources
\end{verbatim}

Additionally, some operations will not work to extract columns or single values from the reference given by the \texttt{tbl} function. Thus, once you have finished your data wrangling of the \texttt{tbl} database reference object, it is advisable to bring it into your local machine's memory using \texttt{collect} as a data frame.

\begin{quote}
Warning: Usually, databases are very big! Reading the object into your local machine may give an error or take a lot of time to run so be careful if you plan to do this!
\end{quote}

\hypertarget{reading-data-from-a-postgresql-database}{%
\subsubsection{Reading data from a PostgreSQL database}\label{reading-data-from-a-postgresql-database}}

PostgreSQL (also called Postgres) is a very popular and open-source option for relational database software. Unlike SQLite, PostgreSQL uses
a client--server database engine, as it was designed to be used and accessed on a network. This means that you have to provide more
information to R when connecting to Postgres databases. The additional information that you need to include when you call the \texttt{dbConnect}
function is listed below:

\begin{itemize}
\tightlist
\item
  \texttt{dbname} - the name of the database (a single PostgreSQL instance can host more than one database)
\item
  \texttt{host} - the URL pointing to where the database is located
\item
  \texttt{port} - the communication endpoint between R and the PostgreSQL database (this is typically 5432 for PostgreSQL)
\item
  \texttt{user} - the username for accessing the database
\item
  \texttt{password} - the password for accessing the database
\end{itemize}

Additionally, we must use the \texttt{RPostgres} package instead of \texttt{RSQLite} in the \texttt{dbConnect} function call.
Below we demonstrate how to connect to a version of the \texttt{can\_mov\_db} database, which contains information about Canadian movies (\emph{note - this is a synthetic, or artificial, database}).

\begin{verbatim}
library(RPostgres)
can_mov_db_con <- dbConnect(RPostgres::Postgres(), dbname = "can_mov_db",
                        host = "r7k3-mds1.stat.ubc.ca", port = 5432,
                        user = "user0001", password = '################')
\end{verbatim}

\hypertarget{interacting-with-a-database}{%
\subsection{Interacting with a database}\label{interacting-with-a-database}}

After opening the connection, everything looks and behaves almost identically to when we were using an SQLite database in R. For example, we can again
use \texttt{dbListTables} to find out what tables are in the \texttt{can\_mov\_db} database:

\begin{verbatim}
dbListTables(can_mov_db_con)
\end{verbatim}

\begin{verbatim}
 [1] "themes"            "medium"           "titles"     "title_aliases"       "forms"            
 [6] "episodes"          "names"      "names_occupations" "occupation"       "ratings" 
\end{verbatim}

We see that there are 10 tables in this database. Let's first look at the \texttt{"ratings"} table to find the lowest rating that exists in the \texttt{can\_mov\_db} database:

\begin{verbatim}
ratings_db <- tbl(can_mov_db_con, "ratings")
ratings_db
\end{verbatim}

\begin{verbatim}
# Source:   table<ratings> [?? x 3]
# Database: postgres [user0001@r7k3-mds1.stat.ubc.ca:5432/can_mov_db]
   title              average_rating num_votes
   <chr>                    <dbl>     <int>
 1 The Grand Seduction       6.6       150
 2 Rhymes for Young Ghouls   6.3      1685
 3 Mommy                     7.5      1060
 4 Incendies                 6.1      1101
 5 Bon Cop, Bad Cop          7.0       894
 6 Goon                      5.5      1111
 7 Monsieur Lazhar           5.6       610
 8 What if                   5.3      1401
 9 The Barbarian Invations   5.8        99
10 Away from Her             6.9      2311
# … with more rows
\end{verbatim}

To find the lowest rating that exists in the data base, we first need to extract the \texttt{average\_rating} column using \texttt{select}:

\begin{verbatim}
avg_rating_db <- select(ratings_db, average_rating)
avg_rating_db
\end{verbatim}

\begin{verbatim}
# Source:   lazy query [?? x 1]
# Database: postgres [user0001@r7k3-mds1.stat.ubc.ca:5432/can_mov_db]
   average_rating
            <dbl>
 1            6.6
 2            6.3
 3            7.5
 4            6.1
 5            7.0
 6            5.5
 7            5.6
 8            5.3
 9            5.8
10            6.9
# … with more rows
\end{verbatim}

Next we use \texttt{min} to find the minimum rating in that column:

\begin{verbatim}
min(avg_rating_db)
\end{verbatim}

\begin{verbatim}
Error in min(avg_rating_db) : invalid 'type' (list) of argument
\end{verbatim}

Instead of the minimum, we get an error! This is another example of when we need to use the \texttt{collect} function to bring the data into R for further computation:

\begin{verbatim}
avg_rating_data <- collect(avg_rating_db)
min(avg_rating_data)
\end{verbatim}

\begin{verbatim}
[1] 1
\end{verbatim}

We see the lowest rating given to a movie is 1, indicating that it must have been a really bad movie\ldots{}

\textbf{Why should we bother with databases at all?}

Opening a database stored in a \texttt{.db} file involved a lot more effort than just opening a \texttt{.csv}, \texttt{.tsv}, or any of the other plain text or Excel formats. It was a bit of a pain to use a database in that setting since we had to use \texttt{dbplyr} to translate \texttt{tidyverse}-like commands (\texttt{filter}, \texttt{select}, \texttt{head}, etc.) into SQL commands that the database understands. Not all \texttt{tidyverse} commands can currently be translated with SQLite databases. For example, we can compute a mean with an SQLite database but can't easily compute a median. So you might be wondering why should we use databases at all?

Databases are beneficial in a large-scale setting:

\begin{itemize}
\tightlist
\item
  they enable storing large data sets across multiple computers with automatic redundancy and backups
\item
  they allow multiple users to access them simultaneously and remotely without conflicts and errors
\item
  they provide mechanisms for ensuring data integrity and validating input
\item
  they provide security to keep data safe
  For example, \href{https://www.internetlivestats.com/google-search-statistics/}{there are billions of Google searches conducted daily}. Can you imagine if Google stored all of the data from those queries in a single \texttt{.csv\ file}!? Chaos would ensue!
\end{itemize}

\hypertarget{writing-data-from-r-to-a-.csv-file}{%
\section{\texorpdfstring{Writing data from R to a \texttt{.csv} file}{Writing data from R to a .csv file}}\label{writing-data-from-r-to-a-.csv-file}}

At the middle and end of a data analysis, we often want to write a data frame that has changed (either through filtering, selecting, mutating or summarizing) to a file
to share it with others or use it for another step in the analysis. The most straightforward way to do this is to use the \texttt{write\_csv} function from the \texttt{tidyverse} package.
The default arguments for this file are to use a comma (\texttt{,}) as the delimiter and include column names. Below we demonstrate creating a new version of the Canadian languages data set without the official languages category according to the Canadian 2016 Census, and then writing this to a \texttt{.csv} file:

\begin{verbatim}
no_official_lang_data <- filter(can_lang, category != "Official languages")
write_csv(no_official_lang_data, "data/no_official_languages.csv")
\end{verbatim}

\hypertarget{scraping-data-off-the-web-using-r}{%
\section{Scraping data off the web using R}\label{scraping-data-off-the-web-using-r}}

In the first part of this chapter, we learned how to read in data from plain text files that are usually ``rectangular'' in shape using the \texttt{tidyverse} \texttt{read\_*} functions. Sadly, not all data comes in this simple format, but we can happily use many other tools to read in more messy/wild data formats. One common place people often want/need to read in data from is websites. Such data exists in a non-rectangular format. One quick and easy solution to get this data is to copy and paste it. However, this becomes painstakingly long and boring when there is a lot of data that needs gathering. And any time you start doing a lot of copying and pasting, you will likely introduce errors.

The formal name for gathering non-rectangular data from the web and transforming it into a more useful format for data analysis is \textbf{web scraping}. There are two different ways to do web scraping: 1) screen scraping (similar to copying and pasting from a website, but done in a programmatic way to minimize errors and maximize efficiency) and 2) web APIs (\textbf{a}pplication \textbf{p}rogramming \textbf{i}nterface) (a website that provides a programatic way of returning the data as JSON or XML files via http requests). In this course, we will explore the first method, screen scraping using R's \href{https://github.com/hadley/rvest}{\texttt{rvest} package}.

\hypertarget{html-and-css-selectors}{%
\subsection{HTML and CSS selectors}\label{html-and-css-selectors}}

Before we jump into scraping, let's set up some motivation and learn a little bit about what the ``source code'' of a website looks like. Say we are interested in knowing the average rental price (per square footage) of the most recently available one-bedroom apartments in Vancouver from \url{https://vancouver.craigslist.org}. When we visit the Vancouver Craigslist website and search for one-bedroom apartments, this is what we are shown:

\includegraphics{img/craigslist_human.png}

From that page, it's pretty easy for our human eyes to find the apartment price and square footage. But how can we do this programmatically, so we don't have to copy and paste all these numbers? Well, we have to deal with the webpage source code, which we show a snippet of below (and link to the \href{img/website_source.txt}{entire source code here}):

\begin{verbatim}
        <span class="result-meta">
                <span class="result-price">$800</span>

                <span class="housing">
                    1br -
                </span>

                <span class="result-hood"> (13768 108th Avenue)</span>

                <span class="result-tags">
                    <span class="maptag" data-pid="6786042973">map</span>
                </span>

                <span class="banish icon icon-trash" role="button">
                    <span class="screen-reader-text">hide this posting</span>
                </span>

            <span class="unbanish icon icon-trash red" role="button" aria-hidden="true"></span>
            <a href="#" class="restore-link">
                <span class="restore-narrow-text">restore</span>
                <span class="restore-wide-text">restore this posting</span>
            </a>

        </span>
    </p>
</li>
         <li class="result-row" data-pid="6788463837">

        <a href="https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html" class="result-image gallery" data-ids="1:00U0U_lLWbuS4jBYN,1:00T0T_9JYt6togdOB,1:00r0r_hlMkwxKqoeq,1:00n0n_2U8StpqVRYX,1:00M0M_e93iEG4BRAu,1:00a0a_PaOxz3JIfI,1:00o0o_4VznEcB0NC5,1:00V0V_1xyllKkwa9A,1:00G0G_lufKMygCGj6,1:00202_lutoxKbVTcP,1:00R0R_cQFYHDzGrOK,1:00000_hTXSBn1SrQN,1:00r0r_2toXdps0bT1,1:01616_dbAnv07FaE7,1:00g0g_1yOIckt0O1h,1:00m0m_a9fAvCYmO9L,1:00C0C_8EO8Yl1ELUi,1:00I0I_iL6IqV8n5MB,1:00b0b_c5e1FbpbWUZ,1:01717_6lFcmuJ2glV">
                <span class="result-price">$2285</span>
        </a>

    <p class="result-info">
        <span class="icon icon-star" role="button">
            <span class="screen-reader-text">favorite this post</span>
        </span>

            <time class="result-date" datetime="2019-01-06 12:06" title="Sun 06 Jan 12:06:01 PM">Jan  6</time>


        <a href="https://vancouver.craigslist.org/nvn/apa/d/north-vancouver-luxury-1-bedroom/6788463837.html" data-id="6788463837" class="result-title hdrlnk">Luxury 1 Bedroom CentreView with View - Lonsdale</a>

\end{verbatim}

This is not easy for our human eyeballs to read! However, it is easy for us to use programmatic tools to extract the data we need by specifying which HTML tags (things inside \texttt{\textless{}} and \texttt{\textgreater{}} in the code above). For example, if we look in the code above and search for lines with a price, we can also look at the tags that are near that price and see if there's a common ``word'' we can use that is near the price but doesn't exist on other lines that have the information we are not interested in:

\begin{verbatim}
<span class="result-price">$800</span>
\end{verbatim}

and

\begin{verbatim}
<span class="result-price">$2285</span>
\end{verbatim}

What we can see is there is a special ``word'' here, ``result-price'', which appears only on the lines with prices and not on the other lines (that have information we are not interested in). This special word and the context in which is is used (learned from the other words inside the HTML tag) can be combined to create something called a CSS selector. The CSS selector can then be used by R's \texttt{rvest} package to select the information we want (here price) from the website source code.

Now, many websites are quite large and complex, and so then is their website source code. And as you saw above, it is not easy to read and pick out the special words we want with our human eyeballs. So to make this easier, we will use the SelectorGadget tool. It is an open source tool that simplifies generating and finding CSS selectors. We recommend you use the Chrome web browser to use this tool, and install the \href{https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb}{selector gadget tool from the Chrome Web Store}. Here is a short video on how to install and use the SelectorGadget tool to get a CSS selector for use in web scraping:

From installing and using the selectorgadget as shown in the video above, we get the two CSS selectors \texttt{.housing} and \texttt{.result-price} that we can use to scrape information about the square footage and the rental price, respectively. The selector gadget returns them to us as a comma separated list (here \texttt{.housing\ ,\ .result-price}), which is exactly the format we need to provide to R if we are using more than one CSS selector.

\hypertarget{are-you-allowed-to-scrape-that-website}{%
\subsection{Are you allowed to scrape that website?}\label{are-you-allowed-to-scrape-that-website}}

\textbf{BEFORE} scraping data from the web, you should always check whether or not you are \textbf{ALLOWED} to scrape it! There are two documents that are important for this: the robots.txt file and reading the website's Terms of Service document. The website's Terms of Service document is probably the more important of the two, and so you should look there first. What happens when we look at Craigslist's Terms of Service document? Well we read this:

\emph{``You agree not to copy/collect CL content via robots, spiders, scripts, scrapers, crawlers, or any automated or manual equivalent (e.g., by hand).''}

source: \url{https://www.craigslist.org/about/terms.of.use}

\begin{quote}
Want to learn more about the legalities of web scraping and crawling? Read this interesting blog post titled \href{https://benbernardblog.com/web-scraping-and-crawling-are-perfectly-legal-right/}{``Web Scraping and Crawling Are Perfectly Legal, Right?'' by Benoit Bernard} (this is optional, not required reading).
\end{quote}

So what to do now? Well, we shouldn't scrape Craigslist! Let's instead scrape some data on the population of Canadian cities from Wikipedia (who's \href{https://foundation.wikimedia.org/wiki/Terms_of_Use/en}{Terms of Service document} does not explicilty say do not scrape). In this video below we demonstrate using the selectorgadget tool to get CSS Selectors from \href{https://en.wikipedia.org/wiki/Canada}{Wikipedia's Canada} page to scrape a table that contains city names and their populations from the 2016 Canadian Census:

\hypertarget{using-rvest}{%
\subsection{\texorpdfstring{Using \texttt{rvest}}{Using rvest}}\label{using-rvest}}

Now that we have our CSS selectors we can use \texttt{rvest} R package to scrape our desired data from the website. First we start by loading the \texttt{rvest} package:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rvest)}
\end{Highlighting}
\end{Shaded}

Next, we tell R what page we want to scrape by providing the webpage's URL in quotations to the function \texttt{read\_html}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{page }\OtherTok{\textless{}{-}} \FunctionTok{read\_html}\NormalTok{(}\StringTok{"https://en.wikipedia.org/wiki/Canada"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then we send the page object to the \texttt{html\_nodes} function. We also provide that function with the CSS selectors we obtained from the selectorgadget tool. These should be surrounded by quotations. The \texttt{html\_nodes} function select nodes from the HTML document using CSS selectors. Nodes are the HTML tag pairs as well as the content between the tags. For our CSS selector \texttt{td:nth-child(5)} and example node that would be selected would be: \texttt{\textless{}td\ style="text-align:left;background:\#f0f0f0;"\textgreater{}\textless{}a\ href="/wiki/London,\_Ontario"\ title="London,\ Ontario"\textgreater{}London\textless{}/a\textgreater{}\textless{}/td\textgreater{}}

We will use \texttt{head()} here to limit the print output of these vectors to 6 lines.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population\_nodes }\OtherTok{\textless{}{-}} \FunctionTok{html\_nodes}\NormalTok{(page, }\StringTok{"td:nth{-}child(5) , td:nth{-}child(7) , .infobox:nth{-}child(122) td:nth{-}child(1) , .infobox td:nth{-}child(3)"}\NormalTok{)}
\FunctionTok{head}\NormalTok{(population\_nodes)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## {xml_nodeset (6)}
## [1] <td style="text-align:right;">5,928,040</td>
## [2] <td style="text-align:left;background:#f0f0f0;"> ...
## [3] <td style="text-align:right;">494,069\n</td>
## [4] <td style="text-align:right;">4,098,927</td>
## [5] <td style="text-align:left;background:#f0f0f0;"> ...
## [6] <td style="text-align:right;">406,074\n</td>
\end{verbatim}

Next we extract the meaningful data from the HTML nodes using the \texttt{html\_text} function. For our example, this functions only required argument is the an html\_nodes object, which we named \texttt{rent\_nodes}. In the case of this example node: \texttt{\textless{}td\ style="text-align:left;background:\#f0f0f0;"\textgreater{}\textless{}a\ href="/wiki/London,\_Ontario"\ title="London,\ Ontario"\textgreater{}London\textless{}/a\textgreater{}\textless{}/td\textgreater{}}, the \texttt{html\_text} function would return \texttt{London}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population\_text }\OtherTok{\textless{}{-}} \FunctionTok{html\_text}\NormalTok{(population\_nodes)}
\FunctionTok{head}\NormalTok{(population\_text)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "5,928,040"              "London"                
## [3] "494,069\n"              "4,098,927"             
## [5] "St. Catharines–Niagara" "406,074\n"
\end{verbatim}

Are we done? Not quite\ldots{} If you look at the data closely you see that the data is not in an optimal format for data analysis. Both the city names and population are encoded as characters in a single vector instead of being in a data frame with one character column for city and one numeric column for population (think of how you would organize the data in a spreadsheet). Additionally, the populations contain commas (not useful for programmatically dealing with numbers), and some even contain a line break character at the end (\texttt{\textbackslash{}n}). Next chapter we will learn more about data wrangling using R so that we can easily clean up this data with a few lines of code.

\hypertarget{additional-resources}{%
\section{Additional resources}\label{additional-resources}}

\begin{itemize}
\tightlist
\item
  \href{https://r4ds.had.co.nz/data-import.html}{Data import chapter} from \href{https://r4ds.had.co.nz/}{R for Data Science} by Garrett Grolemund \& Hadley Wickham
\end{itemize}

\hypertarget{wrangling}{%
\chapter{Cleaning and wrangling data}\label{wrangling}}

\hypertarget{overview-1}{%
\section{Overview}\label{overview-1}}

This chapter will be centered around tools for cleaning and wrangling data that move data from its raw format into a format that is suitable for data analysis. They
will be presented in the context of a real world data science application, providing more practice working through a whole case study.

\hypertarget{chapter-learning-objectives-2}{%
\section{Chapter learning objectives}\label{chapter-learning-objectives-2}}

By the end of the chapter, students will be able to:

\begin{itemize}
\tightlist
\item
  define the term ``tidy data''
\item
  discuss the advantages and disadvantages from storing data in a tidy data format
\item
  recall and use the following tidyverse functions and operators for their intended data wrangling tasks:

  \begin{itemize}
  \tightlist
  \item
    \texttt{select}
  \item
    \texttt{filter}
  \item
    \texttt{mutate}
  \item
    \texttt{\%\textgreater{}\%}
  \item
    \texttt{\%in\%}
  \item
    \texttt{pivot\_longer}
  \item
    \texttt{pivot\_wider}
  \item
    \texttt{separate}
  \item
    \texttt{summarize}
  \item
    \texttt{group\_by}
  \item
    \texttt{map}
  \end{itemize}
\end{itemize}

\hypertarget{vectors-and-data-frames}{%
\section{Vectors and Data frames}\label{vectors-and-data-frames}}

At this point, we know how to load data into R from various file formats. Once loaded into R, most of the tools we have learned about for reading data into R represent the data as a data frame. So now we will spend some time learning more about data frames in R so that we have a better understanding
of how we can use and manipulate these objects.

\hypertarget{what-is-a-data-frame}{%
\subsection{What is a data frame?}\label{what-is-a-data-frame}}

Let's first start by defining what a data frame is exactly. From a data perspective, it is a rectangle where the rows are the observations:

\begin{figure}
\includegraphics[width=1\linewidth]{img/obs} \caption{Rows are observations in a data frame}\label{fig:02-obs}
\end{figure}

and the columns are the variables:

\begin{figure}
\includegraphics[width=1\linewidth]{img/vars} \caption{Columns are variables in a data frame}\label{fig:02-vars}
\end{figure}

From a computer programming perspective, in R, a data frame is a special subtype of a list object whose elements (columns) are \emph{vectors}.
For example, the data frame below has 3 elements that are vectors whose names are \texttt{state}, \texttt{year} and \texttt{population}.

\begin{figure}
\includegraphics[width=1\linewidth]{img/vectors} \caption{Data frame with 3 vectors}\label{fig:02-vectors}
\end{figure}

\hypertarget{what-is-a-vector}{%
\subsection{What is a vector?}\label{what-is-a-vector}}

In R, vectors are objects that can contain 1 or more elements. The vector elements are ordered, and they must all be of the same type. Common
example types of vectors are character (e.g., letter or words), numeric (whole numbers and fractions) and logical (e.g., \texttt{TRUE} or \texttt{FALSE}). In
the vector shown below, the elements are of numeric type:

\begin{figure}
\includegraphics[width=1\linewidth]{img/vector} \caption{Example of a numeric type vector}\label{fig:02-vector}
\end{figure}

\hypertarget{how-are-vectors-different-from-a-list}{%
\subsection{How are vectors different from a list?}\label{how-are-vectors-different-from-a-list}}

Lists are also objects in R that have multiple elements. Vectors and lists differ by the requirement of element type consistency. All elements
within a single vector must be of the same type (e.g., all elements are numbers), whereas elements within a single list can be of different
types (e.g., characters, numbers, logicals and even other lists can be elements in the same list).

\begin{figure}
\includegraphics[width=1\linewidth]{img/vec_vs_list} \caption{A vector versus a list}\label{fig:02-vec-vs-list}
\end{figure}

\hypertarget{what-does-this-have-to-do-with-data-frames}{%
\subsection{What does this have to do with data frames?}\label{what-does-this-have-to-do-with-data-frames}}

As mentioned earlier, a data frame is really a special type of list where the elements can only be vectors. Representing data with such an
object enables us to easily work with our data in a rectangular/spreadsheet-like manner, and to have columns/vectors of
different characteristics associated/linked in one object. This is similar to a table in a spreadsheet or a database.

\begin{figure}
\includegraphics[width=1\linewidth]{img/dataframe} \caption{Data frame and vector types}\label{fig:02-dataframe}
\end{figure}

The functions from the \texttt{tidyverse} package that we are using often give us a
special class of data frame, called a tibble. Tibbles have some additional
features and benefits over the built-in data frame object. These include
ability to add grouping (and other useful) attributes, as well as more
predictable type preservation when subsetting. Because tibble is just a data
frame with some added features, we will collectively refer to both built-in R
data frames and tibbles as data frames in this book.

\begin{quote}
You can use the function \texttt{class} on a data object to assess whether a data
frame is a built in R data frame or a tibble. If the data object is a date frame
\texttt{class} will return \texttt{"data.frame"}, whereas if the data object is a tibble it
will return \texttt{"tbl\_df"\ "tbl"\ "data.frame"}. You can easily convert built in R
data frames to tibbles using the \texttt{tidyverse} \texttt{as\_tibble} function.
\end{quote}

\hypertarget{tidy-data}{%
\section{Tidy Data}\label{tidy-data}}

There are many ways a spreadsheet-like data set can be organized. This chapter will focus on the \emph{tidy data} format of organization, and how to make your raw (and likely messy) data tidy. We want to tidy our data because a variety of tools we would like to use in R are designed to work most effectively (and efficiently) with tidy data.

\hypertarget{what-is-tidy-data}{%
\subsection{What is tidy data?}\label{what-is-tidy-data}}

Tidy data satisfy the following three criteria \citep{wickham2014tidy}:

\begin{itemize}
\tightlist
\item
  each row is a single observation,
\item
  each column is a single variable, and
\item
  each value is a single cell (i.e., its row and column position in the data frame is not shared with another value)
\end{itemize}

\begin{figure}
\includegraphics[width=1\linewidth]{img/tidy_data} \caption{Tidy data}\label{fig:02-tidy-image}
\end{figure}

\begin{quote}
\textbf{Definitions to know:}

observation - all of the quantities or a qualities we collect from a given entity/object

variable - any characteristic, number, or quantity that can be measured or collected

value - a single collected quantity or a quality from a given entity/object
\end{quote}

\hypertarget{why-is-tidy-data-important-in-r}{%
\subsection{Why is tidy data important in R?}\label{why-is-tidy-data-important-in-r}}

First, one of R's most popular plotting tool sets, the \texttt{ggplot2} package (which is one of the packages that the \texttt{tidyverse} package loads), expects the data to be in a tidy format. Second, most statistical analysis functions also expect data in a tidy format. Given that both of these tasks are central in almost all data analysis projects, it is well worth spending the time to get your data into a tidy format upfront. Luckily there are many well-designed \texttt{tidyverse} data cleaning/wrangling tools to help you easily tidy your data. Let's explore them now!

\hypertarget{going-from-wide-to-long-or-tidy-using-pivot_longer}{%
\subsection{\texorpdfstring{Going from wide to long (or tidy!) using \texttt{pivot\_longer}}{Going from wide to long (or tidy!) using pivot\_longer}}\label{going-from-wide-to-long-or-tidy-using-pivot_longer}}

One common thing that often has to be done to get data into a tidy format is to combine columns that are really part of the same variable but currently stored in separate columns. Data is often stored in a wider, not tidy, format because this format is often more intuitive for human readability and understanding, and humans create data sets. We can use the function \texttt{pivot\_longer}, which combines columns, thus making the data frame longer and narrower.

To learn how to use \texttt{pivot\_longer}, we will work with a data set called \href{https://ttimbers.github.io/canlang/}{\texttt{region\_lang}}, containing data retrieved from the 2016 Canadian census. For each census metropolitan area, this data set includes counts of how many Canadians cited each language as their mother tongue, the language spoken most often at home/work and which language they know.

We will use \texttt{read\_csv} to import a subset of the \texttt{region\_lang} data called \texttt{region\_lang\_top5\_cities\_wide.csv}, which contains only the counts of how many Canadians cited each language as their mother tongue for five major Canadian cities (Toronto, Montreal, Vancouver, Calgary and Edmonton). Our data set is stored in an untidy format, as shown below:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\NormalTok{lang\_wide }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/region\_lang\_top5\_cities\_wide.csv"}\NormalTok{)}
\NormalTok{lang\_wide}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 7
##    category language Toronto Montréal Vancouver Calgary
##    <chr>    <chr>      <dbl>    <dbl>     <dbl>   <dbl>
##  1 Aborigi~ Aborigi~      80       30        70      20
##  2 Non-Off~ Afrikaa~     985       90      1435     960
##  3 Non-Off~ Afro-As~     360      240        45      45
##  4 Non-Off~ Akan (T~    8485     1015       400     705
##  5 Non-Off~ Albanian   13260     2450      1090    1365
##  6 Aborigi~ Algonqu~       5        5         0       0
##  7 Aborigi~ Algonqu~       5       30         5       5
##  8 Non-Off~ America~     470       50       265     100
##  9 Non-Off~ Amharic     7460      665      1140    4075
## 10 Non-Off~ Arabic     85175   151955     14320   18965
## # ... with 204 more rows, and 1 more variable:
## #   Edmonton <dbl>
\end{verbatim}

What is wrong with our untidy format above? From a data analysis perspective, this format is not ideal because, in this format, the outcome of the variable \emph{region} (Toronto, Montreal, Vancouver, Calgary and Edmonton) is stored as column names. Thus it is not easily accessible for the data analysis functions we will want to apply to our data set. Additionally, the values of the \emph{mother tongue} variable are spread across multiple columns. This will prevent us from doing any desired visualization or statistical tasks until we somehow combine them into one column. For instance, suppose we want to know which languages had the highest number of Canadians reporting it as their mother tongue among all five regions? This question would be very difficult to answer with the data in its current format. It would be much easier to answer if we tidy our data first.

To accomplish this data transformation, we will use the \texttt{tidyverse} function \texttt{pivot\_longer}. To use \texttt{pivot\_longer} we need to specify the:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{data}: the data set
\item
  \texttt{cols} : the names of the columns that we want to combine
\item
  \texttt{names\_to}: the name of a new column that will be created, whose values will come from the \emph{names of the columns} that we want to combine
\item
  \texttt{values\_to}: the name of a new column that will be created, whose values will come from the \emph{values of the columns} we want to combine
\end{enumerate}

For the above example, we use \texttt{pivot\_longer} to combine the Toronto, Montreal, Vancouver, Calgary and Edmonton columns into a single column called \texttt{region}, and create a column called \texttt{mother\_tongue} that contains the count of how many Canadians report each language as their mother tongue for each metropolitan area. We use a colon \texttt{:} between Toronto and Edmonton tells R to select all the columns in between Toronto and Edmonton:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lang\_mother\_tidy }\OtherTok{\textless{}{-}} \FunctionTok{pivot\_longer}\NormalTok{(lang\_wide,}
  \AttributeTok{cols =}\NormalTok{ Toronto}\SpecialCharTok{:}\NormalTok{Edmonton,}
  \AttributeTok{names\_to =} \StringTok{"region"}\NormalTok{,}
  \AttributeTok{values\_to =} \StringTok{"mother\_tongue"}
\NormalTok{)}
\NormalTok{lang\_mother\_tidy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,070 x 4
##    category          language      region mother_tongue
##    <chr>             <chr>         <chr>          <dbl>
##  1 Aboriginal langu~ Aboriginal l~ Toron~            80
##  2 Aboriginal langu~ Aboriginal l~ Montr~            30
##  3 Aboriginal langu~ Aboriginal l~ Vanco~            70
##  4 Aboriginal langu~ Aboriginal l~ Calga~            20
##  5 Aboriginal langu~ Aboriginal l~ Edmon~            25
##  6 Non-Official & N~ Afrikaans     Toron~           985
##  7 Non-Official & N~ Afrikaans     Montr~            90
##  8 Non-Official & N~ Afrikaans     Vanco~          1435
##  9 Non-Official & N~ Afrikaans     Calga~           960
## 10 Non-Official & N~ Afrikaans     Edmon~           575
## # ... with 1,060 more rows
\end{verbatim}

\begin{quote}
\textbf{Splitting code across lines}: In the code above, the call to the \texttt{pivot\_longer} function is split across several lines.\emph{
}This is allowed and encouraged when programming in R when your code line gets too long\emph{
}to read clearly. When doing this, it is important to end the line with a comma \texttt{,} so that R\emph{
}knows the function should continue to the next line.*
\end{quote}

The data above is now tidy because all 3 criteria for tidy data have now been met:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All the variables (\texttt{category}, \texttt{language}, \texttt{region} and \texttt{mother\_tongue}) are now their own columns in the data frame.
\item
  Each observation, i.e., each \texttt{category}, \texttt{language}, \texttt{region}, and count of Canadians where that language is the mother tongue, are in a single row.
\item
  Each value is a single cell, i.e., its row, column position in the data frame is not shared with another value.
\end{enumerate}

\hypertarget{going-from-long-to-wide-using-pivot_wider}{%
\subsection{\texorpdfstring{Going from long to wide using \texttt{pivot\_wider}}{Going from long to wide using pivot\_wider}}\label{going-from-long-to-wide-using-pivot_wider}}

Suppose we have observations spread across multiple rows rather than in a single row. To tidy this data, we can use the function \texttt{pivot\_wider}, which generally increases the number of columns (widens) and decreases the number of rows in a data set.

The data set \texttt{region\_lang\_top5\_cities\_long.csv} contains the number of Canadians reporting the primary language at home and work for five major cities (Toronto, Montreal, Vancouver, Calgary and Edmonton).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lang\_long }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/region\_lang\_top5\_cities\_long.csv"}\NormalTok{)}
\NormalTok{lang\_long}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2,140 x 5
##    region   category      language        type    count
##    <chr>    <chr>         <chr>           <chr>   <dbl>
##  1 Montréal Aboriginal l~ Aboriginal lan~ most_a~    15
##  2 Montréal Aboriginal l~ Aboriginal lan~ most_a~     0
##  3 Toronto  Aboriginal l~ Aboriginal lan~ most_a~    50
##  4 Toronto  Aboriginal l~ Aboriginal lan~ most_a~     0
##  5 Calgary  Aboriginal l~ Aboriginal lan~ most_a~     5
##  6 Calgary  Aboriginal l~ Aboriginal lan~ most_a~     0
##  7 Edmonton Aboriginal l~ Aboriginal lan~ most_a~    10
##  8 Edmonton Aboriginal l~ Aboriginal lan~ most_a~     0
##  9 Vancouv~ Aboriginal l~ Aboriginal lan~ most_a~    15
## 10 Vancouv~ Aboriginal l~ Aboriginal lan~ most_a~     0
## # ... with 2,130 more rows
\end{verbatim}

What is wrong with this format above? In this example, each observation should be a language in a region. However, in the messy data set above, each observation is split across multiple two rows - one where the count for \texttt{most\_at\_home} is recorded and one where the count for \texttt{most\_at\_work} is recorded. Suppose we wanted to visualize the relationship between the number of Canadians reporting their primary language at home and at work. It would be difficult to do that with the data in its current format. To fix this, we will use \texttt{pivot\_wider}, and we need to specify the:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{data}: the data set
\item
  \texttt{names\_from}: the name of a the column from which to take the variable names
\item
  \texttt{values\_from}: the name of the column from which to take the values
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lang\_home\_tidy }\OtherTok{\textless{}{-}} \FunctionTok{pivot\_wider}\NormalTok{(lang\_long,}
  \AttributeTok{names\_from =}\NormalTok{ type,}
  \AttributeTok{values\_from =}\NormalTok{ count}
\NormalTok{)}
\NormalTok{lang\_home\_tidy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,070 x 5
##    region  category  language most_at_home most_at_work
##    <chr>   <chr>     <chr>           <dbl>        <dbl>
##  1 Montré~ Aborigin~ Aborigi~           15            0
##  2 Toronto Aborigin~ Aborigi~           50            0
##  3 Calgary Aborigin~ Aborigi~            5            0
##  4 Edmont~ Aborigin~ Aborigi~           10            0
##  5 Vancou~ Aborigin~ Aborigi~           15            0
##  6 Montré~ Non-Offi~ Afrikaa~           10            0
##  7 Toronto Non-Offi~ Afrikaa~          265            0
##  8 Calgary Non-Offi~ Afrikaa~          505           15
##  9 Edmont~ Non-Offi~ Afrikaa~          300            0
## 10 Vancou~ Non-Offi~ Afrikaa~          520           10
## # ... with 1,060 more rows
\end{verbatim}

The data above is now tidy! We can go through the three criteria again to check that this data is a tidy data set.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  All the variables are their own columns in the data frame, i.e., \texttt{most\_at\_home}, and \texttt{most\_at\_work} have been separated into their own columns in the data frame.
\item
  Each observation, i.e., each \texttt{category}, \texttt{language}, \texttt{region}, \texttt{most\_at\_home} and \texttt{most\_at\_work}, are in a single row.
\item
  Each value is a single cell, i.e., its row, column position in the data frame is not shared with another value.
\end{enumerate}

You might notice that we have the same number of columns in our tidy data set as we did in our messy one. Therefore \texttt{pivot\_wider} didn't really ``widen'' our data as the name suggests. However, if we had more than two categories in the original \texttt{type} column, then we would see the data set ``widen.''

\hypertarget{using-separate-to-deal-with-multiple-delimiters}{%
\subsection{\texorpdfstring{Using \texttt{separate} to deal with multiple delimiters}{Using separate to deal with multiple delimiters}}\label{using-separate-to-deal-with-multiple-delimiters}}

Data are also not considered tidy when multiple values are stored in the same cell, as discussed above. In addition to the previous untidiness we addressed in the earlier versions of this data set, the one we show below is even messier:

the \texttt{Toronto}, \texttt{Montreal}, \texttt{Vancouver}, \texttt{Calgary} and \texttt{Edmonton} columns contain the number of Canadians reporting their primary language at home and work in one column separated by the delimiter ``/''. The column names are the values of a variable, AND each value does not have its own cell! To make this messy data tidy, we'll have to fix both of these issues.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lang\_messy }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/region\_lang\_top5\_cities\_messy.csv"}\NormalTok{)}
\NormalTok{lang\_messy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 7
##    category language Toronto Montréal Vancouver Calgary
##    <chr>    <chr>    <chr>   <chr>    <chr>     <chr>  
##  1 Aborigi~ Aborigi~ 50/0    15/0     15/0      5/0    
##  2 Non-Off~ Afrikaa~ 265/0   10/0     520/10    505/15 
##  3 Non-Off~ Afro-As~ 185/10  65/0     10/0      15/0   
##  4 Non-Off~ Akan (T~ 4045/20 440/0    125/10    330/0  
##  5 Non-Off~ Albanian 6380/2~ 1445/20  530/10    620/25 
##  6 Aborigi~ Algonqu~ 5/0     0/0      0/0       0/0    
##  7 Aborigi~ Algonqu~ 0/0     10/0     0/0       0/0    
##  8 Non-Off~ America~ 720/245 70/0     300/140   85/25  
##  9 Non-Off~ Amharic  3820/55 315/0    540/10    2730/50
## 10 Non-Off~ Arabic   45025/~ 72980/1~ 8680/275  11010/~
## # ... with 204 more rows, and 1 more variable:
## #   Edmonton <chr>
\end{verbatim}

First we'll use \texttt{pivot\_longer} to create two columns, \texttt{region} and \texttt{value}, similar to what we did previously:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lang\_messy\_longer }\OtherTok{\textless{}{-}} \FunctionTok{pivot\_longer}\NormalTok{(lang\_messy,}
  \AttributeTok{cols =}\NormalTok{ Toronto}\SpecialCharTok{:}\NormalTok{Edmonton,}
  \AttributeTok{names\_to =} \StringTok{"region"}\NormalTok{,}
  \AttributeTok{values\_to =} \StringTok{"value"}
\NormalTok{)}
\NormalTok{lang\_messy\_longer}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,070 x 4
##    category              language         region  value
##    <chr>                 <chr>            <chr>   <chr>
##  1 Aboriginal languages  Aboriginal lang~ Toronto 50/0 
##  2 Aboriginal languages  Aboriginal lang~ Montré~ 15/0 
##  3 Aboriginal languages  Aboriginal lang~ Vancou~ 15/0 
##  4 Aboriginal languages  Aboriginal lang~ Calgary 5/0  
##  5 Aboriginal languages  Aboriginal lang~ Edmont~ 10/0 
##  6 Non-Official & Non-A~ Afrikaans        Toronto 265/0
##  7 Non-Official & Non-A~ Afrikaans        Montré~ 10/0 
##  8 Non-Official & Non-A~ Afrikaans        Vancou~ 520/~
##  9 Non-Official & Non-A~ Afrikaans        Calgary 505/~
## 10 Non-Official & Non-A~ Afrikaans        Edmont~ 300/0
## # ... with 1,060 more rows
\end{verbatim}

Then we'll use \texttt{separate} to split the \texttt{value} column into two columns, one that contains only the counts of Canadians that speak each language most at home, and one that contains the counts for most at work for each region. To use \texttt{separate} we need to specify the:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \texttt{data}: the data set
\item
  \texttt{col}: the name of a the column we need to split
\item
  \texttt{into}: a character vector of the new column names we would like to put the split data into
\item
  \texttt{sep}: the separator on which to split
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lang\_no\_delimiter }\OtherTok{\textless{}{-}} \FunctionTok{separate}\NormalTok{(lang\_messy\_longer,}
  \AttributeTok{col =}\NormalTok{ value,}
  \AttributeTok{into =} \FunctionTok{c}\NormalTok{(}\StringTok{"most\_at\_home"}\NormalTok{, }\StringTok{"most\_at\_work"}\NormalTok{),}
  \AttributeTok{sep =} \StringTok{"/"}
\NormalTok{)}
\NormalTok{lang\_no\_delimiter}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,070 x 5
##    category   language region most_at_home most_at_work
##    <chr>      <chr>    <chr>  <chr>        <chr>       
##  1 Aborigina~ Aborigi~ Toron~ 50           0           
##  2 Aborigina~ Aborigi~ Montr~ 15           0           
##  3 Aborigina~ Aborigi~ Vanco~ 15           0           
##  4 Aborigina~ Aborigi~ Calga~ 5            0           
##  5 Aborigina~ Aborigi~ Edmon~ 10           0           
##  6 Non-Offic~ Afrikaa~ Toron~ 265          0           
##  7 Non-Offic~ Afrikaa~ Montr~ 10           0           
##  8 Non-Offic~ Afrikaa~ Vanco~ 520          10          
##  9 Non-Offic~ Afrikaa~ Calga~ 505          15          
## 10 Non-Offic~ Afrikaa~ Edmon~ 300          0           
## # ... with 1,060 more rows
\end{verbatim}

You might notice in the table above the word \texttt{\textless{}chr\textgreater{}} appears beneath each of the column names. The word under the column name indicates the data type of each column. Here all of our variables are ``character'' data types. Recall, a character data type is a letter or a number. In the previous example, \texttt{most\_at\_home} and \texttt{most\_at\_work} were \texttt{\textless{}dbl\textgreater{}} (double) (you can verify this by looking at the tables in the previous sections), which is a numeric data type. This change is due to the delimiter ``/'' when we read in this messy data set. R read the columns in as character types, and it stayed that way after we separated the columns.

Here it makes sense for \texttt{region}, \texttt{category}, and \texttt{language} to be stored as a character type. However, if we want to apply any functions that treat the \texttt{most\_at\_home} and \texttt{most\_at\_work} columns as a number (e.g.~finding the maximum of the column), it won't be possible to do if the variable is stored as a \texttt{character}. R has a variety of data types, but here we will use the function \texttt{mutate} to convert these two columns to an ``numeric'' data type. \texttt{mutate} is a function that will allow us to create a new variable in our data set. We specify the data set in the first argument, and in the proceeding arguments, we specify the function we want to apply (\texttt{as.numeric}) to which columns (\texttt{most\_at\_home}, \texttt{most\_at\_work}). Then we give the mutated variable a new name. Here we are naming the columns the same names (``most\_at\_home'', ``most\_at\_work''), but you can call these mutated variables anything you'd like.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lang\_no\_delimiter }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{(lang\_no\_delimiter,}
  \AttributeTok{most\_at\_home =} \FunctionTok{as.numeric}\NormalTok{(most\_at\_home),}
  \AttributeTok{most\_at\_work =} \FunctionTok{as.numeric}\NormalTok{(most\_at\_work)}
\NormalTok{)}
\NormalTok{lang\_no\_delimiter}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,070 x 5
##    category   language region most_at_home most_at_work
##    <chr>      <chr>    <chr>         <dbl>        <dbl>
##  1 Aborigina~ Aborigi~ Toron~           50            0
##  2 Aborigina~ Aborigi~ Montr~           15            0
##  3 Aborigina~ Aborigi~ Vanco~           15            0
##  4 Aborigina~ Aborigi~ Calga~            5            0
##  5 Aborigina~ Aborigi~ Edmon~           10            0
##  6 Non-Offic~ Afrikaa~ Toron~          265            0
##  7 Non-Offic~ Afrikaa~ Montr~           10            0
##  8 Non-Offic~ Afrikaa~ Vanco~          520           10
##  9 Non-Offic~ Afrikaa~ Calga~          505           15
## 10 Non-Offic~ Afrikaa~ Edmon~          300            0
## # ... with 1,060 more rows
\end{verbatim}

Now we see \texttt{\textless{}dbl\textgreater{}} appears under our columns, \texttt{most\_at\_home} and \texttt{most\_at\_work}, indicating they are double data types (which is one of the sub-types of numeric)!

Is this data now tidy? Well, if we recall the three criteria for tidy data:

\begin{itemize}
\tightlist
\item
  each row is a single observation,
\item
  each column is a single variable, and
\item
  each value is a single cell.
\end{itemize}

We can see that this data now satisfies all three criteria, making it easier to analyze.
For example, we could visualize how many people speak each of Canada's two
official languages (English and French) as their primary language at home in
these 5 regions. To do this, we first need to filter the data set for the
rows that list the category as ``Official languages'', and then we can again use
\texttt{ggplot} to create our data visualization. Here we create a bar chart to
represent the counts for each region, and colour the counts by language.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{official\_langs }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(lang\_no\_delimiter, category }\SpecialCharTok{==} \StringTok{"Official languages"}\NormalTok{)}

\FunctionTok{ggplot}\NormalTok{(official\_langs, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ region, }\AttributeTok{y =}\NormalTok{ most\_at\_work, }\AttributeTok{fill =}\NormalTok{ language)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{values =} \FunctionTok{c}\NormalTok{(}\StringTok{"deepskyblue2"}\NormalTok{, }\StringTok{"firebrick1"}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Region"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}
    \AttributeTok{name =} \StringTok{"Number of Canadians reporting their primary language at home"}\NormalTok{,}
    \AttributeTok{labels =}\NormalTok{ scales}\SpecialCharTok{::}\NormalTok{comma}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+} \CommentTok{\# making bars horizontal}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/02-plot-1.pdf}

From this visualization, we can see that in Calgary, Edmonton, Toronto and Vancouver, English was reported as the most common primary language used at home compared to French. However, in Montreal, French was reported as the most common primary language used at home over English.

\hypertarget{notes-on-defining-tidy-data}{%
\subsection{Notes on defining tidy data}\label{notes-on-defining-tidy-data}}

Is there only one shape for tidy data for a given data set? Not necessarily! It depends on the statistical question you are asking and what the variables are for that question. For tidy data, each variable should be its own column. So, just as it's essential to match your statistical question with the appropriate data analysis tool (classification, clustering, hypothesis testing, etc.). It's important to match your statistical question with the appropriate variables and ensure they are represented as individual columns to make the data tidy.

\hypertarget{combining-functions-using-the-pipe-operator}{%
\section{\texorpdfstring{Combining functions using the pipe operator, \texttt{\%\textgreater{}\%}:}{Combining functions using the pipe operator, \%\textgreater\%:}}\label{combining-functions-using-the-pipe-operator}}

In R, we often have to call multiple functions in a sequence to process a data frame. The basic ways of doing this can become quickly unreadable if there are many steps. For example, suppose we need to perform three operations on
a data frame \texttt{data}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  add a new column \texttt{new\_col} that is double another \texttt{old\_col}
\item
  filter for rows where another column, \texttt{other\_col}, is more than 5, and
\item
  select only the new column \texttt{new\_col} for those rows.
\end{enumerate}

One way of doing is to just write multiple lines of code, storing temporary objects as you go:

\begin{verbatim}
output_1 <- mutate(data, new_col = old_col * 2)
output_2 <- filter(output_1, other_col > 5)
output <- select(output_2, new_col)
\end{verbatim}

This is difficult to understand for multiple reasons. The reader may be tricked into thinking the named \texttt{output\_1} and \texttt{output\_2}
objects are important for some reason, while they are just temporary intermediate computations. Further, the reader has to look
through and find where \texttt{output\_1} and \texttt{output\_2} are used in each subsequent line.

Another option for doing this would be to \emph{compose} the functions:

\begin{verbatim}
output <- select(filter(mutate(data, new_col = old_col * 2), other_col > 5), new_col)
\end{verbatim}

Code like this can also be difficult to understand. Functions compose (reading from left to right) in the \emph{opposite order} in which
they are computed by R (above, \texttt{mutate} happens first, then \texttt{filter}, then \texttt{select}). It is also just a really long line of code
to read in one go.

The \emph{pipe operator} \texttt{\%\textgreater{}\%} solves this problem, resulting in cleaner and easier-to-follow code. The below accomplishes the same thing as the previous two code blocks:

\begin{verbatim}
output <- data %>% 
            mutate(new_col = old_col * 2) %>%
            filter(other_col > 5) %>%
            select(new_col)
\end{verbatim}

You can think of the pipe as a physical pipe. It takes the output from the function on the left-hand side of the pipe, and
passes it as the first argument to the function on the right-hand side of the pipe. Note here that we have again split the
code across multiple lines for readability; R is fine with this, since it knows that a line ending in a pipe \texttt{\%\textgreater{}\%} is continued
on the next line. Similarly, you see that after the first pipe, the remaining
lines are indented until the end of the pipeline. This is not required for the
R code to work, but again is used to aid in improving code readability.

Next, let's learn about the details of using the pipe, and look at some examples of how to use it in data analysis.

\hypertarget{using-to-combine-filter-and-select}{%
\subsection{\texorpdfstring{Using \texttt{\%\textgreater{}\%} to combine \texttt{filter} and \texttt{select}}{Using \%\textgreater\% to combine filter and select}}\label{using-to-combine-filter-and-select}}

Let's work with our tidy \texttt{lang\_home\_tidy} data set from above, which contains the number of Canadians reporting their primary language at home and work for five major cities (Toronto, Montreal, Vancouver, Calgary and Edmonton):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lang\_home\_tidy}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,070 x 5
##    region  category  language most_at_home most_at_work
##    <chr>   <chr>     <chr>           <dbl>        <dbl>
##  1 Montré~ Aborigin~ Aborigi~           15            0
##  2 Toronto Aborigin~ Aborigi~           50            0
##  3 Calgary Aborigin~ Aborigi~            5            0
##  4 Edmont~ Aborigin~ Aborigi~           10            0
##  5 Vancou~ Aborigin~ Aborigi~           15            0
##  6 Montré~ Non-Offi~ Afrikaa~           10            0
##  7 Toronto Non-Offi~ Afrikaa~          265            0
##  8 Calgary Non-Offi~ Afrikaa~          505           15
##  9 Edmont~ Non-Offi~ Afrikaa~          300            0
## 10 Vancou~ Non-Offi~ Afrikaa~          520           10
## # ... with 1,060 more rows
\end{verbatim}

Suppose we want to create a subset of the data with only the languages and counts of each language spoken most at home for the city of Vancouver. To do this, we can use the functions \texttt{filter} and \texttt{select}. First, we use \texttt{filter} to create a data frame called \texttt{van\_data} that contains only values for Vancouver. We then use \texttt{select} on this data frame to keep only the variables we want:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{van\_data }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(lang\_home\_tidy, region }\SpecialCharTok{==} \StringTok{"Vancouver"}\NormalTok{)}
\NormalTok{van\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 5
##    region  category  language most_at_home most_at_work
##    <chr>   <chr>     <chr>           <dbl>        <dbl>
##  1 Vancou~ Aborigin~ Aborigi~           15            0
##  2 Vancou~ Non-Offi~ Afrikaa~          520           10
##  3 Vancou~ Non-Offi~ Afro-As~           10            0
##  4 Vancou~ Non-Offi~ Akan (T~          125           10
##  5 Vancou~ Non-Offi~ Albanian          530           10
##  6 Vancou~ Aborigin~ Algonqu~            0            0
##  7 Vancou~ Aborigin~ Algonqu~            0            0
##  8 Vancou~ Non-Offi~ America~          300          140
##  9 Vancou~ Non-Offi~ Amharic           540           10
## 10 Vancou~ Non-Offi~ Arabic           8680          275
## # ... with 204 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{van\_data\_selected }\OtherTok{\textless{}{-}} \FunctionTok{select}\NormalTok{(van\_data, language, most\_at\_home)}
\NormalTok{van\_data\_selected}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 2
##    language                       most_at_home
##    <chr>                                 <dbl>
##  1 Aboriginal languages, n.o.s.             15
##  2 Afrikaans                               520
##  3 Afro-Asiatic languages, n.i.e.           10
##  4 Akan (Twi)                              125
##  5 Albanian                                530
##  6 Algonquian languages, n.i.e.              0
##  7 Algonquin                                 0
##  8 American Sign Language                  300
##  9 Amharic                                 540
## 10 Arabic                                 8680
## # ... with 204 more rows
\end{verbatim}

Although this is valid code, there is a more readable approach we could take by using the pipe, \texttt{\%\textgreater{}\%}. With the pipe, we do not need to create an intermediate object to store the output from \texttt{filter}. Instead we can directly send the
output of \texttt{filter} to the input of \texttt{select}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{van\_data\_selected }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(lang\_home\_tidy, region }\SpecialCharTok{==} \StringTok{"Vancouver"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(language, most\_at\_home)}
\NormalTok{van\_data\_selected}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 214 x 2
##    language                       most_at_home
##    <chr>                                 <dbl>
##  1 Aboriginal languages, n.o.s.             15
##  2 Afrikaans                               520
##  3 Afro-Asiatic languages, n.i.e.           10
##  4 Akan (Twi)                              125
##  5 Albanian                                530
##  6 Algonquian languages, n.i.e.              0
##  7 Algonquin                                 0
##  8 American Sign Language                  300
##  9 Amharic                                 540
## 10 Arabic                                 8680
## # ... with 204 more rows
\end{verbatim}

But wait - why does our \texttt{select} function call look different in these two examples? When you use the pipe,
the output of the function on the left is automatically provided as the first argument for the function on the right, and thus you do not specify that argument in that function call. In the code above, the first
argument of \texttt{select} is the data frame we are \texttt{select}-ing from, which is provided by the output of \texttt{filter}.

As you can see, both of these approaches give us the same output, but the second approach is more clear and readable.

\hypertarget{using-with-more-than-two-functions}{%
\subsection{\texorpdfstring{Using \texttt{\%\textgreater{}\%} with more than two functions}{Using \%\textgreater\% with more than two functions}}\label{using-with-more-than-two-functions}}

The \texttt{\%\textgreater{}\%} can be used with any function in R. Additionally, we can pipe together more than two functions. For example, we can pipe together three functions to order the rows by counts of the language most spoken at home for only the counts that are more than 10,000 and only include the region, language and count of Canadians reporting their primary language at home in our table.

To order the by counts of the language most spoken at home we will use another
\texttt{tidyverse} function, \texttt{arrange}. This function takes column names as input and
orders the rows in the data frame in ascending order based on the values in the
columns. Here we use only one column for sorting (\texttt{most\_at\_home}), but more than
one can also be used. To do this, list additional columns separated by commas.
The order they are listed in indicates the order in which they will be used for
sorting. This is much like how an English dictionary sorts words: first by the
first letter, then by the second letter, and so on. \emph{Note: If you want to sort
in reverse order, you can pair a function called \texttt{desc} with \texttt{arrange} (e.g.,
\texttt{arrange(desc(column\_name))}).}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{large\_region\_lang }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(lang\_home\_tidy, most\_at\_home }\SpecialCharTok{\textgreater{}} \DecValTok{10000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(region, language, most\_at\_home) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(most\_at\_home)}
\NormalTok{large\_region\_lang}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 67 x 3
##    region    language most_at_home
##    <chr>     <chr>           <dbl>
##  1 Edmonton  Arabic          10590
##  2 Montréal  Tamil           10670
##  3 Vancouver Russian         10795
##  4 Edmonton  Spanish         10880
##  5 Edmonton  French          10950
##  6 Calgary   Arabic          11010
##  7 Calgary   Urdu            11060
##  8 Vancouver Hindi           11235
##  9 Montréal  Armenian        11835
## 10 Toronto   Romanian        12200
## # ... with 57 more rows
\end{verbatim}

\begin{quote}
\textbf{Note:} You might also have noticed that we split the function calls across lines after the pipe, similar as to
when we did this earlier in the chapter for long function calls. Again this is allowed and recommeded, especially when the
piped function calls would create a long line of code. Doing this makes your code more readable. When you do this it is important
to end each line with the pipe operator \texttt{\%\textgreater{}\%} to tell R that your code is continuing onto the next line.
\end{quote}

\hypertarget{iterating-over-data-with-group_by-summarize}{%
\section{\texorpdfstring{Iterating over data with \texttt{group\_by} + \texttt{summarize}}{Iterating over data with group\_by + summarize}}\label{iterating-over-data-with-group_by-summarize}}

\hypertarget{calculating-summary-statistics}{%
\subsection{Calculating summary statistics:}\label{calculating-summary-statistics}}

As a part of many data analyses, we need to calculate a summary value for the data (a summary statistic). A useful \texttt{dplyr} function for doing this is
\texttt{summarize}. Examples of summary statistics we might want to calculate are the number of observations, the average/mean value
for a column, the minimum value for a column, etc. Below we show how to use the \texttt{summarize} function to calculate the minimum and maximum
number of Canadians reporting a particular language as their primary language at home:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lang\_summary }\OtherTok{\textless{}{-}} \FunctionTok{summarize}\NormalTok{(lang\_home\_tidy,}
  \AttributeTok{min\_most\_at\_home =} \FunctionTok{min}\NormalTok{(most\_at\_home),}
  \AttributeTok{most\_most\_at\_home =} \FunctionTok{max}\NormalTok{(most\_at\_home)}
\NormalTok{)}
\NormalTok{lang\_summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 2
##   min_most_at_home most_most_at_home
##              <dbl>             <dbl>
## 1                0           3836770
\end{verbatim}

From this we see that there are some languages in the data set the no one speaks
as their primary language at home, as well as that the most commonly spoken
primary language at home is spoken by 3,836,770
people.

\hypertarget{calculating-group-summary-statistics}{%
\subsection{Calculating group summary statistics:}\label{calculating-group-summary-statistics}}

A common pairing with \texttt{summarize} is \texttt{group\_by}. Pairing these functions together can let you summarize values for subgroups within a data set. For example, here, we can use \texttt{group\_by} to group the regions and then calculate the minimum and maximum number of Canadians reporting the language as the primary language at home for each of the groups.

The \texttt{group\_by} function takes at least two arguments. The first is the data frame that will be grouped, and the second and onwards are columns to use in the grouping. Here we use only one column for grouping (\texttt{region}), but more than one can also be used. To do this, list additional columns separated by commas.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lang\_summary\_by\_region }\OtherTok{\textless{}{-}} \FunctionTok{group\_by}\NormalTok{(lang\_home\_tidy, region) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}
    \AttributeTok{min\_most\_at\_home =} \FunctionTok{min}\NormalTok{(most\_at\_home),}
    \AttributeTok{max\_most\_at\_home =} \FunctionTok{max}\NormalTok{(most\_at\_home)}
\NormalTok{  )}
\NormalTok{lang\_summary\_by\_region}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 3
##   region    min_most_at_home max_most_at_home
##   <chr>                <dbl>            <dbl>
## 1 Calgary                  0          1065070
## 2 Edmonton                 0          1050410
## 3 Montréal                 0          2669195
## 4 Toronto                  0          3836770
## 5 Vancouver                0          1622735
\end{verbatim}

\hypertarget{additional-reading-on-the-dplyr-functions}{%
\subsection{\texorpdfstring{Additional reading on the \texttt{dplyr} functions}{Additional reading on the dplyr functions}}\label{additional-reading-on-the-dplyr-functions}}

As we breifly mentioned earlier in a note, the \texttt{tidyverse} is actually a \emph{meta R package}: it installs and loads a collection of R packages that all follow
the tidy data philosophy we discussed above. One of the \texttt{tidyverse} packages is \texttt{dplyr} - a data wrangling workhorse. You have already met six of
the dplyr function (\texttt{select}, \texttt{filter}, \texttt{mutate}, \texttt{arrange}, \texttt{summarize}, and \texttt{group\_by}). To learn more about those six and meet a few more
useful functions, we recommend you checkout \href{http://stat545.com/block010_dplyr-end-single-table.html\#where-were-we}{this chapter} of the Stat 545 book.

\hypertarget{using-purrrs-map-functions-to-iterate}{%
\section{\texorpdfstring{Using \texttt{purrr}'s \texttt{map*} functions to iterate}{Using purrr's map* functions to iterate}}\label{using-purrrs-map-functions-to-iterate}}

Where should you turn when you discover the next step in your data wrangling/cleaning process requires you to apply a function to
each column in a data frame? For example, if you wanted to know the maximum value of each column in a data frame? Well, you could use \texttt{summarize}
as discussed above. However, this becomes inconvenient when you have many columns, as \texttt{summarize} requires you to type out a column name and a data
transformation for each summary statistic you want to calculate.

In cases like this, where you want to apply the same data transformation to all columns, it is more efficient to use \texttt{purrr}'s \texttt{map} function to
apply it to each column. For example, let's find the maximum value of each column of the complete \texttt{region\_lang} data frame by using \texttt{map} with the \texttt{max} function. First, let's peak at the data to familiarize ourselves with it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{region\_lang }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/region\_lang.csv"}\NormalTok{)}
\NormalTok{region\_lang}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 7,490 x 7
##    region category language mother_tongue most_at_home
##    <chr>  <chr>    <chr>            <dbl>        <dbl>
##  1 St. J~ Aborigi~ Aborigi~             5            0
##  2 Halif~ Aborigi~ Aborigi~             5            0
##  3 Monct~ Aborigi~ Aborigi~             0            0
##  4 Saint~ Aborigi~ Aborigi~             0            0
##  5 Sague~ Aborigi~ Aborigi~             5            5
##  6 Québec Aborigi~ Aborigi~             0            5
##  7 Sherb~ Aborigi~ Aborigi~             0            0
##  8 Trois~ Aborigi~ Aborigi~             0            0
##  9 Montr~ Aborigi~ Aborigi~            30           15
## 10 Kings~ Aborigi~ Aborigi~             0            0
## # ... with 7,480 more rows, and 2 more variables:
## #   most_at_work <dbl>, lang_known <dbl>
\end{verbatim}

Next, we will select only the numeric columns of the data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{region\_lang\_numeric }\OtherTok{\textless{}{-}}\NormalTok{ region\_lang }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(mother\_tongue}\SpecialCharTok{:}\NormalTok{lang\_known)}
\NormalTok{region\_lang\_numeric}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 7,490 x 4
##    mother_tongue most_at_home most_at_work lang_known
##            <dbl>        <dbl>        <dbl>      <dbl>
##  1             5            0            0          0
##  2             5            0            0          0
##  3             0            0            0          0
##  4             0            0            0          0
##  5             5            5            0          0
##  6             0            5            0         20
##  7             0            0            0          0
##  8             0            0            0          0
##  9            30           15            0         10
## 10             0            0            0          0
## # ... with 7,480 more rows
\end{verbatim}

Next, we use \texttt{map} to apply the \texttt{max} function to each column. \texttt{map} takes two arguments, an object (a vector, data frame or list) that you want
to apply the function to, and the function that you would like to apply. Here our arguments will be \texttt{region\_lang\_numeric} and \texttt{max}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{max\_of\_columns }\OtherTok{\textless{}{-}} \FunctionTok{map}\NormalTok{(region\_lang\_numeric, max)}
\NormalTok{max\_of\_columns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $mother_tongue
## [1] 3061820
## 
## $most_at_home
## [1] 3836770
## 
## $most_at_work
## [1] 3218725
## 
## $lang_known
## [1] 5600480
\end{verbatim}

\begin{quote}
\textbf{Note:} \texttt{purrr} is part of the tidyverse, and so like the \texttt{dplyr} and \texttt{ggplot} functions, once we call \texttt{library(tidyverse)} we
do not need to load the \texttt{purrr} package separately.
\end{quote}

Our output looks a bit weird\ldots{} we passed in a data frame, but our output doesn't look like a data frame. As it so happens, it is \emph{not} a
data frame, but rather a plain vanilla list:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{typeof}\NormalTok{(max\_of\_columns)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "list"
\end{verbatim}

So what do we do? Should we convert this to a data frame? We could, but a simpler alternative is to just use a different \texttt{map\_*} function from
the \texttt{purrr} package. There are quite a few to choose from, they all work similarly, and their name reflects the type of output you want from
the mapping operation:

\begin{longtable}[]{@{}ll@{}}
\toprule
\texttt{map} function & Output\tabularnewline
\midrule
\endhead
\texttt{map()} & list\tabularnewline
\texttt{map\_lgl()} & logical vector\tabularnewline
\texttt{map\_int()} & integer vector\tabularnewline
\texttt{map\_dbl()} & double vector\tabularnewline
\texttt{map\_chr()} & character vector\tabularnewline
\texttt{map\_df()} & data frame\tabularnewline
\bottomrule
\end{longtable}

Let's get the columns' maximums again, but this time use the \texttt{map\_df} function to return the output as a data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{max\_of\_columns }\OtherTok{\textless{}{-}} \FunctionTok{map\_df}\NormalTok{(region\_lang\_numeric, max)}
\NormalTok{max\_of\_columns}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 4
##   mother_tongue most_at_home most_at_work lang_known
##           <dbl>        <dbl>        <dbl>      <dbl>
## 1       3061820      3836770      3218725    5600480
\end{verbatim}

Which \texttt{map\_*} function you choose depends on what you want to do with the output; you don't always have to pick \texttt{map\_df}!

What if you need to add other arguments to the functions you want to map? For example, what if there were NA values in our columns that we wanted to know the maximum of?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{region\_with\_nas }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/region\_lang\_with\_nas.csv"}\NormalTok{)}
\NormalTok{region\_with\_nas}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 7,491 x 4
##    mother_tongue most_at_home most_at_work lang_known
##            <dbl>        <dbl>        <dbl>      <dbl>
##  1             5            5           NA         NA
##  2             5            0            0          0
##  3             5            0            0          0
##  4             0            0            0          0
##  5             0            0            0          0
##  6             5            5            0          0
##  7             0            0            0          0
##  8             0            0            0          0
##  9             0            0            0          0
## 10             0            0            0          0
## # ... with 7,481 more rows
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{map\_df}\NormalTok{(region\_with\_nas, max)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 4
##   mother_tongue most_at_home most_at_work lang_known
##           <dbl>        <dbl>        <dbl>      <dbl>
## 1       3061820      3836770           NA         NA
\end{verbatim}

Notice \texttt{map\_df()} returns \texttt{NA} for the \texttt{most\_at\_work} and \texttt{lang\_known} variables since those columns contained NAs in the data frame. Thus, we also need to add the argument \texttt{na.rm\ \ =\ TRUE} to the \texttt{max} function so that we get a more useful value than \texttt{NA} returned (remember that is what happens with many of the built-in R statistical functions when NA's are present\ldots). What we need to do in that case is add these additional arguments to the end of our call to to \texttt{map} and they will be passed to the function that we are mapping. An example of this is shown below:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{map\_df}\NormalTok{(region\_with\_nas, max, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 4
##   mother_tongue most_at_home most_at_work lang_known
##           <dbl>        <dbl>        <dbl>      <dbl>
## 1       3061820      3836770      3218725    5600480
\end{verbatim}

Now \texttt{map\_df()} returns the maximum count for each column ignoring the NAs in the data set!

The \texttt{map\_*} functions are generally quite useful for solving problems involving iteration/repetition. Additionally, their use is not limited to columns
of a data frame; \texttt{map\_*} functions can be used to apply functions to elements of a vector or list, and even to lists of data frames, or nested data frames.

\hypertarget{additional-resources-1}{%
\section{Additional resources}\label{additional-resources-1}}

Grolemund \& Wickham's \href{https://r4ds.had.co.nz/}{R for Data Science} has a number of useful sections that provide additional information:

\begin{itemize}
\tightlist
\item
  \href{https://r4ds.had.co.nz/transform.html}{Data transformation}
\item
  \href{https://r4ds.had.co.nz/tidy-data.html}{Tidy data}
\item
  \href{https://r4ds.had.co.nz/iteration.html\#the-map-functions}{The \texttt{map\_*} functions}
\end{itemize}

\hypertarget{viz}{%
\chapter{Effective data visualization}\label{viz}}

\hypertarget{overview-2}{%
\section{Overview}\label{overview-2}}

This chapter will introduce concepts and tools relating to data visualization beyond what we have seen and practiced so far.
We will focus on guiding principles for effective data visualization and explaining visualizations independent of any particular tool or programming language.
In the process, we will cover some specifics of creating visualizations (scatter plots, bar charts, line graphs, and histograms) for data using R. There
are external references that contain a wealth of additional information on the topic of data visualization:

\begin{itemize}
\tightlist
\item
  Professor Claus Wilke's \href{https://serialmentor.com/dataviz/}{Fundamentals of Data Visualization} has more details on general principles of effective visualizations
\item
  Grolemund \& Wickham's \href{https://r4ds.had.co.nz/}{R for Data Science} chapter on \href{https://r4ds.had.co.nz/data-visualisation.html}{creating visualizations using \texttt{ggplot2}} has
  a more in-depth introduction into the syntax and grammar of plotting with \texttt{ggplot2} specifically
\item
  the \href{https://ggplot2.tidyverse.org/reference/}{ggplot2 reference} has a useful list of useful \texttt{ggplot2} functions
\end{itemize}

\hypertarget{chapter-learning-objectives-3}{%
\section{Chapter learning objectives}\label{chapter-learning-objectives-3}}

\begin{itemize}
\tightlist
\item
  Describe when to use the following kinds of visualizations to answer specific questions using a dataset:

  \begin{itemize}
  \tightlist
  \item
    scatter plots
  \item
    line plots
  \item
    bar plots
  \item
    histogram plots
  \end{itemize}
\item
  Given a data set and a question, select from the above plot types and use R to create a visualization that best answers the question
\item
  Given a visualization and a question, evaluate the effectiveness of the visualization and suggest improvements to better answer the question
\item
  Referring to the visualization, communicate the conclusions in nontechnical terms
\item
  Identify rules of thumb for creating effective visualizations
\item
  Define the three key aspects of ggplot objects:

  \begin{itemize}
  \tightlist
  \item
    aesthetic mappings
  \item
    geometric objects
  \item
    scales
  \end{itemize}
\item
  Use the \texttt{ggplot2} package in R to create and refine the above visualizations using:

  \begin{itemize}
  \tightlist
  \item
    geometric objects: \texttt{geom\_point}, \texttt{geom\_line}, \texttt{geom\_histogram}, \texttt{geom\_bar}, \texttt{geom\_vline}, \texttt{geom\_hline}
  \item
    scales: \texttt{scale\_x\_continuous}, \texttt{scale\_y\_continuous}
  \item
    aesthetic mappings: \texttt{x}, \texttt{y}, \texttt{fill}, \texttt{colour}, \texttt{shape}
  \item
    labelling: \texttt{xlab}, \texttt{ylab}, \texttt{labs}
  \item
    font control and legend positioning: \texttt{theme}
  \item
    flipping axes: \texttt{coord\_flip}
  \item
    subplots: \texttt{facet\_grid}
  \end{itemize}
\item
  Describe the difference in raster and vector output formats
\item
  Use \texttt{ggsave} to save visualizations in \texttt{.png} and \texttt{.svg} format
\end{itemize}

\hypertarget{choosing-the-visualization}{%
\section{Choosing the visualization}\label{choosing-the-visualization}}

\hypertarget{ask-a-question-and-answer-it}{%
\subsubsection{\texorpdfstring{\emph{Ask a question, and answer it}}{Ask a question, and answer it}}\label{ask-a-question-and-answer-it}}

The purpose of a visualization is to answer a question about a data set of interest. So naturally, the first thing to do \textbf{before} creating
a visualization is to formulate the question about the data that you are trying to answer.
A good visualization will clearly answer your question without distraction; a \emph{great} visualization will suggest even what the question
was itself without additional explanation. Imagine your visualization as part of a poster presentation for a project; even if you aren't standing
at the poster explaining things, an effective visualization will be able to convey your message to the audience.

Recall \href{index.html\#chapter-learning-objectives}{the different data analysis questions} from the first chapter of this book. With the visualizations
we will cover in this chapter, we will be able to answer \emph{only descriptive and exploratory} questions. Be careful not to try to answer any \emph{predictive, inferential, causal or mechanistic}
questions, as we have not learned the tools necessary to do that properly just yet.

As with most coding tasks, it is totally fine (and quite common) to make mistakes and iterate a few times before you find
the right visualization for your data and question. There are \href{https://serialmentor.com/dataviz/directory-of-visualizations.html}{many different kinds of plotting graphics} available
to use. For the kinds we will introduce in this course, the general rules of thumb are:

\begin{itemize}
\tightlist
\item
  \textbf{line plots} visualize trends with respect to an independent, ordered quantity (e.g.~time)
\item
  \textbf{histograms} visualize the distribution of one quantitative variable (i.e., all its possible values and how often they occur)
\item
  \textbf{scatter plots} visualize the distribution / relationship of two quantitative variables
\item
  \textbf{bar plots} visualize comparisons of amounts
\end{itemize}

All types of visualization have their (mis)uses, but three kinds are usually hard to understand or are easily replaced with an oft-better alternative.
In particular, you should avoid \textbf{pie charts}; it is generally better to use bars, as it is easier to compare bar heights than pie slice sizes.
You should also not use \textbf{3-D visualizations}, as they are typically hard to understand when converted to a static 2-D image format. Finally,
do not use tables to make numerical comparisons; humans are much better at quickly processing visual information than text and math. Bar plots are
again typically a better alternative.

\hypertarget{refining-the-visualization}{%
\section{Refining the visualization}\label{refining-the-visualization}}

\hypertarget{convey-the-message-minimize-noise}{%
\subsubsection{\texorpdfstring{\emph{Convey the message, minimize noise}}{Convey the message, minimize noise}}\label{convey-the-message-minimize-noise}}

Just being able to make a visualization in R with \texttt{ggplot2} (or any other tool for that matter) doesn't mean that it effectively communicates your message to others. Once you have selected a broad type of visualization to use, you will have to refine it to suit your particular need.
Some rules of thumb for doing this are listed below. They generally fall into two classes: you want to \emph{make your
visualization convey your message}, and you want to \emph{reduce visual noise} as much as possible. Humans have limited cognitive ability
to process information; both of these types of refinement aim to reduce the mental load on your audience when viewing your visualization,
making it easier for them to understand and remember your message quickly.

\textbf{Convey the message}

\begin{itemize}
\tightlist
\item
  Make sure the visualization answers the question you have asked most simply and plainly as possible.
\item
  Use legends and labels so that your visualization is understandable without reading the surrounding text.
\item
  Ensure the text, symbols, lines, etc., on your visualization are big enough to be easily read.
\item
  Ensure the data are clearly visible; don't hide the shape/distribution of the data behind other objects (e.g.~a bar).
\item
  Make sure to use colour schemes that are understandable by those with colourblindness (a surprisingly large fraction of the
  overall population). For example, \href{https://colorbrewer2.org}{colorbrewer.org}
  and the \texttt{RColorBrewer} R package provide the ability to pick such colour schemes, and you can check your visualizations after
  you have created them by uploading to online tools such as the \href{https://www.color-blindness.com/coblis-color-blindness-simulator/}{colour blindness simulator}.
\item
  Redundancy can be helpful; sometimes conveying the same message in multiple ways reinforces it for the audience.
\end{itemize}

\textbf{Minimize noise}

\begin{itemize}
\tightlist
\item
  Use colours sparingly. Too many different colours can be distracting, create false patterns, and detract from the message.
\item
  Be wary of overplotting. If your plot has too many dots or lines and starts to look like a mess, you need to do something different.
\item
  Only make the plot area (where the dots, lines, bars are) as big as needed. Simple plots can be made small.
\item
  Don't adjust the axes to zoom in on small differences. If the difference is small, show that it's small!
\end{itemize}

\hypertarget{creating-visualizations-with-ggplot2}{%
\section{\texorpdfstring{Creating visualizations with \texttt{ggplot2}}{Creating visualizations with ggplot2}}\label{creating-visualizations-with-ggplot2}}

\hypertarget{build-the-visualization-iteratively}{%
\subsubsection{\texorpdfstring{\emph{Build the visualization iteratively}}{Build the visualization iteratively}}\label{build-the-visualization-iteratively}}

This section will cover examples of how to choose and refine a visualization given a data set and a question that you want to answer,
and then how to create the visualization in R using \texttt{ggplot2}. To use the \texttt{ggplot2} package, we need to load the \texttt{tidyverse} metapackage.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\hypertarget{the-mauna-loa-co2-data-set}{%
\subsection{The Mauna Loa CO2 data set}\label{the-mauna-loa-co2-data-set}}

The \href{https://www.esrl.noaa.gov/gmd/ccgg/trends/data.html}{Mauna Loa CO2 data set}, curated by \href{https://www.esrl.noaa.gov/gmd/staff/Pieter.Tans/}{Dr.~Pieter Tans, NOAA/GML} and \href{https://scrippsco2.ucsd.edu/}{Dr.~Ralph Keeling, Scripps Institution of Oceanography}
records the atmospheric concentration of carbon dioxide (CO2, in parts per million) at the Mauna Loa research station in Hawaii from 1959 onwards.

\textbf{Question:} Does the concentration of atmospheric CO2 change over time, and are there any interesting patterns to note?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# mauna loa carbon dioxide data}
\NormalTok{co2\_df }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/mauna\_loa.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(ppm }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{, date\_decimal }\SpecialCharTok{\textless{}} \DecValTok{2000}\NormalTok{)}
\NormalTok{co2\_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 495 x 4
##     year month date_decimal   ppm
##    <dbl> <dbl>        <dbl> <dbl>
##  1  1958     3        1958.  316.
##  2  1958     4        1958.  317.
##  3  1958     5        1958.  318.
##  4  1958     7        1959.  316.
##  5  1958     8        1959.  315.
##  6  1958     9        1959.  313.
##  7  1958    11        1959.  313.
##  8  1958    12        1959.  315.
##  9  1959     1        1959.  316.
## 10  1959     2        1959.  316.
## # ... with 485 more rows
\end{verbatim}

Since we are investigating a relationship between two variables (CO2 concentration and date), a scatter plot is a good place to start. Scatter plots
show the data as individual points with \texttt{x} (horizontal axis) and \texttt{y} (vertical axis) coordinates. Here, we will use the decimal
date as the \texttt{x} coordinate
and CO2 concentration as the \texttt{y} coordinate. When using the \texttt{ggplot2} package, we create the plot object with the \texttt{ggplot} function; there are
a few basic aspects of a plot that we need to specify:

\begin{itemize}
\tightlist
\item
  the \emph{data}: the name of the data frame object that we would like to visualize

  \begin{itemize}
  \tightlist
  \item
    here, we specify the \texttt{co2\_df} data frame
  \end{itemize}
\item
  the \emph{aesthetic mapping}: tells \texttt{ggplot} how the columns in the data frame map to properties of the visualization

  \begin{itemize}
  \tightlist
  \item
    to create an aesthetic mapping, we use the \texttt{aes} function
  \item
    here, we set the plot \texttt{x} axis to the \texttt{date\_decimal} variable, and the plot \texttt{y} axis to the \texttt{ppm} variable
  \end{itemize}
\item
  the \emph{geometric object}: specifies how the mapped data should be displayed

  \begin{itemize}
  \tightlist
  \item
    to create a geometric object, we use a \texttt{geom\_*} function (see the \href{https://ggplot2.tidyverse.org/reference/}{ggplot reference} for a list of geometric objects)
  \item
    here, we use the \texttt{geom\_point} function to visualize our data as a scatterplot
  \end{itemize}
\end{itemize}

We could pass many other possible arguments to the aesthetic mapping and geometric object to change how the plot looks. For
the purposes of quickly testing things out to see what they look like, though, we can just go with the default settings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{co2\_scatter }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(co2\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ date\_decimal, }\AttributeTok{y =}\NormalTok{ ppm)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\NormalTok{co2\_scatter}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/03-data-co2-scatter-1.pdf}
\caption{\label{fig:03-data-co2-scatter}Scatter plot of atmospheric concentration of CO2 over time}
\end{figure}

Certainly, the visualization shows a clear upward trend in the atmospheric concentration of CO2 over time.
This plot answers the first part of our question in the affirmative, but that appears to be the only conclusion one can make from the scatter visualization.
However, since time is an ordered quantity, we can try using a line plot instead using the \texttt{geom\_line} function. Line plots require
that their \texttt{x} coordinate orders the data, and connect the sequence of \texttt{x} and \texttt{y} coordinates with line segments. Let's again try this with just
the default arguments:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{co2\_line }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(co2\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ date\_decimal, }\AttributeTok{y =}\NormalTok{ ppm)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{()}
\NormalTok{co2\_line}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/03-data-co2-line-1.pdf}
\caption{\label{fig:03-data-co2-line}Line plot of atmospheric concentration of CO2 over time}
\end{figure}

Aha! There \emph{is} another interesting phenomenon in the data: in addition to increasing over time, the concentration seems to oscillate as well.
Given the visualization as it is now, it is still hard to tell how fast the oscillation is, but nevertheless, the line seems to
be a better choice for answering the question than the scatter plot was. The comparison between these two visualizations illustrates a common issue with
scatter plots: often, the points are shown too close together or even on top of one another, muddling information that would otherwise be clear (\emph{overplotting}).

Now that we have settled on the rough details of the visualization, it is time to refine things. This plot is fairly straightforward, and there is not much
visual noise to remove. But there are a few things we must do to improve clarity, such as adding informative axis labels and making the font a more readable size.
To add axis labels, we use the \texttt{xlab} and \texttt{ylab} functions. To change the font size, we use the \texttt{theme} function with the \texttt{text} argument:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{co2\_line }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(co2\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ date\_decimal, }\AttributeTok{y =}\NormalTok{ ppm)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Year"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Atmospheric CO2 (ppm)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{18}\NormalTok{))}
\NormalTok{co2\_line}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/03-data-co2-line-2-1.pdf}
\caption{\label{fig:03-data-co2-line-2}Line plot of atmospheric concentration of CO2 over time with clearer axes and labels}
\end{figure}

Finally, let's see if we can better understand the oscillation by changing the visualization slightly. Note that it is totally fine to use a small number of visualizations to answer different aspects of the question you are trying to answer. We will accomplish
this by using \emph{scales}, another important feature of \texttt{ggplot2} that easily transforms the different variables and set limits.
We scale the horizontal axis using the \texttt{scale\_x\_continuous} function, and the vertical axis with the \texttt{scale\_y\_continuous} function.
We can transform the axis by passing the \texttt{trans} argument, and set limits by passing the \texttt{limits} argument. In particular, here, we
will use the \texttt{scale\_x\_continuous} function with the \texttt{limits} argument to zoom in on just five years of data (say, 1990-1995):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{co2\_line }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(co2\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ date\_decimal, }\AttributeTok{y =}\NormalTok{ ppm)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Year"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Atmospheric CO2 (ppm)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{limits =} \FunctionTok{c}\NormalTok{(}\DecValTok{1990}\NormalTok{, }\DecValTok{1995}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{18}\NormalTok{))}
\NormalTok{co2\_line}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/03-data-co2-line-3-1.pdf}
\caption{\label{fig:03-data-co2-line-3}Line plot of atmospheric concentration of CO2 from 1990 to 1995 only}
\end{figure}

Interesting! It seems that each year, the atmospheric CO2 increases until it reaches its peak somewhere around April, decreases until around late September,
and finally increases again until the end of the year. In Hawaii, there are two seasons: summer from May through October, and winter from November through April.
Therefore, the oscillating pattern in CO2 matches up fairly closely with the two seasons.

\hypertarget{the-island-landmass-data-set}{%
\subsection{The island landmass data set}\label{the-island-landmass-data-set}}

The \texttt{islands.csv} data set contains a list of Earth's landmasses as well as their area (in thousands of square miles).

\textbf{Question:} Are the continents (North / South America, Africa, Europe, Asia, Australia, Antarctica) Earth's seven largest landmasses? If so, what are the next few largest landmasses after those?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# islands data}
\NormalTok{islands\_df }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/islands.csv"}\NormalTok{)}
\NormalTok{islands\_df}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 48 x 2
##    landmass      size
##    <chr>        <dbl>
##  1 Africa       11506
##  2 Antarctica    5500
##  3 Asia         16988
##  4 Australia     2968
##  5 Axel Heiberg    16
##  6 Baffin         184
##  7 Banks           23
##  8 Borneo         280
##  9 Britain         84
## 10 Celebes         73
## # ... with 38 more rows
\end{verbatim}

Here, we have a list of Earth's landmasses, and are trying to compare their sizes. The right type of visualization to answer this
question is a bar plot, specified by the \texttt{geom\_bar} function in \texttt{ggplot2}. However, by default, \texttt{geom\_bar} sets the heights
of bars to the number of times a value appears in a data frame (its \emph{count}); here, we want to plot exactly the values in the data frame, i.e.,
the landmass sizes. So we have to pass the \texttt{stat\ =\ "identity"} argument to \texttt{geom\_bar}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{islands\_bar }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(islands\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ landmass, }\AttributeTok{y =}\NormalTok{ size)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{)}
\NormalTok{islands\_bar}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/03-data-islands-bar-1.pdf}
\caption{\label{fig:03-data-islands-bar}Bar plot of all Earth's landmasses' size with squished labels}
\end{figure}

Alright, not bad! This plot is definitely the right kind of visualization, as we can clearly see and compare sizes of landmasses. The major issues are that the smaller landmasses' sizes are hard to distinguish, and the names of the landmasses are obscuring each other as they have been squished into too little space. But remember that the question we asked was only about the largest landmasses; let's make the plot a little bit clearer by keeping only the largest 12 landmasses. We do this using the \texttt{top\_n} function.
Then to help us make sure the labels have enough space, we'll use horizontal bars instead of vertical ones. We do this using the
\texttt{coord\_flip} function, which swaps the \texttt{x} and \texttt{y} coordinate axes:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{islands\_top12 }\OtherTok{\textless{}{-}} \FunctionTok{top\_n}\NormalTok{(islands\_df, }\DecValTok{12}\NormalTok{, size)}
\NormalTok{islands\_bar }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(islands\_top12, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ landmass, }\AttributeTok{y =}\NormalTok{ size)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}
\NormalTok{islands\_bar}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/03-data-islands-bar-2-1.pdf}
\caption{\label{fig:03-data-islands-bar-2}Bar plot of size for Earth's largest 12 landmasses}
\end{figure}

This plot is definitely clearer now, and allows us to answer our question (``are the top 7 largest landmasses continents?'') in
the affirmative. But the question could be made clearer from the plot by organizing the bars not by alphabetical order
but by size, and to colour them based on whether they are a continent. To do this, we
use \texttt{mutate} to add a column to the data regarding whether or not the landmass is a continent:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{islands\_top12 }\OtherTok{\textless{}{-}} \FunctionTok{top\_n}\NormalTok{(islands\_df, }\DecValTok{12}\NormalTok{, size)}
\NormalTok{continents }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Africa"}\NormalTok{, }\StringTok{"Antarctica"}\NormalTok{, }\StringTok{"Asia"}\NormalTok{, }\StringTok{"Australia"}\NormalTok{, }\StringTok{"Europe"}\NormalTok{, }\StringTok{"North America"}\NormalTok{, }\StringTok{"South America"}\NormalTok{)}
\NormalTok{islands\_ct }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{(islands\_top12, }\AttributeTok{is\_continent =} \FunctionTok{ifelse}\NormalTok{(landmass }\SpecialCharTok{\%in\%}\NormalTok{ continents, }\StringTok{"Continent"}\NormalTok{, }\StringTok{"Other"}\NormalTok{))}
\NormalTok{islands\_ct}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 12 x 3
##    landmass       size is_continent
##    <chr>         <dbl> <chr>       
##  1 Africa        11506 Continent   
##  2 Antarctica     5500 Continent   
##  3 Asia          16988 Continent   
##  4 Australia      2968 Continent   
##  5 Baffin          184 Other       
##  6 Borneo          280 Other       
##  7 Europe         3745 Continent   
##  8 Greenland       840 Other       
##  9 Madagascar      227 Other       
## 10 New Guinea      306 Other       
## 11 North America  9390 Continent   
## 12 South America  6795 Continent
\end{verbatim}

In order to colour the bars, we add the \texttt{fill} argument to the aesthetic mapping. Then we use the \texttt{reorder}
function in the aesthetic mapping to organize the landmasses by their \texttt{size} variable.
Finally, we use the \texttt{labs} and \texttt{theme} functions to add labels, change the font size, and position the legend:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{islands\_bar }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(islands\_ct, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \FunctionTok{reorder}\NormalTok{(landmass, size), }\AttributeTok{y =}\NormalTok{ size, }\AttributeTok{fill =}\NormalTok{ is\_continent)) }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Landmass"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Size (1000 square mi)"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"Type"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{18}\NormalTok{), }\AttributeTok{legend.position =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.75}\NormalTok{, }\FloatTok{0.45}\NormalTok{))}
\NormalTok{islands\_bar}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/03-data-islands-bar-4-1.pdf}
\caption{\label{fig:03-data-islands-bar-4}Bar plot of size for Earth's largest 12 landmasses coloured by whether its a continent with clearer axes and labels}
\end{figure}

This is now a very effective visualization for answering our original questions. Landmasses are organized by their size,
and continents are coloured differently than other landmasses, making it quite clear that continents are the largest seven landmasses.

\hypertarget{the-old-faithful-eruptionwaiting-time-data-set}{%
\subsection{The Old Faithful eruption/waiting time data set}\label{the-old-faithful-eruptionwaiting-time-data-set}}

The \texttt{faithful} data set contains measurements of the waiting time between eruptions and the subsequent eruption duration (in minutes). The \texttt{faithful} data set is available in base R under the name \texttt{faithful} so it does not need to be loaded.

\textbf{Question:} Is there a relationship between the waiting time before an eruption to the duration of the eruption?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# old faithful eruption time / wait time data}
\NormalTok{faithful}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 272 x 2
##    eruptions waiting
##        <dbl>   <dbl>
##  1      3.6       79
##  2      1.8       54
##  3      3.33      74
##  4      2.28      62
##  5      4.53      85
##  6      2.88      55
##  7      4.7       88
##  8      3.6       85
##  9      1.95      51
## 10      4.35      85
## # ... with 262 more rows
\end{verbatim}

Here again, we investigate the relationship between two quantitative variables (waiting time and eruption time). But if you look at the output of the data frame, you'll notice that neither of the columns are ordered. So, in this case, let's start again with a scatter plot:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{faithful\_scatter }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(faithful, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ waiting, }\AttributeTok{y =}\NormalTok{ eruptions)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}
\NormalTok{faithful\_scatter}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/03-data-faithful-scatter-1.pdf}
\caption{\label{fig:03-data-faithful-scatter}Scatter plot of waiting time and eruption time}
\end{figure}

We can see that the data tend to fall into two groups: one with short waiting and eruption times, and one with long waiting and eruption
times. Note that in this case, there is no overplotting: the points are generally nicely visually separated, and the pattern they form is clear.
In order to refine the visualization, we need only to add axis labels and make the font more readable:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{faithful\_scatter }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(faithful, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ waiting, }\AttributeTok{y =}\NormalTok{ eruptions)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Waiting Time (mins)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Eruption Duration (mins)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{18}\NormalTok{))}
\NormalTok{faithful\_scatter}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/03-data-faithful-scatter-2-1.pdf}
\caption{\label{fig:03-data-faithful-scatter-2}Scatter plot of waiting time and eruption time with clearer axes and labels}
\end{figure}

\hypertarget{the-michelson-speed-of-light-data-set}{%
\subsection{The Michelson speed of light data set}\label{the-michelson-speed-of-light-data-set}}

The \texttt{morley} data set contains measurements of the speed of light (in kilometres per second with 299,000 subtracted) from the year 1879
for five experiments, each with 20 consecutive runs. This data set is available in base R under the name \texttt{morley} so it does not need to be loaded.

\textbf{Question:} Given what we know now about the speed of
light (299,792.458 kilometres per second), how accurate were each of the experiments?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# michelson morley experimental data}
\NormalTok{morley}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 100 x 3
##     Expt   Run Speed
##    <int> <int> <int>
##  1     1     1   850
##  2     1     2   740
##  3     1     3   900
##  4     1     4  1070
##  5     1     5   930
##  6     1     6   850
##  7     1     7   950
##  8     1     8   980
##  9     1     9   980
## 10     1    10   880
## # ... with 90 more rows
\end{verbatim}

In this experimental data, Michelson was trying to measure just a single quantitative number (the speed of light). The data set
contains many measurements of this single quantity. To tell how accurate the experiments were, we need to visualize the
distribution of the measurements (i.e., all their possible values and how often each occurs). We can do this using a \emph{histogram}. A histogram helps us visualize how a
particular variable is distributed in a data set by separating the data into bins, and then using vertical bars
to show how many data points fell in each bin. To create a histogram in \texttt{ggplot2} we will use the \texttt{geom\_histogram} geometric
object, setting the \texttt{x} axis to the \texttt{Speed} measurement variable; and as we did before, let's use the default arguments just to see how things look:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{morley\_hist }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(morley, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Speed)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{()}
\NormalTok{morley\_hist}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/03-data-morley-hist-1.pdf}
\caption{\label{fig:03-data-morley-hist}Histogram of Michelson's speed of light data}
\end{figure}

This is a great start. However, we cannot tell how accurate the measurements are using this visualization unless we can see what the true value is.
In order to visualize the true speed of light, we will add a vertical line with the \texttt{geom\_vline} function, setting the \texttt{xintercept} argument
to the true value. There is a similar function, \texttt{geom\_hline}, that is used for plotting horizontal lines. Note that \emph{vertical lines}
are used to denote quantities on the \emph{horizontal axis}, while \emph{horizontal lines} are used to denote quantities on the \emph{vertical axis}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{morley\_hist }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(morley, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Speed)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FloatTok{792.458}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{size =} \FloatTok{1.0}\NormalTok{)}
\NormalTok{morley\_hist}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/03-data-morley-hist-2-1.pdf}
\caption{\label{fig:03-data-morley-hist-2}Histogram of Michelson's speed of light data with vertical line indicating true speed of light}
\end{figure}

We also still cannot tell which experiments (denoted in the \texttt{Expt} column) led to which measurements; perhaps some experiments were more accurate than
others. To fully answer our question, we need to separate the measurements from each other visually. We can
try to do this using a \emph{coloured} histogram, where counts from different experiments are stacked on top of each
other in different colours. We create a histogram coloured by the \texttt{Expt} variable by adding
it to the \texttt{fill} aesthetic mapping. We make sure the different colours can
be seen (despite them all sitting on top of each other) by setting the \texttt{alpha} argument in \texttt{geom\_histogram}
to \texttt{0.5} to make the bars slightly translucent:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{morley\_hist }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(morley, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Speed, }\AttributeTok{fill =} \FunctionTok{factor}\NormalTok{(Expt))) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FloatTok{792.458}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{size =} \FloatTok{1.0}\NormalTok{)}
\NormalTok{morley\_hist}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/03-data-morley-hist-3-1.pdf}
\caption{\label{fig:03-data-morley-hist-3}Histogram of Michelson's speed of light data coloured by experiment}
\end{figure}

Unfortunately, the attempt to separate out the experiment number visually has created a bit of a mess. All of the colours
are blending together, and although it is possible to derive \emph{some} insight from this (e.g., experiments 1 and 3 had some
of the most incorrect measurements), it isn't the clearest way to convey our message and answer the question. Let's try a different strategy of
creating multiple separate histograms on top of one another.

In order to create a plot in \texttt{ggplot2} that has multiple subplots arranged in a grid, we use the \texttt{facet\_grid} function.
The argument to \texttt{facet\_grid} specifies the variable(s) used to split the plot into subplots. It has the syntax \texttt{vertical\_variable\ \textasciitilde{}\ horizontal\_variable},
where \texttt{veritcal\_variable} is used to split the plot vertically, \texttt{horizontal\_variable} is used to split horizontally, and \texttt{.} is used if there should be no split
along that axis. In our case, we only want to split vertically along the \texttt{Expt} variable, so we use \texttt{Expt\ \textasciitilde{}\ .} as the argument to \texttt{facet\_grid}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{morley\_hist }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(morley, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Speed, }\AttributeTok{fill =} \FunctionTok{factor}\NormalTok{(Expt))) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{(Expt }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \FloatTok{792.458}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{size =} \FloatTok{1.0}\NormalTok{)}
\NormalTok{morley\_hist}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/03-data-morley-hist-4-1.pdf}
\caption{\label{fig:03-data-morley-hist-4}Histogram of Michelson's speed of light data split vertically by experiment}
\end{figure}

The visualization now makes it quite clear how accurate the different experiments were with respect to one another.
There are two finishing touches to make this visualization even clearer. First and foremost, we need to add informative axis labels
using the \texttt{labs} function, and increase the font size to make it readable using the \texttt{theme} function. Second, and perhaps more subtly, even though it
is easy to compare the experiments on this plot to one another, it is hard to get a sense
for just how accurate all the experiments were overall. For example, how accurate is the value 800 on the plot, relative to the true speed of light?
To answer this question, we'll use the \texttt{mutate} function to transform our data into a relative measure of accuracy rather than absolute measurements:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{morley\_rel }\OtherTok{\textless{}{-}} \FunctionTok{mutate}\NormalTok{(morley, }\AttributeTok{relative\_accuracy =} \DecValTok{100} \SpecialCharTok{*}\NormalTok{ ((}\DecValTok{299000} \SpecialCharTok{+}\NormalTok{ Speed) }\SpecialCharTok{{-}} \FloatTok{299792.458}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ (}\FloatTok{299792.458}\NormalTok{))}
\NormalTok{morley\_hist }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(morley\_rel, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ relative\_accuracy, }\AttributeTok{fill =} \FunctionTok{factor}\NormalTok{(Expt))) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{position =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_grid}\NormalTok{(Expt }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{, }\AttributeTok{size =} \FloatTok{1.0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Relative Accuracy (\%)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"\# Measurements"}\NormalTok{, }\AttributeTok{fill =} \StringTok{"Experiment ID"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{text =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{size =} \DecValTok{18}\NormalTok{))}
\NormalTok{morley\_hist}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/03-data-morley-hist-5-1.pdf}
\caption{\label{fig:03-data-morley-hist-5}Histogram of relative accuracy split vertically by experiment with clearer axes and labels}
\end{figure}

Wow, impressive! These measurements of the speed of light from 1879 had errors around \emph{0.05\%} of the true speed. This shows
you that even though experiments 2 and 5 were perhaps the most accurate, all of the experiments did quite an
admirable job given the technology available at the time.

\hypertarget{explaining-the-visualization}{%
\section{Explaining the visualization}\label{explaining-the-visualization}}

\hypertarget{tell-a-story}{%
\subsubsection{\texorpdfstring{\emph{Tell a story}}{Tell a story}}\label{tell-a-story}}

Typically, your visualization will not be shown entirely on its own, but rather it will be part of a larger presentation.
Further, visualizations can provide supporting information for any aspect of a presentation, from opening to conclusion.
For example, you could use an exploratory visualization in the opening of the presentation to motivate your choice
of a more detailed data analysis / model, a visualization of the results of your analysis to show what your analysis has uncovered,
or even one at the end of a presentation to help suggest directions for future work.

Regardless of where it appears, a good way to discuss your visualization is as a story:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  Establish the setting and scope, and motivate why you did what you did.
\item
  Pose the question that your visualization answers. Justify why the question is important to answer.
\item
  Answer the question using your visualization. Make sure you describe \emph{all} aspects of the visualization (including describing the axes). But you
  can emphasize different aspects based on what is important to answer your question:

  \begin{itemize}
  \tightlist
  \item
    \textbf{trends (lines):} Does a line describe the trend well? If so, the trend is \emph{linear}, and if not, the trend is \emph{nonlinear}. Is the trend increasing, decreasing, or neither?
    Is there a periodic oscillation (wiggle) in the trend? Is the trend noisy (does the line ``jump around'' a lot) or smooth?
  \item
    \textbf{distributions (scatters, histograms):} How spread out are the data? Where are they centered, roughly? Are there any obvious ``clusters'' or ``subgroups'', which would be visible as multiple bumps in the histogram?\\
  \item
    \textbf{distributions of two variables (scatters):} is there a clear / strong relationship between the variables (points fall in a distinct pattern), a weak one (points fall in a pattern but there is some noise), or no discernible
    relationship (the data are too noisy to make any conclusion)?
  \item
    \textbf{amounts (bars):} How large are the bars relative to one another? Are there patterns in different groups of bars?
  \end{itemize}
\item
  Summarize your findings, and use them to motivate whatever you will discuss next.
\end{enumerate}

Below are two examples of how one might take these four steps in describing the example visualizations that appeared earlier in this chapter.
Each of the steps is denoted by its numeral in parentheses, e.g.~(3).

\textbf{Mauna Loa Atmospheric CO2 Measurements:} (1) Many current forms of energy generation and conversion---from automotive engines to natural gas power plants---rely on burning
fossil fuels and produce greenhouse gases, typically primarily carbon dioxide (CO2), as a byproduct. Too much of these gases in the Earth's atmosphere will cause it to trap
more heat from the sun, leading to global warming. (2) In order to assess how quickly the atmospheric concentration of CO2 is increasing over time, we (3) used a data set from
the Mauna Loa observatory from Hawaii, consisting of CO2 measurements from 1959 to the present. We plotted the measured concentration of CO2 (on the vertical axis) over time
(on the horizontal axis). From this plot, you can see a clear, increasing, and generally linear trend over time. There is also a periodic oscillation that occurs once per year and
aligns with Hawaii's seasons, with an amplitude that is small relative to the growth in the overall trend. This shows that atmospheric CO2 is clearly increasing over
time, and (4) it is perhaps worth investigating more into the causes.

\textbf{Michelson Light Speed Experiments:} (1) Our modern understanding of the physics of light has advanced significantly from the
late 1800s when Michelson and Morley's experiments first demonstrated that it had a finite speed. We now know based on modern experiments that it
moves at roughly 299792.458 kilometres per second. (2) But how accurately were we first able to measure this fundamental physical constant, and did certain
experiments produce more accurate results than others?
(3) To better understand this we plotted data from 5 experiments by Michelson in 1879, each with 20 trials, as histograms stacked on top of one another.
The horizontal axis shows the accuracy of the measurements relative to the true speed of light as we know it today, expressed as a percentage.
From this visualization, you can see that most results had relative errors of at most 0.05\%. You can also see that experiments 1 and 3 had
measurements that were the farthest from the true value, and experiment 5 tended to provide the most consistently accurate result. (4) It would be
worth further investigating the differences between these experiments to see why they produced different results.

\hypertarget{saving-the-visualization}{%
\section{Saving the visualization}\label{saving-the-visualization}}

\hypertarget{choose-the-right-output-format-for-your-needs}{%
\subsubsection{\texorpdfstring{\emph{Choose the right output format for your needs}}{Choose the right output format for your needs}}\label{choose-the-right-output-format-for-your-needs}}

Just as there are many ways to store data sets, there are many ways to store visualizations and images.
Which one you choose can depend on several factors, such as file size/type limitations
(e.g., if you are submitting your visualization as part of a conference paper or to a poster printing shop)
and where it will be displayed (e.g., online, in a paper, on a poster, on a billboard, in talk slides).
Generally speaking, images come in two flavours: \emph{bitmap} (or \emph{raster}) formats and \emph{vector} (or \emph{scalable graphics}) formats.

\textbf{Bitmap / Raster} images are represented as a 2-D grid of square pixels, each with its own colour. Raster images are often \emph{compressed} before storing so they take up less space. A compressed format is \emph{lossy} if the image cannot be perfectly recreated when loading and displaying, with the hope that the change is not noticeable. \emph{Lossless} formats, on the other hand, allow a perfect display of the original image.

\begin{itemize}
\tightlist
\item
  \emph{Common file types:}

  \begin{itemize}
  \tightlist
  \item
    \href{https://en.wikipedia.org/wiki/JPEG}{JPEG} (\texttt{.jpg}, \texttt{.jpeg}): lossy, usually used for photographs
  \item
    \href{https://en.wikipedia.org/wiki/Portable_Network_Graphics}{PNG} (\texttt{.png}): lossless, usually used for plots / line drawings
  \item
    \href{https://en.wikipedia.org/wiki/BMP_file_format}{BMP} (\texttt{.bmp}): lossless, raw image data, no compression (rarely used)
  \item
    \href{https://en.wikipedia.org/wiki/TIFF}{TIFF} (\texttt{.tif}, \texttt{.tiff}): typically lossless, no compression, used mostly in graphic arts, publishing
  \end{itemize}
\item
  \emph{Open-source software:} \href{https://www.gimp.org/}{GIMP}
\end{itemize}

\textbf{Vector / Scalable Graphics} images are represented as a collection of mathematical objects (lines, surfaces, shapes, curves). When the computer displays the image, it redraws all of the elements using their mathematical formulas.

\begin{itemize}
\tightlist
\item
  \emph{Common file types:}

  \begin{itemize}
  \tightlist
  \item
    \href{https://en.wikipedia.org/wiki/Scalable_Vector_Graphics}{SVG} (\texttt{.svg}): general-purpose use
  \item
    \href{https://en.wikipedia.org/wiki/Encapsulated_PostScript}{EPS} (\texttt{.eps}), general-purpose use (rarely used)
  \end{itemize}
\item
  \emph{Open-source software:} \href{https://inkscape.org/}{Inkscape}
\end{itemize}

Raster and vector images have opposing advantages and disadvantages. A raster image of a fixed width / height takes the same amount of space and time to load regardless of
what the image shows (caveat: the compression algorithms may shrink the image more or run faster for certain images). A vector image takes space and time to load
corresponding to how complex the image is, since the computer has to draw all the elements each time it is displayed. For example, if you have a scatter plot with 1 million
points stored as an SVG file, it may take your computer some time to open the image. On the other hand, you can zoom into / scale up
vector graphics as much as you like without the image looking bad, while raster images eventually start to look ``pixellated.''

\begin{quote}
\textbf{PDF files:}
The portable document format \href{https://en.wikipedia.org/wiki/PDF}{PDF} (\texttt{.pdf}) is commonly used to
store \emph{both} raster and vector graphics formats. If you try to open a PDF and it's taking a long time
to load, it may be because there is a complicated vector graphics image that your computer is rendering.
\end{quote}

Let's investigate how different image file formats behave with a scatter plot of the
\href{https://www.stat.cmu.edu/~larry/all-of-statistics/=data/faithful.dat}{Old Faithful data set}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(svglite) }\CommentTok{\# we need this to save SVG files}
\NormalTok{faithful\_plot }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(}\AttributeTok{data =}\NormalTok{ faithful, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ waiting, }\AttributeTok{y =}\NormalTok{ eruptions)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}

\NormalTok{faithful\_plot}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/03-plot-line-1.pdf}
\caption{\label{fig:03-plot-line}Scatter plot of waiting time and eruption time}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]

\FunctionTok{ggsave}\NormalTok{(}\StringTok{"faithful\_plot.png"}\NormalTok{, faithful\_plot)}
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"faithful\_plot.jpg"}\NormalTok{, faithful\_plot)}
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"faithful\_plot.bmp"}\NormalTok{, faithful\_plot)}
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"faithful\_plot.tiff"}\NormalTok{, faithful\_plot)}
\FunctionTok{ggsave}\NormalTok{(}\StringTok{"faithful\_plot.svg"}\NormalTok{, faithful\_plot)}

\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"PNG filesize: "}\NormalTok{, }\FunctionTok{file.info}\NormalTok{(}\StringTok{"faithful\_plot.png"}\NormalTok{)[}\StringTok{"size"}\NormalTok{] }\SpecialCharTok{/} \DecValTok{1000000}\NormalTok{, }\StringTok{"MB"}\NormalTok{))}
\DocumentationTok{\#\# [1] "PNG filesize:  0.178079 MB"}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"JPG filesize: "}\NormalTok{, }\FunctionTok{file.info}\NormalTok{(}\StringTok{"faithful\_plot.jpg"}\NormalTok{)[}\StringTok{"size"}\NormalTok{] }\SpecialCharTok{/} \DecValTok{1000000}\NormalTok{, }\StringTok{"MB"}\NormalTok{))}
\DocumentationTok{\#\# [1] "JPG filesize:  0.19125 MB"}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"BMP filesize: "}\NormalTok{, }\FunctionTok{file.info}\NormalTok{(}\StringTok{"faithful\_plot.bmp"}\NormalTok{)[}\StringTok{"size"}\NormalTok{] }\SpecialCharTok{/} \DecValTok{1000000}\NormalTok{, }\StringTok{"MB"}\NormalTok{))}
\DocumentationTok{\#\# [1] "BMP filesize:  10.522254 MB"}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"TIFF filesize: "}\NormalTok{, }\FunctionTok{file.info}\NormalTok{(}\StringTok{"faithful\_plot.tiff"}\NormalTok{)[}\StringTok{"size"}\NormalTok{] }\SpecialCharTok{/} \DecValTok{1000000}\NormalTok{, }\StringTok{"MB"}\NormalTok{))}
\DocumentationTok{\#\# [1] "TIFF filesize:  10.52511 MB"}
\FunctionTok{print}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"SVG filesize: "}\NormalTok{, }\FunctionTok{file.info}\NormalTok{(}\StringTok{"faithful\_plot.svg"}\NormalTok{)[}\StringTok{"size"}\NormalTok{] }\SpecialCharTok{/} \DecValTok{1000000}\NormalTok{, }\StringTok{"MB"}\NormalTok{))}
\DocumentationTok{\#\# [1] "SVG filesize:  0.046062 MB"}
\end{Highlighting}
\end{Shaded}

Wow, that's quite a difference! Notice that for such a simple plot with few graphical elements (points), the vector graphics format (SVG) is over 100
times smaller than the uncompressed raster images (BMP, TIFF). Also, note that the JPG format is twice as large as the PNG format since the JPG
compression algorithm is designed for natural images (not plots). Below, we also show what the images look like when we zoom in to a rectangle with only 3 data points.
You can see why vector graphics formats are so useful: because they're just based on mathematical formulas, vector graphics can be scaled up to arbitrary sizes.
This makes them great for presentation media of all sizes, from papers to posters to billboards.

\begin{figure}

{\centering \includegraphics[width=0.5\linewidth,height=0.5\textheight]{img/faithful_zoom} \includegraphics[width=0.5\linewidth,height=0.5\textheight]{img/faithful_zoom_screenshot_svg} 

}

\caption{Zoomed in `faithful`, raster (PNG, left) and vector (SVG, right) formats}\label{fig:03-raster-image}
\end{figure}

\hypertarget{version_control}{%
\chapter{Collaboration with version control}\label{version_control}}

\hypertarget{overview-3}{%
\section{Overview}\label{overview-3}}

This chapter will introduce the concept of using version control systems to track changes to a project over
its lifespan, to share and edit code in a collaborative team, and to distribute the finished project to its intended audience.
This chapter also demonstrates how to implement these ideas effectively
in practice using \href{https://git-scm.com}{Git}, \href{https://github.com}{GitHub}, and JupyterHub.

\hypertarget{chapter-learning-objectives-4}{%
\section{Chapter learning objectives}\label{chapter-learning-objectives-4}}

By the end of the chapter, students will be able to:

\begin{itemize}
\tightlist
\item
  Describe what version control is and why data analysis projects can benefit from it
\item
  Create a remote version control repository on GitHub
\item
  Move changes to files from GitHub to JupyterHub, and from JupyterHub to GitHub
\item
  Give collaborators access to the repository
\item
  Resolve conflicting edits made by multiple collaborators
\item
  Communicate with collaborators using issues
\item
  Use best practices when collaborating on a project with others
\end{itemize}

\hypertarget{what-is-version-control-and-why-should-i-use-it}{%
\section{What is version control, and why should I use it?}\label{what-is-version-control-and-why-should-i-use-it}}

Data analysis projects often require iteration and revision to move from an initial idea
to a finished product that is ready for the intended audience. Without
deliberate and conscious effort towards tracking changes made to the analysis,
projects tend to become messy, with mystery results files that cannot be reproduced,
temporary files with snippets of ideas, mind-boggling filenames like \texttt{document\_final\_draft\_v5\_final.txt},
large blocks of commented code ``saved for later,'' and more. Additionally,
the iterative nature of data analysis projects makes it important to be able to examine
earlier versions of code and writing. Finally, data analyses are typically completed by a team of
people rather than a single person. This means that files need to be
shared across multiple computers, and multiple people often end up editing the project
simultaneously. In such a situation, determining who has the latest version of the project---and
how to resolve conflicting edits---can be a real challenge.

Version control helps solve these challenges by tracking changes to the files in the
analysis (code, writing, data, etc) over the lifespan of the project, including when the changes were made and who made
them. This provides the means both to view earlier versions of the project and to revert changes.
Version control also facilitates collaboration via tools to share edits with others and resolve conflicting edits.
But even if you're working on a project alone, you should still use version control.
It helps you keep track of what you've done, when you did it, and what you're planning to do next!

\begin{quote}
\emph{You mostly collaborate with yourself, and me-from-two-months-ago never responds to email.}

--Mark T. Holder
\end{quote}

In order to version control a project, you generally need two things: a \emph{version control system}
and a \emph{repository hosting service}. The version control system is the software that is responsible
for tracking changes, sharing changes you make with others, obtaining changes others have made, and resolving conflicting edits.
The repository hosting service is responsible for storing a copy of the version controlled project online (a \emph{repository}),
where you and your collaborators can access it remotely, discuss issues and bugs, and distribute your final product.
For both of these items, there is a wide variety of choices; some of the more popular ones are:

\begin{itemize}
\tightlist
\item
  \textbf{Version control systems:}

  \begin{itemize}
  \tightlist
  \item
    \href{https://git-scm.com}{Git}
  \item
    \href{https://mercurial-scm.org}{Mercurial}
  \item
    \href{https://subversion.apache.org}{Subversion}
  \end{itemize}
\item
  \textbf{Repository hosting services:}

  \begin{itemize}
  \tightlist
  \item
    \href{https://github.com}{GitHub}
  \item
    \href{https://gitlab.com}{GitLab}
  \item
    \href{https://bitbucket.org}{BitBucket}
  \end{itemize}
\end{itemize}

In this textbook we'll use Git for version control, and GitHub for repository hosting, because both are currently the most widely-used platforms.

\begin{quote}
\textbf{Note:} technically you don't \emph{have to} use a repository hosting service. You can, for example, use Git to version control a project
that is stored only in a folder on your computer. But using a repository hosting service provides a few big benefits, including managing collaborator access permissions,
tools to discuss and track bugs, and the ability to have external collaborators contribute work, not to mention the safety of having your work backed up in the cloud. Since most
repository hosting services now offer free accounts, there are not many situations in which you wouldn't want to use one for your project.
\end{quote}

\hypertarget{creating-a-space-for-your-project-online}{%
\section{Creating a space for your project online}\label{creating-a-space-for-your-project-online}}

Before you can create repositories, you will need a GitHub account;
you can sign up for a free account at \url{https://github.com/}.
Once you have logged into your account, you can create a new repository to host your project
by clicking on the ``+'' icon in the upper right
hand corner, and then on ``New Repository'' as shown below:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/new-repository-01-1}

On the next page, do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Enter the name for your project repository. In the example below, we use \texttt{canadian\_languages}. Most repositories follow this naming convention, which involves lowercase letter words separated by either underscores or hyphens.
\item
  Choose an option for the privacy of your repository

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \tightlist
  \item
    If you select ``Public'', your repository may be \emph{viewed} by anyone, but only you and collaborators you designate will be able to \emph{modify} it.
  \item
    If you select ``Private'', only you and your collaborators can \emph{view} or \emph{modify} it.
  \end{enumerate}
\item
  Select ``Add a README file.'' This creates a template \texttt{README.md} file in your repository's root folder.
\item
  When you are happy with your repository name and configuration, click on the green ``Create Repository'' button.
\end{enumerate}

\includegraphics[width=0.75\linewidth]{bookdown_files/figure-latex/new-repository-02-1}

Now you should have a repository that looks something like this:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/new-repository-03-1}

\hypertarget{creating-and-editing-files-on-github}{%
\section{Creating and editing files on GitHub}\label{creating-and-editing-files-on-github}}

There are several ways to use the GitHub interface to add files to your repository and to edit them.
Below we cover how to use the pen tool to edit existing files, and how to use the ``Add file'' drop down
to create a new file or upload files from your computer. These techniques are useful for handling simple
plaintext files, for example, the \texttt{README.md} file that is already present in the repository.

\hypertarget{the-pen-tool}{%
\subsection{The pen tool}\label{the-pen-tool}}

The pen tool can be used to edit existing plaintext files. Click on the pen tool:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/pen-tool-01-1}

Use the text box to make your changes:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/pen-tool-02-1}

Finally, \emph{commit} your changes. When you \emph{commit a file} in a repository, the version control system takes a snapshot
of what the file looks like. As you continue working on the project, over time you will possibly make many commits to a single file; this generates
a useful version history for that file. On GitHub, if you click the green ``Commit changes'' button, it will save the file and then make a commit.
Do this now:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/pen-tool-03-1}

\hypertarget{the-add-file-menu}{%
\subsection{The ``Add file'' menu}\label{the-add-file-menu}}

The ``Add file'' menu can be used to create new plaintext files and upload files from your computer.
To create a new plaintext file, click the ``Add file'' drop down menu and
select the ``Create new file'' option:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/create-new-file-01-1}

A page will open with a small text box for the file name to be entered, and a larger text box where the desired file
content text can be entered. Note the two tabs, ``Edit new file'' and ``Preview''.
Toggling between them lets you enter and edit text and view what the text will
look like when rendered, respectively. Note that GitHub understands and renders \texttt{.md}
files using a \href{https://guides.github.com/pdfs/markdown-cheatsheet-online.pdf}{markdown syntax} very similar to
Jupyter notebooks, so the ``Preview'' tab is especially helpful for checking markdown code correctness.

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/create-new-file-02-1}

Save and commit your changes by click the green ``Commit changes'' button at the
bottom of the page.

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/create-new-file-03-1}

You can also upload files that you have created on your local machine by using
the ``Add file'' drop down menu and selecting ``Upload files'':

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/upload-files-01-1}

To select the files from your local computer to upload, you can either drag and
drop them into the grey box area shown below, or click the ``choose your files''
link to access a file browser dialog. Once the files you want to upload have
been selected, click the green ``Commit changes'' button at the bottom of the
page.

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/upload-files-02-1}

Note that Git and GitHub are designed to track changes in individual files. \emph{Do not}
upload your whole project in an archive file (e.g.~\texttt{.zip}), because
then Git can only keep track of changes to the entire \texttt{.zip} file---that wouldn't be very useful
if you're trying to see the history of changes to a single code file in your project!

\hypertarget{cloning-your-repository-on-jupyterhub}{%
\section{Cloning your repository on JupyterHub}\label{cloning-your-repository-on-jupyterhub}}

Although there are several ways to create and edit files on
GitHub, they are not quite powerful enough for
efficiently creating and editing complex files, or files that need to be
executed to assess whether they work (e.g., files containing code). Thus, it is
useful to be able to connect the project repository that was created on
GitHub to a coding environment. This can be done on
your local computer, or using a JupyterHub; below we show how to do this using a JupyterHub.

We need to \emph{clone} our project's Git repository to our JupyterHub---i.e., make a copy that knows where it was obtained from so that it knows where
send/receive new committed edits. In order to do this, first copy the url from the HTTPS tab of the Code drop down menu on
GitHub:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/clone-02-1}

Then open JupyterHub, and click the Git+ icon on the file browser tab:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/clone-01-1}

Paste the url of the GitHub project repository you
created and click the blue ``CLONE'' button:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/clone-03-1}

On the file browser tab, you will now see a folder for your project's repository
(and inside it will be all the files that existed on GitHub):

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/clone-04-1}

\hypertarget{working-in-a-cloned-repository-on-jupyterhub}{%
\section{Working in a cloned repository on JupyterHub}\label{working-in-a-cloned-repository-on-jupyterhub}}

Now that you have cloned your repository on JupyterHub, you can get to work
editing, creating, and deleting files. Once you reach a point that you want
Git to keep a record of the current version, you need to \emph{commit} (i.e., snapshot) your changes.
Then once you have made commits that you want to share with your collaborators,
you need to \emph{push} (i.e., send) those commits back to GitHub.
Again, we can use the JupyterLab Git extension tool
to do all of this. In particular, your workflow on JupyterHub should look like this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  You edit, create, and delete files in your cloned repository on JupyterHub.
\item
  Once you want a record of the current version, you specify which files to ``add'' to Git's \emph{staging area}. You can think of files in the staging area as
  those modified files for which you want a snapshot.
\item
  You commit those flagged files to your repository, and include a helpful \emph{commit message} to tell your collaborators about the changes
  you made. \textbf{Note:} here you are \emph{only} committing to your \emph{cloned repository} stored on JupyterHub. The repository on GitHub has not changed, and your collaborators
  cannot see your work yet.
\item
  Go back to step 1. and keep working!
\item
  When you want to store your commits (that only exist in your cloned repository right now) on the cloud where they can be shared with your collaborators, you \emph{push} them
  back to the hosted repository on GitHub.
\end{enumerate}

Below we walk through how to use the Jupyter Git extension tool to do
each of the steps outlined above.

\hypertarget{specifying-files-to-commit}{%
\subsection{Specifying files to commit}\label{specifying-files-to-commit}}

Below we created and saved a new file (named \texttt{eda.ipynb}) that we would
like to send back to the project repository on GitHub.
To ``add'' this modified file to the staging area (\emph{i.e.}, flag that this is a
file whose changes we would like to commit), we click the Jupyter Git extension
icon on the far left-hand side of JupyterLab:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/git-add-01-1}

This opens the Jupyter Git graphical user interface pane, and then we
click the plus sign beside the file that we want to ``add''.

\begin{quote}
\textbf{Note:} because this is the first change for this file that we want to add, it falls under the
``Untracked'' heading. However, next time we edit this file and want to add the
changes we made, we will find it under the ``Changed'' heading.
\end{quote}

\begin{quote}
\textbf{Note:} do not add the \texttt{eda-checkpoint.ipynb} file (sometimes called \texttt{.ipynb\_checkpoints}). This file is
automatically created by Jupyter when you work on \texttt{eda.ipynb}. You generally do not add auto-generated files to Git repositories,
only add the files you directly create and edit.
\end{quote}

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/git-add-02-1}

This moves the file from the ``Untracked'' heading to the ``Staged'' heading,
flagging this file so that Git knows we want a snapshot of its current state as a commit.
Now we are ready to ``commit'' the changes. Make sure to include a (clear and helpful!) message about what was changed
so that your collaborators (and future you) know what happened in this commit.

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/git-add-03-1}

\hypertarget{making-the-commit}{%
\subsection{Making the commit}\label{making-the-commit}}

To snapshot the changes with an associated commit message, we put the message in the text box
at the bottom of the Git pane and click on the blue ``Commit'' button.
It is highly recommended to write useful and meaningful messages about what
was changed. These commit messages, and the datetime stamp for a given
commit, are the primary means to navigate through the project's histry in the
event that we need to view or retrieve a past version of a file, or
revert our project to an earlier state.

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/git-commit-01-1}

\begin{quote}
When you click the ``Commit'' button for the first time, you will be prompted to
enter your name and email. This only needs to be done once for each machine
you use Git on.
\end{quote}

After ``commiting'' the file(s), you will see there there are 0 ``Staged'' files and we
are now ready to push our changes (and the attached commit message) to our
project repository on GitHub:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/git-commit-03-1}

\hypertarget{pushing-the-commits-to-github}{%
\subsection{Pushing the commits to GitHub}\label{pushing-the-commits-to-github}}

To send the committed changes back to the project repository on
GitHub, we need to \emph{push} them. To do this we click on
the cloud icon with the up arrow on the Jupyter Git tab:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/git-push-01-1}

We will then be prompted to enter our GitHub username
and password, and click the blue ``OK'' button:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/git-push-02-1}

If the files were successfully pushed to our project repository on
GitHub we will be given the success message shown
below. Click ``Dismiss'' to continue working in Jupyter.

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/git-push-03-1}

You will see that the changes now exist there!

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/git-push-04-1}

\hypertarget{collaboration}{%
\section{Collaboration}\label{collaboration}}

\hypertarget{giving-collaborators-access-to-your-project}{%
\subsection{Giving collaborators access to your project}\label{giving-collaborators-access-to-your-project}}

As mentioned earlier, GitHub allows you to control who has access to your
project. The default of both public and private projects are that only the
person who created the GitHub repository has permissions to create, edit and
delete files (\emph{write access}). To give your collaborators write access to the projects, navigate to the ``Settings'' tab:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/add-collab-01-1}

Then click ``Manage access'':

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/add-collab-02-1}

Click the green ``Invite a collaborator'' button:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/add-collab-03-1}

Type in the collaborator's GitHub username and select their name when it
appears:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/add-collab-04-1}

Finally, click the green ``Add to this repository'' button:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/add-collab-05-1}

After this you should see your newly added collaborator listed under the
``Manage access'' tab. They should receive an email invitation to join the
GitHub repository as a collaborator. They need to accept this invitation
to enable write access.

\hypertarget{pulling-changes-from-github}{%
\subsection{Pulling changes from GitHub}\label{pulling-changes-from-github}}

If your collaborators send their own commits to the GitHub repository, you will need to
\emph{pull} those changes to your own cloned copy on JupyterHub before you're allowed to push
any more changes yourself. By pulling their changes, you sync your local repository
to what is present on GitHub.

\begin{quote}
\textbf{Note:} you can still work on your own cloned repository and commit changes even if collaborators
have pushed changes to the GitHub repository. It is only when you try to \emph{push} your changes back to GitHub
that Git will make sure nobody else has pushed any work in the meantime.
\end{quote}

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/git-pull-00-1}
You can do this using the Jupyter Git tab by clicking on the cloud icon with
the down arrow:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/git-pull-01-1}

Once the files are successfully pulled from GitHub, you need to click ``Dismiss''
to keep working:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/git-pull-02-1}

And then when you open (or refresh) the files whose changes you just pulled,
you should be able to see them:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/git-pull-03-1}

It can be very useful to review the history of the changes to your project. You
can do this directly on the JupyterHub by clicking ``History'' in the Git tab.

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/git-pull-04-1}

It is good practice to pull any changes at the start of \emph{every} work session before
you start working on your local copy. If you do not do this, and your collaborators have pushed some changes
to the project to GitHub, then you will be unable to push your changes to
GitHub until you pull. This situation can be recognized by this error message:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/merge-conflict-01-1}

Usually, getting out of this situation is not too troublesome. First you need to
pull the changes that exist on GitHub that you do not yet have on your machine.
Usually when this happens, Git can automatically merge the changes for you,
even if you and your collaborators were working on different parts of the same
file!

If however, you and your collaborators made changes to the same line of the same
file, Git will not be able to automatically merge the changes---it will not know whether to keep your version of the line(s), your
collaborators version of the line(s), or some blend of the two. When this
happens, Git will tell you that you have a merge conflict and that it needs
human intervention (you!), and which file(s) this occurs in.

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/merge-conflict-03-1}

\hypertarget{handling-merge-conflicts}{%
\subsection{Handling merge conflicts}\label{handling-merge-conflicts}}

To fix the merge conflict we need to open the file that had the merge
conflict in a plain text editor and look for special marks that Git puts in the
file to tell you where the merge conflict occurred.

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/merge-conflict-04-1}

The beginning of the merge
conflict is preceded by \texttt{\textless{}\textless{}\textless{}\textless{}\textless{}\textless{}\textless{}\ HEAD} and the end of the merge conflict is
marked by \texttt{\textgreater{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}\textgreater{}}. Between these markings, Git also inserts a separator
(\texttt{=======}). The version of the change before the separator is your change, and
the version that follows the separator was the change that existed on GitHub.

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/merge-conflict-05-1}

Once you have decided which version of the change (or what combination!) to keep, you need to use the
plain text editor to remove the special marks that Git added.

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/merge-conflict-06-1}

The file must be saved, added to the staging area, and then committed before you will be able to
push your changes to GitHub.

\hypertarget{communicating-using-github-issues}{%
\subsection{Communicating using GitHub issues}\label{communicating-using-github-issues}}

When working on a project in a team, you don't just want a historical record of who changed
what file and when in the project---you also want a record of decisions that were made,
ideas that were floated, problems that were identified and addressed, and all other
communication surrounding the project. Email and messaging apps are both very popular for general communication, but are not
designed for project-specific communication: they both generally do not have facilities for organizing conversations by project subtopics,
searching for conversations related to particular bugs or software versions, etc.

GitHub \emph{issues} are an alternative written communication medium to email and
messaging apps, and were designed specifically to facilitate project-specific
communication. Issues are \emph{opened} from the ``Issues'' tab on the project's
GitHub page, and they persist there even after the conversation is over and the issue is \emph{closed} (in
contrast to email, issues are not usually deleted). One issue thread is usually created
per topic, and they are easily searchable using GitHub's search tools. All
issues are accessible to all project collaborators, so no one is left out of
the conversation. Finally, issues can be setup so that team members get email
notifications when a new issue is created or a new post is made in an issue
thread. Replying to issues from email is also possible. Given all of these advantages,
we highly recommend the use of issues for project-related communication.

To open a GitHub issue, first click on the ``Issues'' tab:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/issue-01-1}

Next click the ``New issue'' button:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/issue-02-1}

Add an issue title (which acts like an email subject line), and then put the
body of the message in the larger text box. Finally click ``Submit new issue''
to post the issue to share with others:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/issue-03-1}

You can reply to an issue that someone opened by adding your written response to
the large text box and clicking comment:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/issue-04-1}

When a conversation is resolved, you can click ``Close issue''. The closed issue
can be later viewed be clicking the ``Closed'' header link in the ``Issue'' tab:

\includegraphics[width=1\linewidth]{bookdown_files/figure-latex/issue-06-1}

\hypertarget{additional-resources-2}{%
\section{Additional resources}\label{additional-resources-2}}

Now that you've picked up the basics of version control with Git and GitHub, you can expand your knowledge using
one of the many tutorials available online:

\hypertarget{best-practices-and-workflows}{%
\subsection{Best practices and workflows}\label{best-practices-and-workflows}}

\begin{itemize}
\tightlist
\item
  \href{https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005510\#sec014}{Good enough practices in scientific computing} \citeyearpar{wilson2014best} by Greg Wilson, Jennifer Bryan, Karen Cranston, Justin Kitzes, Lex Nederbragt \& Tracy K. Teal
\item
  \href{https://ieeexplore.ieee.org/iel5/5992/4653193/04653206.pdf?casa_token=6gY_5D7UbE4AAAAA:Z7BBJHplUfZEq31EfOpF_iP1qUQJwn170LdU8-ZUTtCu1N-ZpXYMV6yZHqIcSuRHLNVSkjM}{Configuration Management for Large-Scale Scientific Computing at the UK Met Office} \citeyearpar{matthews2008configuration} by David Mathews, Greg Wilson \& Steve Easterbrook
\end{itemize}

\hypertarget{technical-references}{%
\subsection{Technical references}\label{technical-references}}

\begin{itemize}
\tightlist
\item
  GitHub's \href{https://guides.github.com/}{guides website}
\item
  GitHub's \href{https://www.youtube.com/githubguides}{YouTube channel}
\item
  BitBucket's \href{https://www.atlassian.com/git/tutorials/what-is-version-control}{tutorials}
\end{itemize}

\hypertarget{classification}{%
\chapter{Classification I: training \& predicting}\label{classification}}

\hypertarget{overview-4}{%
\section{Overview}\label{overview-4}}

Up until this point, we have focused solely on descriptive and exploratory
questions about data. This chapter and the next together serve as our first
foray into answering \emph{predictive} questions about data. In particular, we will
focus on the problem of \emph{classification}, i.e., using one or more quantitative
variables to predict the value of a third, categorical variable. This chapter
will cover the basics of classification, how to preprocess data to make it
suitable for use in a classifier, and how to use our observed data to make
predictions. The next will focus on how to evaluate how accurate the
predictions from our classifier are, as well as how to improve our classifier
(where possible) to maximize its accuracy.

\hypertarget{chapter-learning-objectives-5}{%
\section{Chapter learning objectives}\label{chapter-learning-objectives-5}}

\begin{itemize}
\tightlist
\item
  Recognize situations where a classifier would be appropriate for making predictions
\item
  Describe what a training data set is and how it is used in classification
\item
  Interpret the output of a classifier
\item
  Compute, by hand, the straight-line (Euclidean) distance between points on a graph when there are two explanatory variables/predictors
\item
  Explain the K-nearest neighbour classification algorithm
\item
  Perform K-nearest neighbour classification in R using \texttt{tidymodels}\\
\item
  Explain why one should center, scale, and balance data in predictive modelling
\item
  Preprocess data to center, scale, and balance a dataset using a \texttt{recipe}
\item
  Combine preprocessing and model training using a Tidymodels \texttt{workflow}
\end{itemize}

\hypertarget{the-classification-problem}{%
\section{The classification problem}\label{the-classification-problem}}

In many situations, we want to make predictions based on the current situation
as well as past experiences. For instance, a doctor may want to diagnose a
patient as either diseased or healthy based on their symptoms and the doctor's
past experience with patients; an email provider might want to tag a given
email as ``spam'' or ``not spam'' depending on past email text data; or an online
store may want to predict whether an order is fraudulent or not.

These tasks are all examples of \textbf{classification}, i.e., predicting a
categorical class (sometimes called a \emph{label}) for an observation given its
other quantitative variables (sometimes called \emph{features}). Generally, a
classifier assigns an observation (e.g.~a new patient) to a class (e.g.
diseased or healthy) on the basis of how similar it is to other observations
for which we know the class (e.g.~previous patients with known diseases and
symptoms). These observations with known classes that we use as a basis for
prediction are called a \textbf{training set}. We call them a ``training set'' because
we use these observations to train, or teach, our classifier so that we can use
it to make predictions on new data that we have not seen previously.

There are many possible classification algorithms that we could use to predict
a categorical class/label for an observation. In addition, there are many
variations on the basic classification problem, e.g., binary classification
where only two classes are involved (e.g.~disease or healthy patient), or
multiclass classification, which involves assigning an object to one of several
classes (e.g., private, public, or not for-profit organization). Here we will
focus on the simple, widely used \textbf{K-nearest neighbours} algorithm for the
binary classification problem. Other examples you may encounter in future
courses include decision trees, support vector machines (SVMs), logistic
regression, and neural networks.

\hypertarget{exploring-a-labelled-data-set}{%
\section{Exploring a labelled data set}\label{exploring-a-labelled-data-set}}

In this chapter and the next, we will study a data set of
\href{http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+\%28Diagnostic\%29}{digitized breast cancer image features},
created by Dr.~William H. Wolberg, W. Nick Street, and Olvi L. Mangasarian at
the University of Wisconsin, Madison. Each row in the data set represents an
image of a tumour sample, including the diagnosis (benign or malignant) and
several other measurements (e.g., nucleus texture, perimeter, area, etc.).
Diagnosis for each image was conducted by physicians.

As with all data analyses, we first need to formulate a precise question that
we want to answer. Here, the question is \emph{predictive}: can we use the tumour
image measurements available to us to predict whether a future tumour image
(with unknown diagnosis) shows a benign or malignant tumour? Answering this
question is important because traditional, non-data-driven methods for tumour
diagnosis are quite subjective and dependent upon how skilled and experienced
the diagnosing physician is. Furthermore, benign tumours are not normally
dangerous; the cells stay in the same place and the tumour stops growing before
it gets very large. By contrast, in malignant tumours, the cells invade the
surrounding tissue and spread into nearby organs where they can cause serious
damage (\href{https://www.worldwidecancerresearch.org/who-we-are/cancer-basics/}{learn more about cancer here}).
Thus, it is important to quickly and accurately diagnose the tumour type to
guide patient treatment.

\textbf{Loading the data}

Our first step is to load, wrangle, and explore the data using visualizations
in order to better understand the data we are working with. We start by
loading the necessary packages for our analysis. Below you'll see (in addition
to the usual \texttt{tidyverse}) a new package: \texttt{forcats}.
The \texttt{forcats} package enables us to easily
manipulate factors in R; factors are a special categorical type of variable in
R that are often used for class label data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(forcats)}
\end{Highlighting}
\end{Shaded}

In this case, the file containing the breast cancer data set is a simple \texttt{.csv}
file with headers. We'll use the \texttt{read\_csv} function with no additional
arguments, and then inspect its contents:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cancer }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/wdbc.csv"}\NormalTok{)}
\NormalTok{cancer}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 569 x 12
##        ID Class Radius Texture Perimeter   Area
##     <dbl> <chr>  <dbl>   <dbl>     <dbl>  <dbl>
##  1 8.42e5 M      1.10   -2.07     1.27    0.984
##  2 8.43e5 M      1.83   -0.353    1.68    1.91 
##  3 8.43e7 M      1.58    0.456    1.57    1.56 
##  4 8.43e7 M     -0.768   0.254   -0.592  -0.764
##  5 8.44e7 M      1.75   -1.15     1.78    1.82 
##  6 8.44e5 M     -0.476  -0.835   -0.387  -0.505
##  7 8.44e5 M      1.17    0.161    1.14    1.09 
##  8 8.45e7 M     -0.118   0.358   -0.0728 -0.219
##  9 8.45e5 M     -0.320   0.588   -0.184  -0.384
## 10 8.45e7 M     -0.473   1.10    -0.329  -0.509
## # ... with 559 more rows, and 6 more variables:
## #   Smoothness <dbl>, Compactness <dbl>,
## #   Concavity <dbl>, Concave_Points <dbl>,
## #   Symmetry <dbl>, Fractal_Dimension <dbl>
\end{verbatim}

\textbf{Variable descriptions}

Breast tumours can be diagnosed by performing a \emph{biopsy}, a process where
tissue is removed from the body and examined for the presence of disease.
Traditionally these procedures were quite invasive; modern methods such as fine
needle asipiration, used to collect the present data set, extract only a small
amount of tissue and are less invasive. Based on a digital image of each breast
tissue sample collected for this data set, 10 different variables were measured
for each cell nucleus in the image (3-12 below), and then the mean
for each variable across the nuclei was recorded. As part of the
data preparation, these values have been \emph{scaled}; we will discuss what this
means and why we do it later in this chapter. Each image additionally was given
a unique ID and a diagnosis for malignance by a physician. Therefore, the
total set of variables per image in this data set are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  ID number
\item
  Class: the diagnosis of \textbf{M}alignant or \textbf{B}enign
\item
  Radius: the mean of distances from center to points on the perimeter
\item
  Texture: the standard deviation of gray-scale values
\item
  Perimeter: the length of the surrounding contour
\item
  Area: the area inside the contour
\item
  Smoothness: the local variation in radius lengths
\item
  Compactness: the ratio of squared perimeter and area
\item
  Concavity: severity of concave portions of the contour
\item
  Concave Points: the number of concave portions of the contour
\item
  Symmetry
\item
  Fractal Dimension
\end{enumerate}

\begin{figure}
\includegraphics[width=1\linewidth]{img/malignant_cancer} \caption{A malignant breast fine needle aspiration image. [Source](https://pubsonline.informs.org/doi/abs/10.1287/opre.43.4.570)}\label{fig:05-bc-cells}
\end{figure}

Below we use \texttt{glimpse} to preview the data frame. This function can make it easier to inspect the data when we have a lot of columns:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(cancer)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 569
## Columns: 12
## $ ID                <dbl> 842302, 842517, 84300903...
## $ Class             <chr> "M", "M", "M", "M", "M",...
## $ Radius            <dbl> 1.0961, 1.8282, 1.5785, ...
## $ Texture           <dbl> -2.0715, -0.3533, 0.4558...
## $ Perimeter         <dbl> 1.26882, 1.68447, 1.5651...
## $ Area              <dbl> 0.98351, 1.90703, 1.5575...
## $ Smoothness        <dbl> 1.56709, -0.82624, 0.941...
## $ Compactness       <dbl> 3.28063, -0.48664, 1.052...
## $ Concavity         <dbl> 2.65054, -0.02382, 1.362...
## $ Concave_Points    <dbl> 2.5302, 0.5477, 2.0354, ...
## $ Symmetry          <dbl> 2.215566, 0.001391, 0.93...
## $ Fractal_Dimension <dbl> 2.2538, -0.8679, -0.3977...
\end{verbatim}

We can see from the summary of the data above that \texttt{Class} is of type character
(denoted by \texttt{\textless{}chr\textgreater{}}). Since we are going to be working with \texttt{Class} as a
categorical statistical variable, we will convert it to factor using the
function \texttt{as\_factor}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cancer }\OtherTok{\textless{}{-}}\NormalTok{ cancer }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Class =} \FunctionTok{as\_factor}\NormalTok{(Class))}
\FunctionTok{glimpse}\NormalTok{(cancer)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 569
## Columns: 12
## $ ID                <dbl> 842302, 842517, 84300903...
## $ Class             <fct> M, M, M, M, M, M, M, M, ...
## $ Radius            <dbl> 1.0961, 1.8282, 1.5785, ...
## $ Texture           <dbl> -2.0715, -0.3533, 0.4558...
## $ Perimeter         <dbl> 1.26882, 1.68447, 1.5651...
## $ Area              <dbl> 0.98351, 1.90703, 1.5575...
## $ Smoothness        <dbl> 1.56709, -0.82624, 0.941...
## $ Compactness       <dbl> 3.28063, -0.48664, 1.052...
## $ Concavity         <dbl> 2.65054, -0.02382, 1.362...
## $ Concave_Points    <dbl> 2.5302, 0.5477, 2.0354, ...
## $ Symmetry          <dbl> 2.215566, 0.001391, 0.93...
## $ Fractal_Dimension <dbl> 2.2538, -0.8679, -0.3977...
\end{verbatim}

Factors have what are called ``levels'', which you can think of as categories. We
can ask for the levels from the \texttt{Class} column by using the \texttt{levels} function.
This function should return the name of each category in that column. Given
that we only have 2 different values in our \texttt{Class} column (B and M), we
only expect to get two names back. Note that the \texttt{levels} function requires
a \emph{vector} argument, while the \texttt{select} function outputs a \emph{data frame};
so we use the \texttt{pull} function, which converts a single
column of a data frame into a vector.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cancer }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(Class) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# turns a data frame into a vector}
  \FunctionTok{levels}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "M" "B"
\end{verbatim}

\textbf{Exploring the data}

Before we start doing any modelling, let's explore our data set. Below we use
the \texttt{group\_by} + \texttt{summarize} code pattern we used before to see that we have
357 (63\%) benign and 212 (37\%) malignant tumour observations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num\_obs }\OtherTok{\textless{}{-}} \FunctionTok{nrow}\NormalTok{(cancer)}
\NormalTok{cancer }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Class) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}
    \AttributeTok{n =} \FunctionTok{n}\NormalTok{(),}
    \AttributeTok{percentage =} \FunctionTok{n}\NormalTok{() }\SpecialCharTok{/}\NormalTok{ num\_obs }\SpecialCharTok{*} \DecValTok{100}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##   Class     n percentage
##   <fct> <int>      <dbl>
## 1 M       212       37.3
## 2 B       357       62.7
\end{verbatim}

Next, let's draw a scatter plot to visualize the relationship between the
perimeter and concavity variables. Rather than use \texttt{ggplot\textquotesingle{}s} default palette,
we define our own here (\texttt{cbPalette}) and pass it as the \texttt{values} argument to
the \texttt{scale\_color\_manual} function. We also make the category labels (``B'' and
``M'') more readable by changing them to ``Benign'' and ``Malignant'' using the
\texttt{labels} argument.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# colour palette}
\NormalTok{cbPalette }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"\#E69F00"}\NormalTok{, }\StringTok{"\#56B4E9"}\NormalTok{, }\StringTok{"\#009E73"}\NormalTok{, }\StringTok{"\#F0E442"}\NormalTok{, }\StringTok{"\#0072B2"}\NormalTok{, }\StringTok{"\#D55E00"}\NormalTok{, }\StringTok{"\#CC79A7"}\NormalTok{, }\StringTok{"\#999999"}\NormalTok{)}

\NormalTok{perim\_concav }\OtherTok{\textless{}{-}}\NormalTok{ cancer }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Perimeter, }\AttributeTok{y =}\NormalTok{ Concavity, }\AttributeTok{color =}\NormalTok{ Class)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{color =} \StringTok{"Diagnosis"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Malignant"}\NormalTok{, }\StringTok{"Benign"}\NormalTok{), }\AttributeTok{values =}\NormalTok{ cbPalette)}
\NormalTok{perim\_concav}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/05-scatter-1.pdf}
\caption{\label{fig:05-scatter}Scatterplot of concavity versus perimeter coloured by diagnosis label}
\end{figure}

In this visualization, we can see that malignant observations typically fall in
the the upper right-hand corner of the plot area. By contrast, benign
observations typically fall in lower left-hand corner of the plot. Suppose we
obtain a new observation not in the current data set that has all the variables
measured \emph{except} the label (i.e., an image without the physician's diagnosis
for the tumour class). We could compute the perimeter and concavity values,
resulting in values of, say, 1 and 1. Could we use this information to classify
that observation as benign or malignant? What about a new observation with
perimeter value of -1 and concavity value of -0.5? What about 0 and 1? It seems
like the \emph{prediction of an unobserved label} might be possible, based on our
visualization. In order to actually do this computationally in practice, we
will need a classification algorithm; here we will use the K-nearest neighbour
classification algorithm.

\hypertarget{classification-with-k-nearest-neighbours}{%
\section{Classification with K-nearest neighbours}\label{classification-with-k-nearest-neighbours}}

To predict the label of a new observation, i.e., classify it as either benign
or malignant, the K-nearest neighbour classifier generally finds the \(K\)
``nearest'' or ``most similar'' observations in our training set, and then uses
their diagnoses to make a prediction for the new observation's diagnosis. To
illustrate this concept, we will walk through an example. Suppose we have a
new observation, with perimeter of 2 and concavity of 4
(labelled in red on the scatterplot), whose diagnosis ``Class'' is
unknown.

\includegraphics{bookdown_files/figure-latex/05-knn-1-1.pdf}

We see that the nearest point to this new observation is \textbf{malignant} and
located at the coordinates (2.1, 3.6). The idea here is that if a point is close to another in the scatterplot,
then the perimeter and concavity values are similar, and so we may expect that
they would have the same diagnosis.

\includegraphics{bookdown_files/figure-latex/05-knn-2-1.pdf}

Suppose we have another new observation with perimeter 0.2 and
concavity of 3.3. Looking at the scatterplot below, how would you
classify this red observation? The nearest neighbour to this new point is a
\textbf{benign} observation at (0.2, 2.7).
Does this seem like the right prediction to make? Probably not, if you consider
the other nearby points\ldots{}

\includegraphics{bookdown_files/figure-latex/05-knn-4-1.pdf}

So instead of just using the one nearest neighbour, we can consider several
neighbouring points, say \(K = 3\), that are closest to the new red observation
to predict its diagnosis class. Among those 3 closest points, we use the
\emph{majority class} as our prediction for the new observation. In this case, we
see that the diagnoses of 2 of the 3 nearest neighbours to our new observation
are malignant. Therefore we take majority vote and classify our new red
observation as malignant.

\includegraphics{bookdown_files/figure-latex/05-knn-5-1.pdf}

Here we chose the \(K=3\) nearest observations, but there is nothing special
about \(K=3\). We could have used \(K=4, 5\) or more (though we may want to choose
an odd number to avoid ties). We will discuss more about choosing \(K\) in the
next chapter.

\textbf{Distance between points}

How do we decide which points are the \(K\) ``nearest'' to our new observation? We
can compute the distance between any pair of points using the following
formula:

\[\mathrm{Distance} = \sqrt{(x_a -x_b)^2 + (y_a - y_b)^2}\]

\begin{quote}
This formula -- sometimes called the \emph{Euclidean distance} -- is simply the straight line distance between two points on the x-y plane with coordinates \((x_a, y_a)\) and \((x_b, y_b)\).
\end{quote}

Suppose we want to classify a new observation with perimeter of 0 and
concavity of 3.5. Let's calculate the distances
between our new point and each of the observations in the training set to find
the \(K=5\) observations in the training data that are nearest to our new point.

\includegraphics{bookdown_files/figure-latex/05-multiknn-1-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_obs\_Perimeter }\OtherTok{\textless{}{-}} \DecValTok{0}
\NormalTok{new\_obs\_Concavity }\OtherTok{\textless{}{-}} \FloatTok{3.5}
\NormalTok{cancer }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(ID, Perimeter, Concavity, Class) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{dist\_from\_new =} \FunctionTok{sqrt}\NormalTok{((Perimeter }\SpecialCharTok{{-}}\NormalTok{ new\_obs\_Perimeter)}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ (Concavity }\SpecialCharTok{{-}}\NormalTok{ new\_obs\_Concavity)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(dist\_from\_new) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) }\CommentTok{\# subset the first 5 rows}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 5
##        ID Perimeter Concavity Class dist_from_new
##     <dbl>     <dbl>     <dbl> <fct>         <dbl>
## 1   86409     0.241      2.65 B             0.881
## 2  887181     0.750      2.87 M             0.980
## 3  899667     0.623      2.54 M             1.14 
## 4  907914     0.417      2.31 M             1.26 
## 5 8710441    -1.16       4.04 B             1.28
\end{verbatim}

From this, we see that 3 of the 5 nearest neighbours to our new observation are
malignant so classify our new observation as malignant. We circle those 5 in
the plot below:

\includegraphics{bookdown_files/figure-latex/05-multiknn-3-1.pdf}

It can be difficult sometimes to read code as math, so here we mathematically
show the calculation of distance for each of the 5 closest points.

\begin{longtable}[]{@{}llll@{}}
\toprule
Perimeter & Concavity & Distance & Class\tabularnewline
\midrule
\endhead
0.24 & 2.65 & \(\sqrt{0 - 0.241)^2 + (3.5 - 2.65)^2}=\) 0.88 & B\tabularnewline
0.75 & 2.87 & \(\sqrt{(0 - 0.750)^2 + (3.5 - 2.87)^2} =\) 0.98 & M\tabularnewline
0.62 & 2.54 & \(\sqrt{(0 - 0.623)^2 + (3.5 - 2.54)^2} =\) 1.14 & M\tabularnewline
0.42 & 2.31 & \(\sqrt{(0 - 0.417)^2 + (3.5 - 2.31)^2} =\) 1.26 & M\tabularnewline
-1.16 & 4.04 & \(\sqrt{(0 - (-1.16))^2 + (3.5 - 4.04)^2} =\) 1.28 & B\tabularnewline
\bottomrule
\end{longtable}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{More than two explanatory variables}

Although the above description is directed toward two explanatory variables /
predictors, exactly the same K-nearest neighbour algorithm applies when you
have a higher number of explanatory variables (i.e., a higher-dimensional
predictor space). Each explanatory variable/predictor can give us new
information to help create our classifier. The only difference is the formula
for the distance between points. In particular, let's say we have \(m\) predictor
variables for two observations \(u\) and \(v\), i.e.,
\(u = (u_{1}, u_{2}, \dots, u_{m})\) and
\(v = (v_{1}, v_{2}, \dots, v_{m})\).
Before, we added up the squared difference between each of our (two) variables,
and then took the square root; now we will do the same, except for \emph{all} of our
\(m\) variables. In other words, the distance formula becomes

\[Distance = \sqrt{(u_{1} -v_{1})^2 + (u_{2} - v_{2})^2 + \dots + (u_{m} - v_{m})^2}\]

\begin{figure}
\includegraphics[width=0.7\linewidth]{img/classification_3d} \caption{3D scatterplot of symmetry, concavity and perimeter}\label{fig:05-more}
\end{figure}

\emph{Click and drag the plot above to rotate it, and scroll to zoom. Note that in
general we recommend against using 3D visualizations; here we show the data in
3D only to illustrate what ``higher dimensions'' look like for learning
purposes.}

\textbf{Summary}

In order to classify a new observation using a K-nearest neighbour classifier, we have to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Compute the distance between the new observation and each observation in the training set
\item
  Sort the data table in ascending order according to the distances
\item
  Choose the top \(K\) rows of the sorted table
\item
  Classify the new observation based on a majority vote of the neighbour classes
\end{enumerate}

\hypertarget{k-nearest-neighbours-with-tidymodels}{%
\section{\texorpdfstring{K-nearest neighbours with \texttt{tidymodels}}{K-nearest neighbours with tidymodels}}\label{k-nearest-neighbours-with-tidymodels}}

Coding the K-nearest neighbour algorithm in R ourselves would get complicated
if we might have to predict the label/class for multiple new observations, or
when there are multiple classes and more than two variables. Thankfully, in R,
the K-nearest neighbour algorithm is implemented in the \texttt{parsnip} package
included in the
\href{https://www.tidymodels.org/}{\texttt{tidymodels} package collection}, along with
many \href{https://www.tidymodels.org/find/parsnip/}{other models}
that you will encounter in this and future classes. The \texttt{tidymodels} collection
provides tools to help make and use models, such as classifiers. Using the packages
in this collection will help keep our code simple, readable and accurate; the
less we have to code ourselves, the fewer mistakes we are likely to make. We
start off by loading \texttt{tidymodels}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidymodels)}
\end{Highlighting}
\end{Shaded}

Let's again suppose we have a new observation with perimeter 0 and concavity
3.5, but its diagnosis is unknown (as in our example above). Suppose we
want to use the perimeter and concavity explanatory variables/predictors to
predict the diagnosis class of this observation. Let's pick out our 2 desired
predictor variables and class label and store it as a new dataset named \texttt{cancer\_train}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cancer\_train }\OtherTok{\textless{}{-}}\NormalTok{ cancer }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(Class, Perimeter, Concavity)}
\NormalTok{cancer\_train}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 569 x 3
##    Class Perimeter Concavity
##    <fct>     <dbl>     <dbl>
##  1 M        1.27      2.65  
##  2 M        1.68     -0.0238
##  3 M        1.57      1.36  
##  4 M       -0.592     1.91  
##  5 M        1.78      1.37  
##  6 M       -0.387     0.866 
##  7 M        1.14      0.300 
##  8 M       -0.0728    0.0610
##  9 M       -0.184     1.22  
## 10 M       -0.329     1.74  
## # ... with 559 more rows
\end{verbatim}

Next, we create a \emph{model specification} for K-nearest neighbours classification
by calling the \texttt{nearest\_neighbor} function, specifying that we want to use \(K = 5\) neighbours
(we will discuss how to choose \(K\) in the next chapter) and the straight-line
distance (\texttt{weight\_func\ =\ "rectangular"}). The \texttt{weight\_func} argument controls
how neighbours vote when classifying a new observation; by setting it to \texttt{"rectangular"},
each of the \(K\) nearest neighbours gets exactly 1 vote as described above. Other choices,
which weight each neighbour's vote differently, can be found on
\href{https://parsnip.tidymodels.org/reference/nearest_neighbor.html}{the tidymodels website}.
We specify the particular computational
engine (in this case, the \texttt{kknn} engine) for training the model with the \texttt{set\_engine} function.
Finally we specify that this is a classification problem with the \texttt{set\_mode} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knn\_spec }\OtherTok{\textless{}{-}} \FunctionTok{nearest\_neighbor}\NormalTok{(}\AttributeTok{weight\_func =} \StringTok{"rectangular"}\NormalTok{, }\AttributeTok{neighbors =} \DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"kknn"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{)}
\NormalTok{knn\_spec}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## K-Nearest Neighbor Model Specification (classification)
## 
## Main Arguments:
##   neighbors = 5
##   weight_func = rectangular
## 
## Computational engine: kknn
\end{verbatim}

In order to fit the model on the breast cancer data, we need to pass the model specification
and the dataset to the \texttt{fit} function. We also need to specify what variables to use as predictors
and what variable to use as the target. Below, the \texttt{Class\ \textasciitilde{}\ .} argument specifies
that \texttt{Class} is the target variable (the one we want to predict),
and \texttt{.} (everything \emph{except} \texttt{Class}) is to be used as the predictor.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knn\_fit }\OtherTok{\textless{}{-}}\NormalTok{ knn\_spec }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(Class }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ cancer\_train)}
\NormalTok{knn\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## parsnip model object
## 
## Fit time:  14ms 
## 
## Call:
## kknn::train.kknn(formula = Class ~ ., data = data, ks = min_rows(5,     data, 5), kernel = ~"rectangular")
## 
## Type of response variable: nominal
## Minimal misclassification: 0.07557
## Best kernel: rectangular
## Best k: 5
\end{verbatim}

Here you can see the final trained model summary. It confirms that the computational engine used
to train the model was \texttt{kknn::train.kknn}. It also shows the fraction of errors made by
the nearest neighbour model, but we will ignore this for now and discuss it in more detail
in the next chapter.
Finally it shows (somewhat confusingly) that the ``best'' weight function
was ``rectangular'' and ``best'' setting of \(K\) was 5; but since we specified these earlier,
R is just repeating those settings to us here. In the next chapter, we will actually
let R tune the model for us.

Finally, we make the prediction on the new observation by calling the \texttt{predict} function,
passing the fit object we just created. As above when we ran the K-nearest neighbours
classification algorithm manually, the \texttt{knn\_fit} object classifies the new observation as
malignant (``M''). Note that the \texttt{predict} function outputs a data frame with a single
variable named \texttt{.pred\_class}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{new\_obs }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{Perimeter =} \DecValTok{0}\NormalTok{, }\AttributeTok{Concavity =} \FloatTok{3.5}\NormalTok{)}
\FunctionTok{predict}\NormalTok{(knn\_fit, new\_obs)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   .pred_class
##   <fct>      
## 1 M
\end{verbatim}

\hypertarget{data-preprocessing-with-tidymodels}{%
\section{\texorpdfstring{Data preprocessing with \texttt{tidymodels}}{Data preprocessing with tidymodels}}\label{data-preprocessing-with-tidymodels}}

\hypertarget{centering-and-scaling}{%
\subsection{Centering and scaling}\label{centering-and-scaling}}

When using K-nearest neighbour classification, the \emph{scale} of each variable
(i.e., its size and range of values) matters. Since the classifier predicts
classes by identifying observations that are nearest to it, any variables that
have a large scale will have a much larger effect than variables with a small
scale. But just because a variable has a large scale \emph{doesn't mean} that it is
more important for making accurate predictions. For example, suppose you have a
data set with two attributes, salary (in dollars) and years of education, and
you want to predict the corresponding type of job. When we compute the
neighbour distances, a difference of \$1000 is huge compared to a difference of
10 years of education. But for our conceptual understanding and answering of
the problem, it's the opposite; 10 years of education is huge compared to a
difference of \$1000 in yearly salary!

In many other predictive models, the \emph{center} of each variable (e.g., its mean)
matters as well. For example, if we had a data set with a temperature variable
measured in degrees Kelvin, and the same data set with temperature measured in
degrees Celcius, the two variables would differ by a constant shift of 273
(even though they contain exactly the same information). Likewise in our
hypothetical job classification example, we would likely see that the center of
the salary variable is in the tens of thousands, while the center of the years
of education variable is in the single digits. Although this doesn't affect the
K-nearest neighbour classification algorithm, this large shift can change the
outcome of using many other predictive models.

\textbf{Standardization:} when all variables in a data set have a mean (center) of 0
and a standard deviation (scale) of 1, we say that the data have been
\emph{standardized}.

To illustrate the effect that standardization can have on the K-nearest
neighbour algorithm, we will read in the original, unscaled Wisconsin breast
cancer data set; we have been using a standardized version of the data set up
until now. To keep things simple, we will just use the \texttt{Area}, \texttt{Smoothness}, and \texttt{Class}
variables:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unscaled\_cancer }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/unscaled\_wdbc.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Class =} \FunctionTok{as\_factor}\NormalTok{(Class)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(Class, Area, Smoothness)}
\NormalTok{unscaled\_cancer}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 569 x 3
##    Class  Area Smoothness
##    <fct> <dbl>      <dbl>
##  1 M     1001      0.118 
##  2 M     1326      0.0847
##  3 M     1203      0.110 
##  4 M      386.     0.142 
##  5 M     1297      0.100 
##  6 M      477.     0.128 
##  7 M     1040      0.0946
##  8 M      578.     0.119 
##  9 M      520.     0.127 
## 10 M      476.     0.119 
## # ... with 559 more rows
\end{verbatim}

Looking at the unscaled / uncentered data above, you can see that the difference
between the values for area measurements are much larger than those for
smoothness, and the mean appears to be much larger too. Will this affect
predictions? In order to find out, we will create a scatter plot of these two
predictors (coloured by diagnosis) for both the unstandardized data we just
loaded, and the standardized version of that same data.

In the \texttt{tidymodels} framework, all data preprocessing happens using a \href{https://tidymodels.github.io/recipes/reference/index.html}{\texttt{recipe}}.
Here we will initialize a recipe for the \texttt{unscaled\_cancer} data above, specifying
that the \texttt{Class} variable is the target, and all other variables are predictors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{uc\_recipe }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(Class }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ unscaled\_cancer)}
\FunctionTok{print}\NormalTok{(uc\_recipe)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          2
\end{verbatim}

So far, there is not much in the recipe; just a statement about the number of targets
and predictors. Let's add scaling (\texttt{step\_scale}) and centering (\texttt{step\_center}) steps for
all of the predictors so that they each have a mean of 0 and standard deviation of 1.
The \texttt{prep} function finalizes the recipe by using the data (here, \texttt{unscaled\_cancer})
to compute anything necessary to run the recipe (in this case, the column means and standard
deviations):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{uc\_recipe }\OtherTok{\textless{}{-}}\NormalTok{ uc\_recipe }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_scale}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_center}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{prep}\NormalTok{()}
\NormalTok{uc\_recipe}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          2
## 
## Training data contained 569 data points and no missing data.
## 
## Operations:
## 
## Scaling for Area, Smoothness [trained]
## Centering for Area, Smoothness [trained]
\end{verbatim}

You can now see that the recipe includes a scaling and centering step for all predictor variables.
Note that when you add a step to a recipe, you must specify what columns to apply the step to.
Here we used the \texttt{all\_predictors()} function to specify that each step should be applied to
all predictor variables. However, there are a number of different arguments one could use here,
as well as naming particular columns with the same syntax as the \texttt{select} function.
For example:

\begin{itemize}
\tightlist
\item
  \texttt{all\_nominal()} and \texttt{all\_numeric()}: specify all categorical or all numeric variables
\item
  \texttt{all\_predictors()} and \texttt{all\_outcomes()}: specify all predictor or all target variables
\item
  \texttt{Area,\ Smoothness}: specify both the \texttt{Area} and \texttt{Smoothness} variable
\item
  \texttt{-Class}: specify everything except the \texttt{Class} variable
\end{itemize}

You can find \href{https://tidymodels.github.io/recipes/reference/index.html}{a full set of all the steps and variable selection functions}
on the recipes home page.
We finally use the \texttt{bake} function to apply the recipe.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scaled\_cancer }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(uc\_recipe, unscaled\_cancer)}
\NormalTok{scaled\_cancer}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 569 x 3
##      Area Smoothness Class
##     <dbl>      <dbl> <fct>
##  1  0.984      1.57  M    
##  2  1.91      -0.826 M    
##  3  1.56       0.941 M    
##  4 -0.764      3.28  M    
##  5  1.82       0.280 M    
##  6 -0.505      2.24  M    
##  7  1.09      -0.123 M    
##  8 -0.219      1.60  M    
##  9 -0.384      2.20  M    
## 10 -0.509      1.58  M    
## # ... with 559 more rows
\end{verbatim}

Now let's generate the two scatter plots, one for \texttt{unscaled\_cancer} and one for
\texttt{scaled\_cancer}, and show them side-by-side. Each has the same new observation
annotated with its \(K=3\) nearest neighbours:

\includegraphics{bookdown_files/figure-latex/05-scaling-plt-1.pdf}

In the plot for the nonstandardized original data, you can see some odd choices
for the three nearest neighbours. In particular, the ``neighbours'' are visually
well within the cloud of benign observations, and the neighbours are all nearly
vertically aligned with the new observation (which is why it looks like there
is only one black line on this plot). Here the computation of nearest
neighbours is dominated by the much larger-scale area variable. On the right,
the plot for standardized data shows a much more intuitively reasonable
selection of nearest neighbours. Thus, standardizing the data can change things
in an important way when we are using predictive algorithms. As a rule of
thumb, standardizing your data should be a part of the preprocessing you do
before any predictive modelling / analysis.

\hypertarget{balancing}{%
\subsection{Balancing}\label{balancing}}

Another potential issue in a data set for a classifier is \emph{class imbalance},
i.e., when one label is much more common than another. Since classifiers like
the K-nearest neighbour algorithm use the labels of nearby points to predict
the label of a new point, if there are many more data points with one label
overall, the algorithm is more likely to pick that label in general (even if
the ``pattern'' of data suggests otherwise). Class imbalance is actually quite a
common and important problem: from rare disease diagnosis to malicious email
detection, there are many cases in which the ``important'' class to identify
(presence of disease, malicious email) is much rarer than the ``unimportant''
class (no disease, normal email).

To better illustrate the problem, let's revisit the breast cancer data; except
now we will remove many of the observations of malignant tumours, simulating
what the data would look like if the cancer was rare. We will do this by
picking only 3 observations randomly from the malignant group, and keeping all
of the benign observations.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\NormalTok{rare\_cancer }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}
  \FunctionTok{filter}\NormalTok{(cancer, Class }\SpecialCharTok{==} \StringTok{"B"}\NormalTok{),}
\NormalTok{  cancer }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(Class }\SpecialCharTok{==} \StringTok{"M"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{sample\_n}\NormalTok{(}\DecValTok{3}\NormalTok{)}
\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(Class, Perimeter, Concavity)}

\NormalTok{rare\_plot }\OtherTok{\textless{}{-}}\NormalTok{ rare\_cancer }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Perimeter, }\AttributeTok{y =}\NormalTok{ Concavity, }\AttributeTok{color =}\NormalTok{ Class)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{color =} \StringTok{"Diagnosis"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Malignant"}\NormalTok{, }\StringTok{"Benign"}\NormalTok{), }\AttributeTok{values =}\NormalTok{ cbPalette)}
\NormalTok{rare\_plot}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/05-unbalanced-1.pdf}
\caption{\label{fig:05-unbalanced}Imbalanced data}
\end{figure}

\begin{quote}
Note: You will see in the code above that we use the \texttt{set.seed} function. This is because we are using \texttt{sample\_n} to artificially pick
only 3 of the malignant tumour observations, which uses random sampling to choose which rows will be in the training set. In order to
make the code reproducible, we use \texttt{set.seed} to specify where the random number generator starts for this
process, which then guarantees the same result, i.e., the same choice of 3 observations, each time the code is run. In general, when your
code involves random numbers, if you want \emph{the same result} each time, you should use \texttt{set.seed}; if you want a \emph{different result} each time,
you should not.
\end{quote}

Suppose we now decided to use \(K = 7\) in K-nearest neighbour classification.
With only 3 observations of malignant tumours, the classifier
will \emph{always predict that the tumour is benign, no matter what its concavity and perimeter
are!} This is because in a majority vote of 7 observations, at most 3 will be
malignant (we only have 3 total malignant observations), so at least 4 must be
benign, and the benign vote will always win. For example, look what happens for
a new tumour observation that is quite close to two that were tagged as
malignant:

\includegraphics{bookdown_files/figure-latex/05-upsample-1.pdf}

And if we set the background colour of each area of the plot to the decision the K-nearest neighbour
classifier would make, we can see that the decision is always ``benign,'' corresponding to the blue colour:

\includegraphics{bookdown_files/figure-latex/05-upsample-2-1.pdf}

Despite the simplicity of the problem, solving it in a statistically sound manner is actually
fairly nuanced, and a careful treatment would require a lot more detail and mathematics than we will cover in this textbook.
For the present purposes, it will suffice to rebalance the data by \emph{oversampling} the rare class.
In other words, we will replicate rare observations multiple times in our data set to give them more
voting power in the K-nearest neighbour algorithm. In order to do this, we will add an oversampling
step to the earlier \texttt{uc\_recipe} recipe with the \texttt{step\_upsample} function.
We show below how to do this, and also
use the \texttt{group\_by\ +\ summarize} pattern we've seen before to see that our classes are now balanced:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ups\_recipe }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(Class }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ rare\_cancer) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_upsample}\NormalTok{(Class, }\AttributeTok{over\_ratio =} \DecValTok{1}\NormalTok{, }\AttributeTok{skip =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{prep}\NormalTok{()}
\NormalTok{ups\_recipe}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor          2
## 
## Training data contained 360 data points and no missing data.
## 
## Operations:
## 
## Up-sampling based on Class [trained]
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{upsampled\_cancer }\OtherTok{\textless{}{-}} \FunctionTok{bake}\NormalTok{(ups\_recipe, rare\_cancer)}

\NormalTok{upsampled\_cancer }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Class) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 2
##   Class     n
##   <fct> <int>
## 1 M       357
## 2 B       357
\end{verbatim}

Now suppose we train our K-nearest neighbour classifier with \(K=7\) on this \emph{balanced} data. Setting the background colour
of each area of our scatter plot to the decision the K-nearest neighbour
classifier would make, we can see that the decision is more reasonable; when the points are close
to those labelled malignant, the classifier predicts a malignant tumour, and vice versa when they are closer to the benign tumour observations:

\includegraphics{bookdown_files/figure-latex/05-upsample-plot-1.pdf}

\hypertarget{putting-it-together-in-a-workflow}{%
\section{\texorpdfstring{Putting it together in a \texttt{workflow}}{Putting it together in a workflow}}\label{putting-it-together-in-a-workflow}}

The \texttt{tidymodels} package collection also provides the \texttt{workflow}, a simple way to chain
together multiple data analysis steps without a lot of otherwise necessary code for intermediate steps.
To illustrate the whole pipeline, let's start from scratch with the \texttt{unscaled\_wdbc.csv} data.
First we will load the data, create a model, and specify a recipe for how the data should be preprocessed:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load the unscaled cancer data and make sure the target Class variable is a factor}
\NormalTok{unscaled\_cancer }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/unscaled\_wdbc.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Class =} \FunctionTok{as\_factor}\NormalTok{(Class))}

\CommentTok{\# create the KNN model}
\NormalTok{knn\_spec }\OtherTok{\textless{}{-}} \FunctionTok{nearest\_neighbor}\NormalTok{(}\AttributeTok{weight\_func =} \StringTok{"rectangular"}\NormalTok{, }\AttributeTok{neighbors =} \DecValTok{7}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"kknn"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{)}

\CommentTok{\# create the centering / scaling recipe}
\NormalTok{uc\_recipe }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(Class }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Area }\SpecialCharTok{+}\NormalTok{ Smoothness, }\AttributeTok{data =}\NormalTok{ unscaled\_cancer) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_scale}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_center}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

Note that each of these steps is exactly the same as earlier, except for one major difference:
we did not use the \texttt{select} function to extract the relevant variables from the data frame,
and instead simply specified the relevant variables to use via the
formula \texttt{Class\ \textasciitilde{}\ Area\ +\ Smoothness} (instead of \texttt{Class\ \textasciitilde{}\ .}) in the recipe.
You will also notice that we did not call \texttt{prep()} on the recipe; this is unnecssary when it is
placed in a workflow.

We will now place these steps in a \texttt{workflow} using the \texttt{add\_recipe} and \texttt{add\_model} functions,
and finally we will use the \texttt{fit} function to run the whole workflow on the \texttt{unscaled\_cancer} data.
Note another difference from earlier here: we do not include a formula in the \texttt{fit} function. This
is again because we included the formula in the recipe, so there is no need to respecify it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knn\_fit }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(uc\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(knn\_spec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}\AttributeTok{data =}\NormalTok{ unscaled\_cancer)}
\NormalTok{knn\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## == Workflow [trained] =================================
## Preprocessor: Recipe
## Model: nearest_neighbor()
## 
## -- Preprocessor ---------------------------------------
## 2 Recipe Steps
## 
## * step_scale()
## * step_center()
## 
## -- Model ----------------------------------------------
## 
## Call:
## kknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(7,     data, 5), kernel = ~"rectangular")
## 
## Type of response variable: nominal
## Minimal misclassification: 0.1125
## Best kernel: rectangular
## Best k: 7
\end{verbatim}

As before, the fit object lists the function that trains the model as well as the ``best'' settings
for the number of neighbours and weight function (for now, these are just the values we chose
manually when we created \texttt{knn\_spec} above). But now the fit object also includes information about
the overall workflow, including the centering and scaling preprocessing steps.

Let's visualize the predictions that this trained K-nearest neighbour model will make on new observations.
Below you will see how to make the coloured prediction map plots from earlier in this chapter.
The basic idea is to create a grid of synthetic new observations using the \texttt{expand.grid} function,
predict the label of each, and visualize the predictions with a coloured scatter having a very high transparency
(low \texttt{alpha} value) and large point radius. We include the code here as a learning challenge; see
if you can figure out what each line is doing!

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create the grid of area/smoothness vals, and arrange in a data frame}
\NormalTok{are\_grid }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FunctionTok{min}\NormalTok{(unscaled\_cancer}\SpecialCharTok{$}\NormalTok{Area), }\FunctionTok{max}\NormalTok{(unscaled\_cancer}\SpecialCharTok{$}\NormalTok{Area), }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{smo\_grid }\OtherTok{\textless{}{-}} \FunctionTok{seq}\NormalTok{(}\FunctionTok{min}\NormalTok{(unscaled\_cancer}\SpecialCharTok{$}\NormalTok{Smoothness), }\FunctionTok{max}\NormalTok{(unscaled\_cancer}\SpecialCharTok{$}\NormalTok{Smoothness), }\AttributeTok{length.out =} \DecValTok{100}\NormalTok{)}
\NormalTok{asgrid }\OtherTok{\textless{}{-}} \FunctionTok{as\_tibble}\NormalTok{(}\FunctionTok{expand.grid}\NormalTok{(}\AttributeTok{Area =}\NormalTok{ are\_grid, }\AttributeTok{Smoothness =}\NormalTok{ smo\_grid))}

\CommentTok{\# use the fit workflow to make predictions at the grid points}
\NormalTok{knnPredGrid }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(knn\_fit, asgrid)}

\CommentTok{\# bind the predictions as a new column with the grid points}
\NormalTok{prediction\_table }\OtherTok{\textless{}{-}} \FunctionTok{bind\_cols}\NormalTok{(knnPredGrid, asgrid) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{rename}\NormalTok{(}\AttributeTok{Class =}\NormalTok{ .pred\_class)}

\CommentTok{\# plot:}
\CommentTok{\# 1. the coloured scatter of the original data}
\CommentTok{\# 2. the faded coloured scatter for the grid points}
\NormalTok{wkflw\_plot }\OtherTok{\textless{}{-}}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ unscaled\_cancer, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Area, }\AttributeTok{y =}\NormalTok{ Smoothness, }\AttributeTok{color =}\NormalTok{ Class), }\AttributeTok{alpha =} \FloatTok{0.75}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{data =}\NormalTok{ prediction\_table, }\AttributeTok{mapping =} \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Area, }\AttributeTok{y =}\NormalTok{ Smoothness, }\AttributeTok{color =}\NormalTok{ Class), }\AttributeTok{alpha =} \FloatTok{0.02}\NormalTok{, }\AttributeTok{size =} \FloatTok{5.}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{color =} \StringTok{"Diagnosis"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Malignant"}\NormalTok{, }\StringTok{"Benign"}\NormalTok{), }\AttributeTok{values =}\NormalTok{ cbPalette)}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/05-workflow-plot-show-1.pdf}
\caption{\label{fig:05-workflow-plot-show}Scatterplot of smoothness versus area where background colour indicates the decision of the classifier}
\end{figure}

\hypertarget{classification_continued}{%
\chapter{Classification II: evaluation \& tuning}\label{classification_continued}}

\hypertarget{overview-5}{%
\section{Overview}\label{overview-5}}

This chapter continues the introduction to predictive modelling through
classification. While the previous chapter covered training and data
preprocessing, this chapter focuses on how to split data, how to evaluate
prediction accuracy, and how to choose model parameters to maximize
performance.

\hypertarget{chapter-learning-objectives-6}{%
\section{Chapter learning objectives}\label{chapter-learning-objectives-6}}

By the end of the chapter, students will be able to:

\begin{itemize}
\tightlist
\item
  Describe what training, validation, and test data sets are and how they are used in classification
\item
  Split data into training, validation, and test data sets
\item
  Evaluate classification accuracy in R using a validation data set and appropriate metrics
\item
  Execute cross-validation in R to choose the number of neighbours in a K-nearest neighbour classifier
\item
  Describe advantages and disadvantages of the K-nearest neighbour classification algorithm
\end{itemize}

\hypertarget{evaluating-accuracy}{%
\section{Evaluating accuracy}\label{evaluating-accuracy}}

Sometimes our classifier might make the wrong prediction. A classifier does not
need to be right 100\% of the time to be useful, though we don't want the
classifier to make too many wrong predictions. How do we measure how ``good'' our
classifier is? Let's revisit the
\href{http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+\%28Diagnostic\%29}{breast cancer images example}
and think about how our classifier will be used in practice. A biopsy will be
performed on a \emph{new} patient's tumour, the resulting image will be analyzed,
and the classifier will be asked to decide whether the tumour is benign or
malignant. The key word here is \emph{new}: our classifier is ``good'' if it provides
accurate predictions on data \emph{not seen during training}. But then how can we
evaluate our classifier without having to visit the hospital to collect more
tumour images?

The trick is to split up the data set into a \textbf{training set} and \textbf{test set},
and only show the classifier the \textbf{training set} when building the classifier.
Then to evaluate the accuracy of the classifier, we can use it to predict the
labels (which we know) in the \textbf{test set}. If our predictions match the true
labels for the observations in the \textbf{test set} very well, then we have some
confidence that our classifier might also do a good job of predicting the class
labels for new observations that we do not have the class labels for.

\begin{quote}
Note: if there were a golden rule of machine learning, it might be this: \emph{you cannot use the test data to build the model!}
If you do, the model gets to ``see'' the test data in advance, making it look more accurate than it really is. Imagine
how bad it would be to overestimate your classifier's accuracy when predicting whether a patient's tumour is malignant or benign!
\end{quote}

\begin{figure}
\includegraphics[width=1\linewidth]{img/training_test} \caption{Splitting the data into training and testing sets}\label{fig:06-training-test}
\end{figure}

How exactly can we assess how well our predictions match the true labels for
the observations in the test set? One way we can do this is to calculate the
\textbf{prediction accuracy}. This is the fraction of examples for which the
classifier made the correct prediction. To calculate this we divide the number
of correct predictions by the number of predictions made. Other measures for
how well our classifier performed include \emph{precision} and \emph{recall}; these will
not be discussed here, but you will encounter them in other more advanced
courses on this topic. This process is illustrated below:

\begin{figure}
\includegraphics[width=1\linewidth]{img/ML-paradigm-test} \caption{Process for splitting the data and finding the prediction accuracy}\label{fig:06-ML-paradigm-test}
\end{figure}

In R, we can use the \texttt{tidymodels} library collection not only to perform K-nearest neighbour
classification, but also to assess how well our classification worked. Let's
start by loading the necessary libraries, reading in the breast cancer data
from the previous chapter, and making a quick scatter plot visualization of
tumour cell concavity versus smoothness coloured by diagnosis.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# load packages}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(tidymodels)}

\CommentTok{\# load data}
\NormalTok{cancer }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/unscaled\_wdbc.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Class =} \FunctionTok{as\_factor}\NormalTok{(Class)) }\CommentTok{\# convert the character Class variable to the factor datatype}

\CommentTok{\# colour palette}
\NormalTok{cbPalette }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"\#E69F00"}\NormalTok{, }\StringTok{"\#56B4E9"}\NormalTok{, }\StringTok{"\#009E73"}\NormalTok{, }\StringTok{"\#F0E442"}\NormalTok{, }\StringTok{"\#0072B2"}\NormalTok{, }\StringTok{"\#D55E00"}\NormalTok{, }\StringTok{"\#CC79A7"}\NormalTok{, }\StringTok{"\#999999"}\NormalTok{)}

\CommentTok{\# create scatter plot of tumour cell concavity versus smoothness,}
\CommentTok{\# labelling the points be diagnosis class}
\NormalTok{perim\_concav }\OtherTok{\textless{}{-}}\NormalTok{ cancer }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Smoothness, }\AttributeTok{y =}\NormalTok{ Concavity, }\AttributeTok{color =}\NormalTok{ Class)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{color =} \StringTok{"Diagnosis"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_color\_manual}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Malignant"}\NormalTok{, }\StringTok{"Benign"}\NormalTok{), }\AttributeTok{values =}\NormalTok{ cbPalette)}

\NormalTok{perim\_concav}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/06-precode-1.pdf}
\caption{\label{fig:06-precode}Scatterplot of tumour cell concavity versus smoothness coloured by diagnosis label}
\end{figure}

\textbf{1. Create the train / test split}

Once we have decided on a predictive question to answer and done some
preliminary exploration, the very next thing to do is to split the data into
the training and test sets. Typically, the training set is between 50 - 100\% of
the data, while the test set is the remaining 0 - 50\%; the intuition is that
you want to trade off between training an accurate model (by using a larger
training data set) and getting an accurate evaluation of its performance (by
using a larger test data set). Here, we will use 75\% of the data for training,
and 25\% for testing. To do this we will use the \texttt{initial\_split} function,
specifying that \texttt{prop\ =\ 0.75} and the target variable is \texttt{Class}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{cancer\_split }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(cancer, }\AttributeTok{prop =} \FloatTok{0.75}\NormalTok{, }\AttributeTok{strata =}\NormalTok{ Class)}
\NormalTok{cancer\_train }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(cancer\_split)}
\NormalTok{cancer\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(cancer\_split)}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Note: You will see in the code above that we use the \texttt{set.seed} function again, as discussed in the previous chapter. In this case it is because
\texttt{initial\_split} uses random sampling to choose which rows will be in the training set. Since we want our code to be reproducible
and generate the same train/test split each time it is run, we use \texttt{set.seed}.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(cancer\_train)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 427
## Columns: 12
## $ ID                <dbl> 842302, 842517, 84300903...
## $ Class             <fct> M, M, M, M, M, M, M, M, ...
## $ Radius            <dbl> 17.990, 20.570, 19.690, ...
## $ Texture           <dbl> 10.38, 17.77, 21.25, 20....
## $ Perimeter         <dbl> 122.80, 132.90, 130.00, ...
## $ Area              <dbl> 1001.0, 1326.0, 1203.0, ...
## $ Smoothness        <dbl> 0.11840, 0.08474, 0.1096...
## $ Compactness       <dbl> 0.27760, 0.07864, 0.1599...
## $ Concavity         <dbl> 0.30010, 0.08690, 0.1974...
## $ Concave_Points    <dbl> 0.14710, 0.07017, 0.1279...
## $ Symmetry          <dbl> 0.2419, 0.1812, 0.2069, ...
## $ Fractal_Dimension <dbl> 0.07871, 0.05667, 0.0599...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(cancer\_test)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 142
## Columns: 12
## $ ID                <dbl> 844981, 84799002, 848406...
## $ Class             <fct> M, M, M, M, B, M, M, M, ...
## $ Radius            <dbl> 13.000, 14.540, 14.680, ...
## $ Texture           <dbl> 21.82, 27.54, 20.13, 22....
## $ Perimeter         <dbl> 87.50, 96.73, 94.74, 130...
## $ Area              <dbl> 519.8, 658.8, 684.5, 126...
## $ Smoothness        <dbl> 0.12730, 0.11390, 0.0986...
## $ Compactness       <dbl> 0.19320, 0.15950, 0.0720...
## $ Concavity         <dbl> 0.18590, 0.16390, 0.0739...
## $ Concave_Points    <dbl> 0.093530, 0.073640, 0.05...
## $ Symmetry          <dbl> 0.2350, 0.2303, 0.1586, ...
## $ Fractal_Dimension <dbl> 0.07389, 0.07077, 0.0592...
\end{verbatim}

We can see from \texttt{glimpse} in the code above that the training set contains 427
observations, while the test set contains 142 observations. This corresponds to
a train / test split of 75\% / 25\%, as desired.

\textbf{2. Pre-process the data}

As we mentioned last chapter, K-NN is sensitive to the scale of the predictors,
and so we should perform some preprocessing to standardize them. An
additional consideration we need to take when doing this is that we should
create the standardization preprocessor using \textbf{only the training data}. This ensures that
our test data does not influence any aspect of our model training. Once we have
created the standardization preprocessor, we can then apply it separately to both the
training and test data sets.

Fortunately, the \texttt{recipe} framework from \texttt{tidymodels} makes it simple to handle
this properly. Below we construct and prepare the recipe using only the training
data (due to \texttt{data\ =\ cancer\_train} in the first line).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cancer\_recipe }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(Class }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Smoothness }\SpecialCharTok{+}\NormalTok{ Concavity, }\AttributeTok{data =}\NormalTok{ cancer\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_scale}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_center}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\textbf{3. Train the classifier}

Now that we have split our original data set into training and test sets, we
can create our K-nearest neighbour classifier with only the training set using
the technique we learned in the previous chapter. For now, we will just choose
the number \(K\) of neighbours to be 3, and use concavity and smoothness as the
predictors.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{knn\_spec }\OtherTok{\textless{}{-}} \FunctionTok{nearest\_neighbor}\NormalTok{(}\AttributeTok{weight\_func =} \StringTok{"rectangular"}\NormalTok{, }\AttributeTok{neighbors =} \DecValTok{3}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"kknn"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{)}

\NormalTok{knn\_fit }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(cancer\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(knn\_spec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}\AttributeTok{data =}\NormalTok{ cancer\_train)}

\NormalTok{knn\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## == Workflow [trained] =================================
## Preprocessor: Recipe
## Model: nearest_neighbor()
## 
## -- Preprocessor ---------------------------------------
## 2 Recipe Steps
## 
## * step_scale()
## * step_center()
## 
## -- Model ----------------------------------------------
## 
## Call:
## kknn::train.kknn(formula = ..y ~ ., data = data, ks = min_rows(3,     data, 5), kernel = ~"rectangular")
## 
## Type of response variable: nominal
## Minimal misclassification: 0.1265
## Best kernel: rectangular
## Best k: 3
\end{verbatim}

\begin{quote}
Note: Here again you see the \texttt{set.seed} function. In the K-nearest neighbour algorithm,
there is a tie for the majority neighbour class, the winner is randomly selected. Although there is no chance
of a tie when \(K\) is odd (here \(K=3\)), it is possible that the code may be changed in the future to have an even value of \(K\).
Thus, to prevent potential issues with reproducibility, we have set the seed. Note that in your own code,
you only have to set the seed once at the beginning of your analysis.
\end{quote}

\textbf{4. Predict the labels in the test set}

Now that we have a K-nearest neighbour classifier object, we can use it to
predict the class labels for our test set. We use the \texttt{bind\_cols} to add the
column of predictions to the original test data, creating the
\texttt{cancer\_test\_predictions} data frame. The \texttt{Class} variable contains the true
diagnoses, while the \texttt{.pred\_class} contains the predicted diagnoses from the
model.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cancer\_test\_predictions }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(knn\_fit, cancer\_test) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_cols}\NormalTok{(cancer\_test)}
\NormalTok{cancer\_test\_predictions}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 142 x 13
##    .pred_class     ID Class Radius Texture Perimeter
##    <fct>        <dbl> <fct>  <dbl>   <dbl>     <dbl>
##  1 M           8.45e5 M       13      21.8      87.5
##  2 M           8.48e7 M       14.5    27.5      96.7
##  3 B           8.48e5 M       14.7    20.1      94.7
##  4 M           8.49e5 M       19.8    22.2     130  
##  5 B           8.51e6 B       13.5    14.4      87.5
##  6 M           8.51e6 M       15.3    14.3     102. 
##  7 M           8.53e5 M       18.6    25.1     125. 
##  8 M           8.54e5 M       19.3    26.5     128. 
##  9 B           8.55e5 M       13.4    21.6      86.2
## 10 M           8.56e5 M       13.3    20.3      87.3
## # ... with 132 more rows, and 7 more variables:
## #   Area <dbl>, Smoothness <dbl>, Compactness <dbl>,
## #   Concavity <dbl>, Concave_Points <dbl>,
## #   Symmetry <dbl>, Fractal_Dimension <dbl>
\end{verbatim}

\textbf{5. Compute the accuracy}

Finally we can assess our classifier's accuracy. To do this we use the \texttt{metrics} function
from \texttt{tidymodels} to get the statistics about the quality of our model, specifying
the \texttt{truth} and \texttt{estimate} arguments:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cancer\_test\_predictions }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{metrics}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ Class, }\AttributeTok{estimate =}\NormalTok{ .pred\_class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   <chr>    <chr>          <dbl>
## 1 accuracy binary         0.880
## 2 kap      binary         0.741
\end{verbatim}

This shows that the accuracy of the classifier on the test data was 88\%.
We can also look at the \emph{confusion matrix} for the classifier, which shows
the table of predicted labels and correct labels, using the \texttt{conf\_mat} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cancer\_test\_predictions }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{conf\_mat}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ Class, }\AttributeTok{estimate =}\NormalTok{ .pred\_class)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##           Truth
## Prediction  M  B
##          M 43  7
##          B 10 82
\end{verbatim}

This says that the classifier labelled 43+82 = 125 observations correctly,
10 observations as benign when they were truly malignant,
and 7 observations as malignant when they were truly benign.

\hypertarget{tuning-the-classifier}{%
\section{Tuning the classifier}\label{tuning-the-classifier}}

The vast majority of predictive models in statistics and machine learning have
\emph{parameters} that you have to pick. For example, in the K-nearest neighbour
classification algorithm we have been using in the past two chapters, we have
had to pick the number of neighbours \(K\) for the class vote. Is it possible to
make this selection, i.e., \emph{tune} the model, in a principled way? Ideally what
we want is to somehow maximize the performance of our classifier on data \emph{it
hasn't seen yet}. So we will play the same trick we did before when evaluating
our classifier: we'll split our \textbf{overall training data set} further into two
subsets, called the \textbf{training set} and \textbf{validation set}. We will use the
newly-named \textbf{training set} for building the classifier, and the \textbf{validation
set} for evaluating it! Then we will try different values of the parameter \(K\)
and pick the one that yields the highest accuracy.

\begin{quote}
\textbf{Remember:} \emph{don't touch the test set during the tuning process. Tuning is a part of model training!}
\end{quote}

\hypertarget{cross-validation}{%
\subsection{Cross-validation}\label{cross-validation}}

There is an important detail to mention about the process of tuning: we can, if
we want to, split our overall training data up in multiple different ways,
train and evaluate a classifier for each split, and then choose the parameter
based on \textbf{\emph{all}} of the different results. If we just split our overall training
data \emph{once}, our best parameter choice will depend strongly on whatever data
was lucky enough to end up in the validation set. Perhaps using multiple
different train / validation splits, we'll get a better estimate of accuracy,
which will lead to a better choice of the number of neighbours \(K\) for the
overall set of training data.

\begin{quote}
\textbf{Note:} you might be wondering why we can't we use the multiple splits to test our final classifier after tuning is done. This is simply
because at the end of the day, we will produce a single classifier using our overall training data. If we do multiple train / test splits, we will
end up with multiple classifiers, each with their own accuracy evaluated on different test data.
\end{quote}

Let's investigate this idea in R! In particular, we will use different seed
values in the \texttt{set.seed} function to generate five different train / validation
splits of our overall training data, train five different K-nearest neighbour
models, and evaluate their accuracy.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{accuracies }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) \{}
  \FunctionTok{set.seed}\NormalTok{(i) }\CommentTok{\# makes the random selection of rows reproducible}

  \CommentTok{\# create the 25/75 split of the training data into training and validation}
\NormalTok{  cancer\_split }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(cancer\_train, }\AttributeTok{prop =} \FloatTok{0.75}\NormalTok{, }\AttributeTok{strata =}\NormalTok{ Class)}
\NormalTok{  cancer\_subtrain }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(cancer\_split)}
\NormalTok{  cancer\_validation }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(cancer\_split)}

  \CommentTok{\# recreate the standardization recipe from before (since it must be based on the training data)}
\NormalTok{  cancer\_recipe }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(Class }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Smoothness }\SpecialCharTok{+}\NormalTok{ Concavity, }\AttributeTok{data =}\NormalTok{ cancer\_subtrain) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{step\_scale}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{step\_center}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{())}

  \CommentTok{\# fit the knn model (we can reuse the old knn\_spec model from before)}
\NormalTok{  knn\_fit }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{add\_recipe}\NormalTok{(cancer\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{add\_model}\NormalTok{(knn\_spec) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{fit}\NormalTok{(}\AttributeTok{data =}\NormalTok{ cancer\_subtrain)}

  \CommentTok{\# get predictions on the validation data}
\NormalTok{  validation\_predicted }\OtherTok{\textless{}{-}} \FunctionTok{predict}\NormalTok{(knn\_fit, cancer\_validation) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{bind\_cols}\NormalTok{(cancer\_validation)}

  \CommentTok{\# compute the accuracy}
\NormalTok{  acc }\OtherTok{\textless{}{-}}\NormalTok{ validation\_predicted }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{metrics}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ Class, }\AttributeTok{estimate =}\NormalTok{ .pred\_class) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{filter}\NormalTok{(.metric }\SpecialCharTok{==} \StringTok{"accuracy"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(.estimate) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{pull}\NormalTok{()}
\NormalTok{  accuracies }\OtherTok{\textless{}{-}} \FunctionTok{append}\NormalTok{(accuracies, acc)}
\NormalTok{\}}
\NormalTok{accuracies}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.9151 0.8679 0.8491 0.8962 0.9151
\end{verbatim}

With five different shuffles of the data, we get five different values for
accuracy. None of these is necessarily ``more correct'' than any other; they're
just five estimates of the true, underlying accuracy of our classifier built
using our overall training data. We can combine the estimates by taking their
average (here 0.8887) to try to get a single assessment of our
classifier's accuracy; this has the effect of reducing the influence of any one
(un)lucky validation set on the estimate.

In practice, we don't use random splits, but rather use a more structured
splitting procedure so that each observation in the data set is used in a
validation set only a single time. The name for this strategy is called
\textbf{cross-validation}. In \textbf{cross-validation}, we split our \textbf{overall training
data} into \(C\) evenly-sized chunks, and then iteratively use \(1\) chunk as the
\textbf{validation set} and combine the remaining \(C-1\) chunks
as the \textbf{training set}:

\begin{figure}
\includegraphics[width=1\linewidth]{img/cv} \caption{5-fold cross validation}\label{fig:06-cv-image}
\end{figure}

In the picture above, \(C=5\) different chunks of the data set are used,
resulting in 5 different choices for the \textbf{validation set}; we call this
\emph{5-fold} cross-validation. To do 5-fold cross-validation in R with \texttt{tidymodels}, we
use another function: \texttt{vfold\_cv}. This function splits our training data into
\texttt{v} folds automatically:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cancer\_vfold }\OtherTok{\textless{}{-}} \FunctionTok{vfold\_cv}\NormalTok{(cancer\_train, }\AttributeTok{v =} \DecValTok{5}\NormalTok{, }\AttributeTok{strata =}\NormalTok{ Class)}
\NormalTok{cancer\_vfold}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## #  5-fold cross-validation using stratification 
## # A tibble: 5 x 2
##   splits           id   
##   <list>           <chr>
## 1 <split [341/86]> Fold1
## 2 <split [341/86]> Fold2
## 3 <split [341/86]> Fold3
## 4 <split [342/85]> Fold4
## 5 <split [343/84]> Fold5
\end{verbatim}

Then, when we create our data analysis workflow, we use the \texttt{fit\_resamples} function
instead of the \texttt{fit} function for training. This runs cross-validation on each
train/validation split.

\begin{quote}
\textbf{Note:} we set the seed when we call \texttt{train} not only because of the potential for ties, but also because we are doing
cross-validation. Cross-validation uses a random process to select how to partition the training data.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}

\CommentTok{\# recreate the standardization recipe from before (since it must be based on the training data)}
\NormalTok{cancer\_recipe }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(Class }\SpecialCharTok{\textasciitilde{}}\NormalTok{ Smoothness }\SpecialCharTok{+}\NormalTok{ Concavity, }\AttributeTok{data =}\NormalTok{ cancer\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_scale}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_center}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{())}

\CommentTok{\# fit the knn model (we can reuse the old knn\_spec model from before)}
\NormalTok{knn\_fit }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(cancer\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(knn\_spec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit\_resamples}\NormalTok{(}\AttributeTok{resamples =}\NormalTok{ cancer\_vfold)}

\NormalTok{knn\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # Resampling results
## # 5-fold cross-validation using stratification 
## # A tibble: 5 x 4
##   splits          id    .metrics         .notes        
##   <list>          <chr> <list>           <list>        
## 1 <split [341/86~ Fold1 <tibble [2 x 4]> <tibble [0 x ~
## 2 <split [341/86~ Fold2 <tibble [2 x 4]> <tibble [0 x ~
## 3 <split [341/86~ Fold3 <tibble [2 x 4]> <tibble [0 x ~
## 4 <split [342/85~ Fold4 <tibble [2 x 4]> <tibble [0 x ~
## 5 <split [343/84~ Fold5 <tibble [2 x 4]> <tibble [0 x ~
\end{verbatim}

The \texttt{collect\_metrics} function is used to aggregate the mean and \emph{standard error}
of the classifier's validation accuracy across the folds. The standard error is
a measure of how uncertain we are in the mean value. A detailed treatment of this
is beyond the scope of this chapter; but roughly, if your estimated mean (that
the \texttt{collect\_metrics} function gives you) is 0.88 and standard
error is 0.02, you can expect the \emph{true} average accuracy of the
classifier to be somewhere roughly between 0.86 and 0.90 (although it may
fall outside this range).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knn\_fit }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 6
##   .metric  .estimator  mean     n std_err .config      
##   <chr>    <chr>      <dbl> <int>   <dbl> <chr>        
## 1 accuracy binary     0.883     5  0.0189 Preprocessor~
## 2 roc_auc  binary     0.923     5  0.0104 Preprocessor~
\end{verbatim}

We can choose any number of folds, and typically the more we use the better our
accuracy estimate will be (lower standard error). However, we are limited
by computational power: the
more folds we choose, the more computation it takes, and hence the more time
it takes to run the analysis. So when you do cross-validation, you need to
consider the size of the data, and the speed of the algorithm (e.g., K-nearest
neighbour) and the speed of your computer. In practice, this is a trial and
error process, but typically \(C\) is chosen to be either 5 or 10. Here we show
how the standard error decreases when we use 10-fold cross validation rather
than 5-fold:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cancer\_vfold }\OtherTok{\textless{}{-}} \FunctionTok{vfold\_cv}\NormalTok{(cancer\_train, }\AttributeTok{v =} \DecValTok{10}\NormalTok{, }\AttributeTok{strata =}\NormalTok{ Class)}

\FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(cancer\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(knn\_spec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit\_resamples}\NormalTok{(}\AttributeTok{resamples =}\NormalTok{ cancer\_vfold) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_metrics}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 6
##   .metric  .estimator  mean     n std_err .config      
##   <chr>    <chr>      <dbl> <int>   <dbl> <chr>        
## 1 accuracy binary     0.873    10  0.0184 Preprocessor~
## 2 roc_auc  binary     0.922    10  0.0147 Preprocessor~
\end{verbatim}

\hypertarget{parameter-value-selection}{%
\subsection{Parameter value selection}\label{parameter-value-selection}}

Using 5- and 10-fold cross-validation, we have estimated that the prediction
accuracy of our classifier is somewhere around 88\%. Whether 88\% is good or not
depends entirely on the downstream application of the data analysis. In the
present situation, we are trying to predict a tumour diagnosis, with expensive,
damaging chemo/radiation therapy or patient death as potential consequences of
misprediction. Hence, we'd like to do better than 88\% for this application.

In order to improve our classifier, we have one choice of parameter: the number of
neighbours, \(K\). Since cross-validation helps us evaluate the accuracy of our
classifier, we can use cross-validation to calculate an accuracy for each value
of \(K\) in a reasonable range, and then pick the value of \(K\) that gives us the
best accuracy. The \texttt{tidymodels} package collection provides a very simple
syntax for tuning models: each parameter in the model to be tuned should be specified
as \texttt{tune()} in the model specification rather than given a particular value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knn\_spec }\OtherTok{\textless{}{-}} \FunctionTok{nearest\_neighbor}\NormalTok{(}\AttributeTok{weight\_func =} \StringTok{"rectangular"}\NormalTok{, }\AttributeTok{neighbors =} \FunctionTok{tune}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"kknn"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"classification"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then instead of using \texttt{fit} or \texttt{fit\_resamples}, we will use the \texttt{tune\_grid} function
to fit the model for each value in a range of parameter values. Here the \texttt{grid\ =\ 10}
argument specifies that the tuning should try 10 values of the number of neighbours
\(K\) when tuning. We set the seed prior to tuning to ensure results are reproducible:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{knn\_results }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(cancer\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(knn\_spec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tune\_grid}\NormalTok{(}\AttributeTok{resamples =}\NormalTok{ cancer\_vfold, }\AttributeTok{grid =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_metrics}\NormalTok{()}
\NormalTok{knn\_results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 20 x 7
##    neighbors .metric .estimator  mean     n std_err
##        <int> <chr>   <chr>      <dbl> <int>   <dbl>
##  1         2 accura~ binary     0.848    10  0.0146
##  2         2 roc_auc binary     0.897    10  0.0149
##  3         3 accura~ binary     0.873    10  0.0184
##  4         3 roc_auc binary     0.922    10  0.0147
##  5         5 accura~ binary     0.880    10  0.0164
##  6         5 roc_auc binary     0.928    10  0.0136
##  7         6 accura~ binary     0.880    10  0.0164
##  8         6 roc_auc binary     0.932    10  0.0144
##  9         7 accura~ binary     0.889    10  0.0167
## 10         7 roc_auc binary     0.934    10  0.0135
## 11         9 accura~ binary     0.878    10  0.0154
## 12         9 roc_auc binary     0.940    10  0.0125
## 13        10 accura~ binary     0.878    10  0.0154
## 14        10 roc_auc binary     0.943    10  0.0123
## 15        12 accura~ binary     0.882    10  0.0154
## 16        12 roc_auc binary     0.941    10  0.0124
## 17        13 accura~ binary     0.875    10  0.0136
## 18        13 roc_auc binary     0.942    10  0.0125
## 19        15 accura~ binary     0.873    10  0.0132
## 20        15 roc_auc binary     0.949    10  0.0127
## # ... with 1 more variable: .config <chr>
\end{verbatim}

We can select the best value of the number of neighbours (i.e., the one that results
in the highest classifier accuracy estimate) by plotting the accuracy versus \(K\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{accuracies }\OtherTok{\textless{}{-}}\NormalTok{ knn\_results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(.metric }\SpecialCharTok{==} \StringTok{"accuracy"}\NormalTok{)}

\NormalTok{accuracy\_vs\_k }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(accuracies, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ neighbors, }\AttributeTok{y =}\NormalTok{ mean)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Neighbors"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Accuracy Estimate"}\NormalTok{)}
\NormalTok{accuracy\_vs\_k}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/06-find-k-1.pdf}
\caption{\label{fig:06-find-k}Plot of accuracy estimate versus number of neighbours}
\end{figure}

This visualization suggests that \(K = 7\) provides the highest accuracy.
But as you can see, there is no exact or perfect answer here;
any selection between \(K = 3\) and \(13\) would be reasonably justified, as all
of these differ in classifier accuracy by less than 1\%. Remember: the
values you see on this plot are \emph{estimates} of the true accuracy of our
classifier. Although the \(K=7\) value is higher than the others on this plot,
that doesn't mean the classifier is actually more accurate with this parameter
value! Generally, when selecting \(K\) (and other parameters for other predictive
models), we are looking for a value where:

\begin{itemize}
\tightlist
\item
  we get roughly optimal accuracy, so that our model will likely be accurate
\item
  changing the value to a nearby one (e.g.~from \(K=7\) to 6 or 8) doesn't decrease accuracy too much, so that our choice is reliable in the presence of uncertainty
\item
  the cost of training the model is not prohibitive (e.g., in our situation, if \(K\) is too large, predicting becomes expensive!)
\end{itemize}

\hypertarget{underoverfitting}{%
\subsection{Under/overfitting}\label{underoverfitting}}

To build a bit more intuition, what happens if we keep increasing the number of neighbours \(K\)? In fact, the accuracy
actually starts to decrease! Rather than setting \texttt{grid\ =\ 10} and letting \texttt{tidymodels} decide what values of \(K\) to try,
let's specify the values explicitly by creating a data frame with a \texttt{neighbors} variable.
Take a look as the plot below as we vary \(K\) from 1 to almost the number of observations in the data set:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{k\_lots }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{neighbors =} \FunctionTok{seq}\NormalTok{(}\AttributeTok{from =} \DecValTok{1}\NormalTok{, }\AttributeTok{to =} \DecValTok{385}\NormalTok{, }\AttributeTok{by =} \DecValTok{10}\NormalTok{))}
\NormalTok{knn\_results }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(cancer\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(knn\_spec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tune\_grid}\NormalTok{(}\AttributeTok{resamples =}\NormalTok{ cancer\_vfold, }\AttributeTok{grid =}\NormalTok{ k\_lots) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_metrics}\NormalTok{()}

\NormalTok{accuracies }\OtherTok{\textless{}{-}}\NormalTok{ knn\_results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(.metric }\SpecialCharTok{==} \StringTok{"accuracy"}\NormalTok{)}

\NormalTok{accuracy\_vs\_k\_lots }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(accuracies, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ neighbors, }\AttributeTok{y =}\NormalTok{ mean)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Neighbors"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Accuracy Estimate"}\NormalTok{)}
\NormalTok{accuracy\_vs\_k\_lots}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/06-lots-of-ks-1.pdf}
\caption{\label{fig:06-lots-of-ks}Plot of accuracy estimate versus number of neighbours for many K values}
\end{figure}

\textbf{Underfitting:} What is actually happening to our classifier that causes
this? As we increase the number of neighbours, more and more of the training
observations (and those that are farther and farther away from the point) get a
``say'' in what the class of a new observation is. This causes a sort of
``averaging effect'' to take place, making the boundary between where our
classifier would predict a tumour to be malignant versus benign to smooth out
and become \emph{simpler.} If you take this to the extreme, setting \(K\) to the total
training data set size, then the classifier will always predict the same label
regardless of what the new observation looks like. In general, if the model
\emph{isn't influenced enough} by the training data, it is said to \textbf{underfit} the
data.

\textbf{Overfitting:} In contrast, when we decrease the number of neighbours, each
individual data point has a stronger and stronger vote regarding nearby points.
Since the data themselves are noisy, this causes a more ``jagged'' boundary
corresponding to a \emph{less simple} model. If you take this case to the extreme,
setting \(K = 1\), then the classifier is essentially just matching each new
observation to its closest neighbour in the training data set. This is just as
problematic as the large \(K\) case, because the classifier becomes unreliable on
new data: if we had a different training set, the predictions would be
completely different. In general, if the model \emph{is influenced too much} by the
training data, it is said to \textbf{overfit} the data.

You can see this effect in the plots below as we vary the number of neighbours \(K\) in (1, 7, 20, 200):

\includegraphics{bookdown_files/figure-latex/06-decision-grid-K-1.pdf}

\hypertarget{splitting-data}{%
\section{Splitting data}\label{splitting-data}}

\textbf{Shuffling:} When we split the data into train, test, and validation sets, we
make the assumption that there is no order to our originally collected data
set. However, if we think that there might be some order to the original data
set, then we can randomly shuffle the data before splitting it. The \texttt{tidymodels}
function \texttt{initial\_split} and \texttt{vfold\_cv} functions do this for us.

\textbf{Stratification:} If the data are imbalanced, we also need to be extra
careful about splitting the data to ensure that enough of each class ends up in
each of the train, validation, and test partitions. The \texttt{strata} argument
in the \texttt{initial\_split} and \texttt{vfold\_cv} functions handles this for us too.

\hypertarget{summary}{%
\section{Summary}\label{summary}}

Classification algorithms use one or more quantitative variables to predict the
value of a third, categorical variable. The K-nearest neighbour algorithm in
particular does this by first finding the K points in the training data nearest
to the new observation, and then returning the majority class vote from those
training observations. We can evaluate a classifier by splitting the data
randomly into a training and test data set, using the training set to build the
classifier, and using the test set to estimate its accuracy. To tune the
classifier (e.g., select the K in K-nearest neighbours), we maximize accuracy
estimates from cross-validation.

\begin{figure}
\includegraphics[width=1\linewidth]{img/train-test-overview} \caption{Overview of K-nn classification}\label{fig:06-overview}
\end{figure}

The overall workflow for performing K-nearest neighbour classification using \texttt{tidymodels} is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use the \texttt{initial\_split} function to split the data into a training and test set. Set the \texttt{strata} argument to the target variable. Put the test set aside for now.
\item
  Use the \texttt{vfold\_cv} function to split up the training data for cross validation.
\item
  Create a \texttt{recipe} that specifies the target and predictor variables, as well as preprocessing steps for all variables. Pass the training data as the \texttt{data} argument of the recipe.
\item
  Create a \texttt{nearest\_neighbors} model specification, with \texttt{neighbors\ =\ tune()}.
\item
  Add the recipe and model specification to a \texttt{workflow()}, and use the \texttt{tune\_grid} function on the train/validation splits to estimate the classifier accuracy for a range of \(K\) values.
\item
  Pick a value of \(K\) that yields a high accuracy estimate that doesn't change much if you change \(K\) to a nearby value.
\item
  Make a new model specification for the best parameter value, and retrain the classifier using the \texttt{fit} function.
\item
  Evaluate the estimated accuracy of the classifier on the test set using the \texttt{predict} function.
\end{enumerate}

\textbf{Strengths:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simple and easy to understand
\item
  No assumptions about what the data must look like
\item
  Works easily for binary (two-class) and multi-class (\textgreater{} 2 classes) classification problems
\end{enumerate}

\textbf{Weaknesses:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  As data gets bigger and bigger, K-nearest neighbour gets slower and slower, quite quickly
\item
  Does not perform well with a large number of predictors
\item
  Does not perform well when classes are imbalanced (when many more observations are in one of the classes compared to the others)
\end{enumerate}

\hypertarget{regression1}{%
\chapter{Regression I: K-nearest neighbours}\label{regression1}}

\hypertarget{overview-6}{%
\section{Overview}\label{overview-6}}

This chapter will provide an introduction to regression through K-nearest
neighbours (K-NN) in a predictive context, focusing primarily on the case where
there is a single predictor and single response variable of interest. The
chapter concludes with an example of K-nearest neighbours regression with
multiple predictors.

\hypertarget{chapter-learning-objectives-7}{%
\section{Chapter learning objectives}\label{chapter-learning-objectives-7}}

By the end of the chapter, students will be able to:

\begin{itemize}
\tightlist
\item
  Recognize situations where a simple regression analysis would be appropriate for making predictions.
\item
  Explain the K-nearest neighbour (K-NN) regression algorithm and describe how it differs from K-NN classification.
\item
  Interpret the output of a K-NN regression.
\item
  In a dataset with two or more variables, perform K-nearest neighbour regression in R using a \texttt{tidymodels} workflow
\item
  Execute cross-validation in R to choose the number of neighbours.
\item
  Evaluate K-NN regression prediction accuracy in R using a test data set and an appropriate metric (\emph{e.g.}, root means square prediction error).
\item
  In the context of K-NN regression, compare and contrast goodness of fit and prediction properties (namely RMSE vs RMSPE).
\item
  Describe advantages and disadvantages of the K-nearest neighbour regression approach.
\end{itemize}

\hypertarget{regression}{%
\section{Regression}\label{regression}}

Regression, like classification, is a predictive problem setting where we want
to use past information to predict future observations. But in the case of
regression, the goal is to predict numerical values instead of class labels.
For example, we could try to use the number of hours a person spends on
exercise each week to predict whether they would qualify for the annual Boston
marathon (\emph{classification}) or to predict their race time itself
(\emph{regression}). As another example, we could try to use the size of a house to
predict whether it sold for more than \$500,000 (\emph{classification}) or to
predict its sale price itself (\emph{regression}). We will use K-nearest neighbours
to explore this question in the rest of this chapter, using a real estate data
set from Sacremento, California.

\hypertarget{sacremento-real-estate-example}{%
\section{Sacremento real estate example}\label{sacremento-real-estate-example}}

Let's start by loading the packages we need and doing some preliminary
exploratory analysis. The Sacramento real estate data set we will study in this chapter
was \href{https://support.spatialkey.com/spatialkey-sample-csv-data/}{originally reported in the Sacramento Bee},
but we have provided it with this repository as a stable source for the data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(tidymodels)}
\FunctionTok{library}\NormalTok{(gridExtra)}

\NormalTok{sacramento }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/sacramento.csv"}\NormalTok{)}
\NormalTok{sacramento}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 932 x 9
##    city    zip    beds baths  sqft type  price latitude
##    <chr>   <chr> <dbl> <dbl> <dbl> <chr> <dbl>    <dbl>
##  1 SACRAM~ z958~     2     1   836 Resi~ 59222     38.6
##  2 SACRAM~ z958~     3     1  1167 Resi~ 68212     38.5
##  3 SACRAM~ z958~     2     1   796 Resi~ 68880     38.6
##  4 SACRAM~ z958~     2     1   852 Resi~ 69307     38.6
##  5 SACRAM~ z958~     2     1   797 Resi~ 81900     38.5
##  6 SACRAM~ z958~     3     1  1122 Condo 89921     38.7
##  7 SACRAM~ z958~     3     2  1104 Resi~ 90895     38.7
##  8 SACRAM~ z958~     3     1  1177 Resi~ 91002     38.5
##  9 RANCHO~ z956~     2     2   941 Condo 94905     38.6
## 10 RIO_LI~ z956~     3     2  1146 Resi~ 98937     38.7
## # ... with 922 more rows, and 1 more variable:
## #   longitude <dbl>
\end{verbatim}

The purpose of this exercise is to understand whether we can we use house size
to predict house sale price in the Sacramento, CA area. The columns in this
data that we are interested in are \texttt{sqft} (house size, in livable square feet)
and \texttt{price} (house price, in US dollars (USD). The first step is to visualize
the data as a scatter plot where we place the predictor/explanatory variable
(house size) on the x-axis, and we place the target/response variable that we
want to predict (price) on the y-axis:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eda }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(sacramento, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sqft, }\AttributeTok{y =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"House size (square footage)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Price (USD)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{dollar\_format}\NormalTok{())}
\NormalTok{eda}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/07-edaRegr-1.pdf}
\caption{\label{fig:07-edaRegr}Scatter plot of price (USD) versus house size (square footage)}
\end{figure}

Based on the visualization above, we can see that in Sacramento, CA, as the
size of a house increases, so does its sale price. Thus, we can reason that we
may be able to use the size of a not-yet-sold house (for which we don't know
the sale price) to predict its final sale price.

\hypertarget{k-nearest-neighbours-regression}{%
\section{K-nearest neighbours regression}\label{k-nearest-neighbours-regression}}

Much like in the case of classification, we can use a K-nearest
neighbours-based approach in regression to make predictions. Let's take a small
sample of the data above and walk through how K-nearest neighbours (knn) works
in a regression context before we dive in to creating our model and assessing
how well it predicts house price. This subsample is taken to allow us to
illustrate the mechanics of K-NN regression with a few data points; later in
this chapter we will use all the data.

To take a small random sample of size 30, we'll use the function \texttt{sample\_n}.
This function takes two arguments:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{tbl} (a data frame-like object to sample from)
\item
  \texttt{size} (the number of observations/rows to be randomly selected/sampled)
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{small\_sacramento }\OtherTok{\textless{}{-}} \FunctionTok{sample\_n}\NormalTok{(sacramento, }\AttributeTok{size =} \DecValTok{30}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next let's say we come across a 2,000 square-foot house in Sacramento we are
interested in purchasing, with an advertised list price of \$350,000. Should we
offer to pay the asking price for this house, or is it overpriced and we should
offer less? Absent any other information, we can get a sense for a good answer
to this question by using the data we have to predict the sale price given the
sale prices we have already observed. But in the plot below, we have no
observations of a house of size \emph{exactly} 2000 square feet. How can we predict
the price?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{small\_plot }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(small\_sacramento, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sqft, }\AttributeTok{y =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"House size (square footage)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Price (USD)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{dollar\_format}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{2000}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dotted"}\NormalTok{)}
\NormalTok{small\_plot}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/07-small-eda-regr-1.pdf}
\caption{\label{fig:07-small-eda-regr}Scatter plot of price (USD) versus house size (square footage) with vertical line indicating 2000 square feet on x-axis}
\end{figure}

We will employ the same intuition from the classification chapter, and use the
neighbouring points to the new point of interest to suggest/predict what its
price should be. For the example above, we find and label the 5 nearest
neighbours to our observation of a house that is 2000 square feet:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nearest\_neighbours }\OtherTok{\textless{}{-}}\NormalTok{ small\_sacramento }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{diff =} \FunctionTok{abs}\NormalTok{(}\DecValTok{2000} \SpecialCharTok{{-}}\NormalTok{ sqft)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(diff) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{) }\CommentTok{\#subset the first 5 rows}
\NormalTok{nearest\_neighbours}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 10
##   city  zip    beds baths  sqft type   price latitude
##   <chr> <chr> <dbl> <dbl> <dbl> <chr>  <dbl>    <dbl>
## 1 GOLD~ z956~     3     2  1981 Resi~ 305000     38.6
## 2 ELK_~ z957~     4     2  2056 Resi~ 275000     38.4
## 3 ELK_~ z956~     5     3  2136 Resi~ 223058     38.4
## 4 RANC~ z957~     4     2  1713 Resi~ 263500     38.6
## 5 RIO_~ z956~     2     2  1690 Resi~ 136500     38.7
## # ... with 2 more variables: longitude <dbl>,
## #   diff <dbl>
\end{verbatim}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/07-knn3-example-1.pdf}
\caption{\label{fig:07-knn3-example}Scatter plot of price (USD) versus house size (square footage) with lines to 5 nearest neighbours}
\end{figure}

Now that we have the 5 nearest neighbours (in terms of house size) to our new
2,000 square-foot house of interest, we can use their values to predict a
selling price for the new home. Specifically, we can take the mean (or
average) of these 5 values as our predicted value.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prediction }\OtherTok{\textless{}{-}}\NormalTok{ nearest\_neighbours }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{predicted =} \FunctionTok{mean}\NormalTok{(price))}
\NormalTok{prediction}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   predicted
##       <dbl>
## 1   240612.
\end{verbatim}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/07-predictedViz-knn-1.pdf}
\caption{\label{fig:07-predictedViz-knn}Scatter plot of price (USD) versus house size (square footage) with predicted price for a 2000 square-foot house based on 5 nearest neighbours represented as a red dot}
\end{figure}

Our predicted price is \$240612
(shown as a red point above), which is much less than \$350,000; perhaps we
might want to offer less than the list price at which the house is advertised.
But this is only the very beginning of the story. We still have all the same
unanswered questions here with K-NN regression that we had with K-NN
classification: which \(K\) do we choose, and is our model any good at making
predictions? In the next few sections, we will address these questions in the
context of K-NN regression.

\hypertarget{training-evaluating-and-tuning-the-model}{%
\section{Training, evaluating, and tuning the model}\label{training-evaluating-and-tuning-the-model}}

As usual, we must start by putting some test data away in a lock box that we
will come back to only after we choose our final model. Let's take care of that
now. Note that for the remainder of the chapter we'll be working with the
entire Sacramento data set, as opposed to the smaller sample of 30 points above.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{sacramento\_split }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(sacramento, }\AttributeTok{prop =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{strata =}\NormalTok{ price)}
\NormalTok{sacramento\_train }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(sacramento\_split)}
\NormalTok{sacramento\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(sacramento\_split)}
\end{Highlighting}
\end{Shaded}

Next, we'll use cross-validation to choose \(K\). In K-NN classification, we used
accuracy to see how well our predictions matched the true labels. Here in the
context of K-NN regression we will use root mean square prediction error
(RMSPE) instead. The mathematical formula for calculating RMSPE is:

\[\text{RMSPE} = \sqrt{\frac{1}{n}\sum\limits_{i=1}^{n}(y_i - \hat{y}_i)^2}\]

Where:

\begin{itemize}
\tightlist
\item
  \(n\) is the number of observations
\item
  \(y_i\) is the observed value for the \(i^\text{th}\) observation
\item
  \(\hat{y}_i\) is the forcasted/predicted value for the \(i^\text{th}\) observation
\end{itemize}

A key feature of the formula for RMPSE is the squared difference between the observed
target/response variable value, \(y\), and the prediction target/response
variable value, \(\hat{y}_i\), for each observation (from 1 to \(i\)).
If the predictions are very close to the true values, then
RMSPE will be small. If, on the other-hand, the predictions are very
different to the true values, then RMSPE will be quite large. When we
use cross validation, we will choose the \(K\) that gives
us the smallest RMSPE.

\begin{quote}
\textbf{RMSPE versus RMSE}
When using many code packages (\texttt{tidymodels} included), the evaluation output
we will get to assess the prediction quality of
our K-NN regression models is labelled ``RMSE'', or ``root mean squared
error''. Why is this so, and why not just RMSPE?
In statistics, we try to be very precise with our
language to indicate whether we are calculating the prediction error on the
training data (\emph{in-sample} prediction) versus on the testing data
(\emph{out-of-sample} prediction). When predicting and evaluating prediction quality on the training data, we
say RMSE. By contrast, when predicting and evaluating prediction quality
on the testing or validation data, we say RMSPE.
The equation for calculating RMSE and RMSPE is exactly the same; all that changes is whether the \(y\)s are
training or testing data. But many people just use RMSE for both,
and rely on context to denote which data the root mean squared error is being calculated on.
\end{quote}

Now that we know how we can assess how well our model predicts a numerical
value, let's use R to perform cross-validation and to choose the optimal \(K\).
First, we will create a model specification for K-nearest neighbours regression,
as well as a recipe for preprocessing our data. Note that we use \texttt{set\_mode("regression")}
now in the model specification to denote a regression problem, as opposed to the classification
problems from the previous chapters. Note also that we include standardization
in our preprocessing to build good habits, but since we only have one
predictor it is technically not necessary; there is no risk of comparing two predictors
of different scales.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sacr\_recipe }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sqft, }\AttributeTok{data =}\NormalTok{ sacramento\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_scale}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_center}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{())}

\NormalTok{sacr\_spec }\OtherTok{\textless{}{-}} \FunctionTok{nearest\_neighbor}\NormalTok{(}\AttributeTok{weight\_func =} \StringTok{"rectangular"}\NormalTok{, }\AttributeTok{neighbors =} \FunctionTok{tune}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"kknn"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{)}

\NormalTok{sacr\_vfold }\OtherTok{\textless{}{-}} \FunctionTok{vfold\_cv}\NormalTok{(sacramento\_train, }\AttributeTok{v =} \DecValTok{5}\NormalTok{, }\AttributeTok{strata =}\NormalTok{ price)}

\NormalTok{sacr\_wkflw }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(sacr\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(sacr\_spec)}
\NormalTok{sacr\_wkflw}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## == Workflow ===========================================
## Preprocessor: Recipe
## Model: nearest_neighbor()
## 
## -- Preprocessor ---------------------------------------
## 2 Recipe Steps
## 
## * step_scale()
## * step_center()
## 
## -- Model ----------------------------------------------
## K-Nearest Neighbor Model Specification (regression)
## 
## Main Arguments:
##   neighbors = tune()
##   weight_func = rectangular
## 
## Computational engine: kknn
\end{verbatim}

The major difference you can see in the above workflow compared to previous
chapters is that we are running regression rather than classification. The fact
that we use \texttt{set\_mode("regression")} essentially
tells \texttt{tidymodels} that we need to use different metrics (RMSPE, not accuracy)
for tuning and evaluation. You can see this in the following code, which tunes
the model and returns the RMSPE for each number of neighbours.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gridvals }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{neighbors =} \FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{200}\NormalTok{))}

\NormalTok{sacr\_results }\OtherTok{\textless{}{-}}\NormalTok{ sacr\_wkflw }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tune\_grid}\NormalTok{(}\AttributeTok{resamples =}\NormalTok{ sacr\_vfold, }\AttributeTok{grid =}\NormalTok{ gridvals) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_metrics}\NormalTok{()}

\CommentTok{\# show all the results}
\NormalTok{sacr\_results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 400 x 7
##    neighbors .metric .estimator    mean     n std_err
##        <int> <chr>   <chr>        <dbl> <int>   <dbl>
##  1         1 rmse    standard   1.17e+5     5 7.19e+3
##  2         1 rsq     standard   3.72e-1     5 4.63e-2
##  3         2 rmse    standard   1.01e+5     5 5.87e+3
##  4         2 rsq     standard   4.53e-1     5 3.86e-2
##  5         3 rmse    standard   9.67e+4     5 4.21e+3
##  6         3 rsq     standard   4.83e-1     5 2.77e-2
##  7         4 rmse    standard   9.39e+4     5 3.50e+3
##  8         4 rsq     standard   5.03e-1     5 2.98e-2
##  9         5 rmse    standard   8.96e+4     5 3.77e+3
## 10         5 rsq     standard   5.43e-1     5 3.03e-2
## # ... with 390 more rows, and 1 more variable:
## #   .config <chr>
\end{verbatim}

We take the \emph{minimum} RMSPE to find the best setting for the number of neighbours:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# show only the row of minimum RMSPE}
\NormalTok{sacr\_min }\OtherTok{\textless{}{-}}\NormalTok{ sacr\_results }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(.metric }\SpecialCharTok{==} \StringTok{"rmse"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(mean }\SpecialCharTok{==} \FunctionTok{min}\NormalTok{(mean))}
\NormalTok{sacr\_min}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 7
##   neighbors .metric .estimator   mean     n std_err
##       <int> <chr>   <chr>       <dbl> <int>   <dbl>
## 1        14 rmse    standard   84356.     5   4050.
## # ... with 1 more variable: .config <chr>
\end{verbatim}

Here we can see that the smallest RMSPE occurs when \(K =\) 14.

\hypertarget{underfitting-and-overfitting}{%
\section{Underfitting and overfitting}\label{underfitting-and-overfitting}}

Similar to the setting of classification, by setting the number of neighbours
to be too small or too large, we cause the RMSPE to increase:

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/07-choose-k-knn-plot-1.pdf}
\caption{\label{fig:07-choose-k-knn-plot}Effect of the number of neighbours on the RMSPE}
\end{figure}

What is happening here? To visualize the effect of different settings of \(K\) on the
regression model, we will plot the predicted values for house price from
our K-NN regression models for 6 different values for \(K\). For each model, we
predict a price for every possible home size across the range of home sizes we
observed in the data set (here 500 to 4250 square feet) and we plot the
predicted prices as a blue line:

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/07-howK-1.pdf}
\caption{\label{fig:07-howK}Predicted values for house price (represented as a blue line) from K-NN regression models for six different values for \(K\)}
\end{figure}

Based on the plots above, we see that when \(K\) = 1, the blue line runs perfectly
through almost all of our training observations. This happens because our
predicted values for a given region depend on just a single observation. A
model like this has high variance and low bias (intuitively, it provides unreliable
predictions). It has high variance because
the flexible blue line follows the training observations very closely, and if
we were to change any one of the training observation data points we would
change the flexible blue line quite a lot. This means that the blue line
matches the data we happen to have in this training data set, however, if we
were to collect another training data set from the Sacramento real estate
market it likely wouldn't match those observations as well. Another term
that we use to collectively describe this phenomenon is \emph{overfitting}.

What about the plot where \(K\) is quite large, say \(K\) = 450, for example? When
\(K\) = 450 for this data set, the blue line is extremely smooth, and almost
flat. This happens because our predicted values for a given x value (here home
size), depend on many many (450) neighbouring observations. A model
like this has low variance and high bias (intuitively, it provides very reliable,
but generally very inaccurate predictions). It has low variance because the
smooth, inflexible blue line does not follow the training observations very
closely, and if we were to change any one of the training observation data
points it really wouldn't affect the shape of the smooth blue line at all. This
means that although the blue line matches does not match the data we happen to
have in this particular training data set perfectly, if we were to collect
another training data set from the Sacramento real estate market it likely
would match those observations equally as well as it matches those in this
training data set. Another term that
we use to collectively describe this kind of model is \emph{underfitting}.

Ideally, what we want is neither of the two situations discussed above. Instead,
we would like a model with low variance (so that it will transfer/generalize
well to other data sets, and isn't too dependent on the
observations that happen to be in the training set) \textbf{and} low bias
(where the model does not completely ignore our training data). If we explore
the other values for \(K\), in particular \(K\) = 14
(as suggested by cross-validation),
we can see it has a lower bias than our model with a very high \(K\) (e.g., 450),
and thus the model/predicted values better match the actual observed values
than the high \(K\) model. Additionally, it has lower variance than our model
with a very low \(K\) (e.g., 1) and thus it should better transer/generalize to
other data sets compared to the low \(K\) model. All of this is similar to how
the choice of \(K\) affects K-NN classification (discussed in the previous
chapter).

\hypertarget{evaluating-on-the-test-set}{%
\section{Evaluating on the test set}\label{evaluating-on-the-test-set}}

To assess how well our model might do at predicting on unseen data, we will
assess its RMSPE on the test data. To do this, we will first
re-train our K-NN regression model on the entire training data set,
using \(K =\) 14 neighbours. Then we will
use \texttt{predict} to make predictions on the test data, and use the \texttt{metrics}
function again to compute the summary of regression quality. Because
we specify that we are performing regression in \texttt{set\_mode}, the \texttt{metrics}
function knows to output a quality summary related to regression, and not, say, classification.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{kmin }\OtherTok{\textless{}{-}}\NormalTok{ sacr\_min }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pull}\NormalTok{(neighbors)}
\NormalTok{sacr\_spec }\OtherTok{\textless{}{-}} \FunctionTok{nearest\_neighbor}\NormalTok{(}\AttributeTok{weight\_func =} \StringTok{"rectangular"}\NormalTok{, }\AttributeTok{neighbors =}\NormalTok{ kmin) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"kknn"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{)}

\NormalTok{sacr\_fit }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(sacr\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(sacr\_spec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}\AttributeTok{data =}\NormalTok{ sacramento\_train)}

\NormalTok{sacr\_summary }\OtherTok{\textless{}{-}}\NormalTok{ sacr\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{predict}\NormalTok{(sacramento\_test) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_cols}\NormalTok{(sacramento\_test) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{metrics}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ price, }\AttributeTok{estimate =}\NormalTok{ .pred)}

\NormalTok{sacr\_summary}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   <chr>   <chr>          <dbl>
## 1 rmse    standard   87737.   
## 2 rsq     standard       0.546
## 3 mae     standard   65400.
\end{verbatim}

Our final model's test error as assessed by RMSPE
is 87737.
But what does this RMSPE score mean? When we calculated test set prediction accuracy in K-NN
classification, the highest possible value was 1 and the lowest possible value was 0.
If we got a value close to 1, our model was ``good;'' and otherwise,
the model was ``not good.'' What about RMSPE? Unfortunately there is no default scale
for RMSPE. Instead, it is measured in
the units of the target/response variable, and so it is a bit hard to
interpret. For now, let's consider this approach to thinking about RMSPE from
our testing data set: as long as its not significantly worse than the cross-validation
RMSPE of our best model, then we can say that we're not doing too much worse on
the test data than we did on the training data. So the model appears to be
generalizing well to a new data set it has never seen before. In future courses
on statistical/machine learning, you will learn more about how to interpret RMSPE
from testing data and other ways to assess models.

Finally, what does our model look like when we predict across all possible
house sizes we might encounter in the Sacramento area? We plotted it above
where we explored how \(k\) affects K-NN regression, but we show it again now,
along with the code that generated it:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{sacr\_preds }\OtherTok{\textless{}{-}}\NormalTok{ sacr\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{predict}\NormalTok{(sacramento\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_cols}\NormalTok{(sacramento\_train)}

\NormalTok{plot\_final }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(sacr\_preds, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sqft, }\AttributeTok{y =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"House size (square footage)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Price (USD)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{dollar\_format}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\AttributeTok{data =}\NormalTok{ sacr\_preds, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sqft, }\AttributeTok{y =}\NormalTok{ .pred), }\AttributeTok{color =} \StringTok{"blue"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\StringTok{"K = "}\NormalTok{, kmin))}

\NormalTok{plot\_final}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/07-predict-all-1.pdf}
\caption{\label{fig:07-predict-all}Predicted values for house price (represented as a blue line) for K-NN regression model with \(K = 14\)}
\end{figure}

\hypertarget{strengths-and-limitations-of-k-nn-regression}{%
\section{Strengths and limitations of K-NN regression}\label{strengths-and-limitations-of-k-nn-regression}}

As with K-NN classification (or any prediction algorithm for that manner), K-NN regression has both strengths and weaknesses. Some are listed here:

\textbf{Strengths of K-NN regression}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Simple and easy to understand
\item
  No assumptions about what the data must look like
\item
  Works well with non-linear relationships (i.e., if the relationship is not a straight line)
\end{enumerate}

\textbf{Limitations of K-NN regression}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  As data gets bigger and bigger, K-NN gets slower and slower, quite quickly 2. Does not perform well with a large number of predictors unless the size of the training set is exponentially larger
\item
  Does not predict well beyond the range of values input in your training data
\end{enumerate}

\hypertarget{multivariate-k-nn-regression}{%
\section{Multivariate K-NN regression}\label{multivariate-k-nn-regression}}

As in K-NN classification, in K-NN regression we can have multiple predictors.
When we have multiple predictors in K-NN regression, we have the same concern
regarding the scale of the predictors. This is because once again,
predictions are made by identifying the \(K\)
observations that are nearest to the new point we want to predict, and any
variables that are on a large scale will have a much larger effect than
variables on a small scale. Since the \texttt{recipe} we built above scales and centers
all predictor variables, this is handled for us.

We will now demonstrate a multivariate K-NN regression analysis of the
Sacramento real estate data using \texttt{tidymodels}. This time we will use
house size (measured in square feet) as well as number of bedrooms as our
predictors, and continue to use house sale price as our outcome/target variable
that we are trying to predict.

It is always a good practice to do exploratory data analysis, such as
visualizing the data, before we start modeling the data. Thus the first thing
we will do is use ggpairs (from the \texttt{GGally} package) to plot all the variables
we are interested in using in our analyses:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(GGally)}
\NormalTok{plot\_pairs }\OtherTok{\textless{}{-}}\NormalTok{ sacramento }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(price, sqft, beds) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggpairs}\NormalTok{()}
\NormalTok{plot\_pairs}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/07-ggpairs-1.pdf}
\caption{\label{fig:07-ggpairs}Scatterplots of each pair of variables (price, house size, and the number of bedrooms) are displayed in the figure's bottom left corner. Correlation coefficients are shown in the top right corner. The distributions of price, house size and number of bedrooms are each shown along the diagonal.}
\end{figure}

From this we can see that generally, as both house size and number of bedrooms increase, so does price. Does adding the number of bedrooms to our model improve our ability to predict house price? To answer that question, we will have to come up with the test error for a K-NN regression model using house size and number of bedrooms, and then we can compare it to the test error for the model we previously came up with that only used house size to see if it is smaller (decreased test error indicates increased prediction quality). Let's do that now!

First we'll build a new model specification and recipe for the analysis. Note that
we use the formula \texttt{price\ \textasciitilde{}\ sqft\ +\ beds} to denote that we have two predictors,
and set \texttt{neighbors\ =\ tune()} to tell \texttt{tidymodels} to tune the number of neighbours for us.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sacr\_recipe }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sqft }\SpecialCharTok{+}\NormalTok{ beds, }\AttributeTok{data =}\NormalTok{ sacramento\_train) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_scale}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{step\_center}\NormalTok{(}\FunctionTok{all\_predictors}\NormalTok{())}

\NormalTok{sacr\_spec }\OtherTok{\textless{}{-}} \FunctionTok{nearest\_neighbor}\NormalTok{(}\AttributeTok{weight\_func =} \StringTok{"rectangular"}\NormalTok{, }\AttributeTok{neighbors =} \FunctionTok{tune}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"kknn"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, we'll use 5-fold cross-validation to choose the number of neighbours via the minimum RMSPE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gridvals }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{neighbors =} \FunctionTok{seq}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{200}\NormalTok{))}
\NormalTok{sacr\_k }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(sacr\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(sacr\_spec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tune\_grid}\NormalTok{(sacr\_vfold, }\AttributeTok{grid =}\NormalTok{ gridvals) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{collect\_metrics}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(.metric }\SpecialCharTok{==} \StringTok{"rmse"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(mean }\SpecialCharTok{==} \FunctionTok{min}\NormalTok{(mean)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull}\NormalTok{(neighbors)}
\NormalTok{sacr\_k}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 14
\end{verbatim}

Here we see that the smallest RMSPE occurs when \(K =\) 14.

Now that we have chosen \(K\), we need to re-train the model on the entire
training data set, and after that we can use that model to predict
on the test data to get our test error.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sacr\_spec }\OtherTok{\textless{}{-}} \FunctionTok{nearest\_neighbor}\NormalTok{(}\AttributeTok{weight\_func =} \StringTok{"rectangular"}\NormalTok{, }\AttributeTok{neighbors =}\NormalTok{ sacr\_k) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"kknn"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{)}

\NormalTok{knn\_mult\_fit }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(sacr\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(sacr\_spec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}\AttributeTok{data =}\NormalTok{ sacramento\_train)}

\NormalTok{knn\_mult\_preds }\OtherTok{\textless{}{-}}\NormalTok{ knn\_mult\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{predict}\NormalTok{(sacramento\_test) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_cols}\NormalTok{(sacramento\_test)}

\NormalTok{knn\_mult\_mets }\OtherTok{\textless{}{-}} \FunctionTok{metrics}\NormalTok{(knn\_mult\_preds, }\AttributeTok{truth =}\NormalTok{ price, }\AttributeTok{estimate =}\NormalTok{ .pred)}
\NormalTok{knn\_mult\_mets}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   <chr>   <chr>          <dbl>
## 1 rmse    standard   85152.   
## 2 rsq     standard       0.572
## 3 mae     standard   63575.
\end{verbatim}

This time when we performed K-NN regression on the same data set, but also
included number of bedrooms as a predictor we obtained a RMSPE test error
of 85152.
This compares to a RMSPE test error
of 87737
when we used only house size as the
single predictor. Thus in this case, we did not improve the model
by a large amount by adding this additional predictor.

We can also visualize the model's predictions overlaid on top of the data. This time the predictions
will be a surface in 3-D space, instead of a line in 2-D space, as we have 2
predictors instead of 1.

\begin{figure}
\includegraphics[width=0.7\linewidth]{img/lm-regr_3d} \caption{K-NN regression model’s predictions represented as a surface in 3-D space overlaid on top of the data using three predictors.}\label{fig:07-knn-mult-viz}
\end{figure}

We can see that the predictions in this case, where we have 2 predictors, form
a surface instead of a line. Because the newly added predictor, number of
bedrooms, is correlated with price (USD) (meaning as price changes, so does
number of bedrooms) and not totally determined by house size (our other predictor),
we get additional and useful information for making our
predictions. For example, in this model we would predict that the cost of a
house with a size of 2,500 square feet generally increases slightly as the number
of bedrooms increases. Without having the additional predictor of number of
bedrooms, we would predict the same price for these two houses.

\hypertarget{regression2}{%
\chapter{Regression II: linear regression}\label{regression2}}

\hypertarget{overview-7}{%
\section{Overview}\label{overview-7}}

This chapter provides an introduction to linear regression models
in a predictive context, focusing primarily on the case where
there is a single predictor and single response variable of interest,
as well as comparison to K-nearest neighbours methods. The
chapter concludes with a discussion of linear regression with
multiple predictors.

\hypertarget{chapter-learning-objectives-8}{%
\section{Chapter learning objectives}\label{chapter-learning-objectives-8}}

By the end of the chapter, students will be able to:

\begin{itemize}
\tightlist
\item
  Perform linear regression in R using \texttt{tidymodels} and evaluate it on a test dataset.
\item
  Compare and contrast predictions obtained from K-nearest neighbour regression to those obtained using simple ordinary least squares regression from the same dataset.
\item
  In R, overlay regression lines from \texttt{geom\_smooth} on a single plot.
\end{itemize}

\hypertarget{simple-linear-regression}{%
\section{Simple linear regression}\label{simple-linear-regression}}

K-NN is not the only type of regression; another
quite useful, and arguably the most common, type of regression is
called simple linear regression. Simple
linear regression is similar to K-NN regression in that the target/response
variable is quantitative. However, one way it varies quite
differently is how the training data is used to predict a value for a new
observation. Instead of looking at the \(K\)-nearest neighbours and averaging
over their values for a prediction, in simple linear regression all the
training data points are used to create a straight line of best fit, and then
the line is used to ``look up'' the predicted value.

\begin{quote}
Note: for simple linear
regression there is only one response variable and only one predictor. Later in
this chapter we introduce the more general linear regression case where more
than one predictor can be used.
\end{quote}

For example, let's revisit the smaller version of the Sacramento housing data
set. Recall that we have come across a new 2,000-square foot house we are interested
in purchasing with an advertised list price of
\$350,000. Should we offer the list price, or is that over/undervalued?

To answer this question using simple linear regression, we use the data we have
to draw the straight line of best fit through our existing data points:

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/08-lin-reg1-1.pdf}
\caption{\label{fig:08-lin-reg1}Scatter plot of price (USD) versus house size (square footage) with line of best fit for subset of the Sacramento housing data set}
\end{figure}

The equation for the straight line is:

\[\text{house price} = \beta_0 + \beta_1 \cdot (\text{house size}),\]
where

\begin{itemize}
\tightlist
\item
  \(\beta_0\) is the vertical intercept of the line (the value where the line cuts the vertical axis)
\item
  \(\beta_1\) is the slope of the line
\end{itemize}

Therefore using the data to find the line of best fit is equivalent to finding coefficients
\(\beta_0\) and \(\beta_1\) that \emph{parametrize} (correspond to) the line of best fit.
Once we have the coefficients, we can use the equation above to evaluate the predicted price given the value we
have for the predictor/explanatory variable---here 2,000 square feet.

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/08-lin-reg2-1.pdf}
\caption{\label{fig:08-lin-reg2}Scatter plot of price (USD) versus house size (square footage) with line of best fit and predicted price for a 2000 square foot home represented as a red dot}
\end{figure}

\begin{verbatim}
## [1] 287962
\end{verbatim}

By using simple linear regression on this small data set to predict the sale price
for a 2,000 square foot house, we get a predicted value of
\$287962. But wait a minute\ldots how
exactly does simple linear regression choose the line of best fit? Many
different lines could be drawn through the data points. We show some examples
below:

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/08-several-lines-1.pdf}
\caption{\label{fig:08-several-lines}Scatter plot of price (USD) versus house size (square footage) with many possible lines that could be drawn through the data points}
\end{figure}

Simple linear regression chooses the straight line of best fit by choosing
the line that minimzes the \textbf{average} vertical distance between itself and
each of the observed data points. From the lines shown above, that is the blue
line. What exactly do we mean by the vertical distance between the predicted
values (which fall along the line of best fit) and the observed data points?
We illustrate these distances in the plot below with a red line:

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/08-verticalDistToMin-1.pdf}
\caption{\label{fig:08-verticalDistToMin}Scatter plot of price (USD) versus house size (square footage) with the vertical distances between the predicted values and the observed data points}
\end{figure}

To assess the predictive accuracy of a simple linear regression model,
we use RMSPE---the same measure of predictive performance we used with K-NN regression.

\hypertarget{linear-regression-in-r}{%
\section{Linear regression in R}\label{linear-regression-in-r}}

We can perform simple linear regression in R using \texttt{tidymodels} in a
very similar manner to how we performed K-NN regression.
To do this, instead of creating a \texttt{nearest\_neighbor} model specification with
the \texttt{kknn} engine, we use a \texttt{linear\_reg} model specification
with the \texttt{lm} engine. Another difference is that we do not need to choose \(K\) in the
context of linear regression, and so we do not need to perform cross validation.
Below we illustrate how we can use the usual \texttt{tidymodels} workflow to predict house sale
price given house size using a simple linear regression approach using the full
Sacramento real estate data set.

\begin{quote}
An additional difference that you will notice below is that we do not standardize
(i.e., scale and center) our predictors. In K-nearest neighbours models, recall that
the model fit changes depending on whether we standardize first or not. In linear regression,
standardization does not affect the fit (it \emph{does} affect the coefficients in the equation, though!).
So you can standardize if you want---it won't hurt anything---but if you leave the
predictors in their original form, the best fit coefficients are usually easier to interpret afterward.
\end{quote}

As usual, we start by putting some test data away in a lock box that we
can come back to after we choose our final model. Let's take care of that now.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{sacramento\_split }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(sacramento, }\AttributeTok{prop =} \FloatTok{0.6}\NormalTok{, }\AttributeTok{strata =}\NormalTok{ price)}
\NormalTok{sacramento\_train }\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(sacramento\_split)}
\NormalTok{sacramento\_test }\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(sacramento\_split)}
\end{Highlighting}
\end{Shaded}

Now that we have our training data, we will create the model specification
and recipe, and fit our simple linear regression model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_spec }\OtherTok{\textless{}{-}} \FunctionTok{linear\_reg}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_engine}\NormalTok{(}\StringTok{"lm"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{set\_mode}\NormalTok{(}\StringTok{"regression"}\NormalTok{)}

\NormalTok{lm\_recipe }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sqft, }\AttributeTok{data =}\NormalTok{ sacramento\_train)}

\NormalTok{lm\_fit }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(lm\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(lm\_spec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}\AttributeTok{data =}\NormalTok{ sacramento\_train)}
\NormalTok{lm\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## == Workflow [trained] =================================
## Preprocessor: Recipe
## Model: linear_reg()
## 
## -- Preprocessor ---------------------------------------
## 0 Recipe Steps
## 
## -- Model ----------------------------------------------
## 
## Call:
## stats::lm(formula = ..y ~ ., data = data)
## 
## Coefficients:
## (Intercept)         sqft  
##       15059          138
\end{verbatim}

Our coefficients are
(intercept) \(\beta_0=\) 15059
and (slope) \(\beta_1=\) 138.
This means that the equation of the line of best fit is
\[\text{house price} = 15059 + 138\cdot (\text{house size}),\]
and that the model predicts that houses
start at \$15059 for 0 square feet, and that
every extra square foot increases the cost of the house by \$138. Finally, we predict on the test data set to assess how well our model does:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_test\_results }\OtherTok{\textless{}{-}}\NormalTok{ lm\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{predict}\NormalTok{(sacramento\_test) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_cols}\NormalTok{(sacramento\_test) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{metrics}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ price, }\AttributeTok{estimate =}\NormalTok{ .pred)}
\NormalTok{lm\_test\_results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   <chr>   <chr>          <dbl>
## 1 rmse    standard   85161.   
## 2 rsq     standard       0.572
## 3 mae     standard   62608.
\end{verbatim}

Our final model's test error as assessed by RMSPE
is 85161.
Remember that this is in units of the target/response variable, and here that
is US Dollars (USD). Does this mean our model is ``good'' at predicting house
sale price based off of the predictor of home size? Again answering this is
tricky to answer and requires to use domain knowledge and think about the
application you are using the prediction for.

To visualize the simple linear regression model, we can plot the predicted house
price across all possible house sizes we might encounter superimposed on a scatter
plot of the original housing price data. There is a plotting function in
the \texttt{tidyverse}, \texttt{geom\_smooth}, that
allows us to do this easily by adding a layer on our plot with the simple
linear regression predicted line of best fit. The default for this adds a
plausible range to this line that we are not interested in at this point, so to
avoid plotting it, we provide the argument \texttt{se\ =\ FALSE} in our call to
\texttt{geom\_smooth}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_plot\_final }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(sacramento\_train, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sqft, }\AttributeTok{y =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.4}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"House size (square footage)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Price (USD)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{scale\_y\_continuous}\NormalTok{(}\AttributeTok{labels =} \FunctionTok{dollar\_format}\NormalTok{()) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method =} \StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{lm\_plot\_final}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/08-lm-predict-all-1.pdf}
\caption{\label{fig:08-lm-predict-all}Scatter plot of price (USD) versus house size (square footage) with line of best fit for complete Sacramento housing data set}
\end{figure}

We can extract the coefficients from our model by accessing the
fit object that is output by the \texttt{fit} function; we first have to extract
it from the workflow using the \texttt{pull\_workflow\_fit} function, and then apply
the \texttt{tidy} function to convert the result into a data frame:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coeffs }\OtherTok{\textless{}{-}} \FunctionTok{tidy}\NormalTok{(}\FunctionTok{pull\_workflow\_fit}\NormalTok{(lm\_fit))}
\NormalTok{coeffs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 5
##   term        estimate std.error statistic   p.value
##   <chr>          <dbl>     <dbl>     <dbl>     <dbl>
## 1 (Intercept)   15059.   8745.        1.72 8.56e-  2
## 2 sqft            138.      4.77     28.9  3.13e-113
\end{verbatim}

\hypertarget{comparing-simple-linear-and-k-nn-regression}{%
\section{Comparing simple linear and K-NN regression}\label{comparing-simple-linear-and-k-nn-regression}}

Now that we have a general understanding of both simple linear and K-NN
regression, we can start to compare and contrast these methods as well as the
predictions made by them. To start, let's look at the visualization of the
simple linear regression model predictions for the Sacramento real estate data
(predicting price from house size) and the ``best'' K-NN regression model
obtained from the same problem:

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/08-compareRegression-1.pdf}
\caption{\label{fig:08-compareRegression}Comparison of simple linear regression and K-NN regression}
\end{figure}

What differences do we observe from the visualization above? One obvious
difference is the shape of the blue lines. In simple linear regression we are
restricted to a straight line, whereas in K-NN regression our line is much more
flexible and can be quite wiggly. But there is a major interpretability advantage in limiting the
model to a straight line. A
straight line can be defined by two numbers, the
vertical intercept and the slope. The intercept tells us what the prediction is when
all of the predictors are equal to 0; and the slope tells us what unit increase in the target/response
variable we predict given a unit increase in the predictor/explanatory
variable. K-NN regression, as simple as it is to implement and understand, has no such
interpretability from its wiggly line.

There can however also be a disadvantage to using a simple linear regression
model in some cases, particularly when the relationship between the target and
the predictor is not linear, but instead some other shape (e.g.~curved or oscillating). In
these cases the prediction model from a simple linear regression
will underfit (have high bias), meaning that model/predicted values does not
match the actual observed values very well. Such a model would probably have a
quite high RMSE when assessing model goodness of fit on the training data and
a quite high RMPSE when assessing model prediction quality on a test data
set. On such a data set, K-NN regression may fare better. Additionally, there
are other types of regression you can learn about in future courses that may do
even better at predicting with such data.

How do these two models compare on the Sacramento house prices data set? On
the visualizations above we also printed the RMPSE as calculated from
predicting on the test data set that was not used to train/fit the models. The RMPSE for the simple linear
regression model is slightly lower than the RMPSE for the K-NN regression model.
Considering that the simple linear regression model is also more interpretable,
if we were comparing these in practice we would likely choose to use the simple
linear regression model.

Finally, note that the K-NN regression model becomes ``flat''
at the left and right boundaries of the data, while the linear model
predicts a constant slope. Predicting outside the range of the observed
data is known as \emph{extrapolation}; K-NN and linear models behave quite differently
when extrapolating. Depending on the application, the flat
or constant slope trend may make more sense. For example, if our housing
data were slightly different, the linear model may have actually predicted
a \emph{negative} price for a small houses (if the intercept \(\beta_0\) was negative),
which obviously does not match reality. On the other hand, the trend of increasing
house size corresponding to increasing house price probably continues for large houses,
so the ``flat'' extrapolation of K-NN likely does not match reality.

\hypertarget{multivariate-linear-regression}{%
\section{Multivariate linear regression}\label{multivariate-linear-regression}}

As in K-NN classification and K-NN regression, we can move beyond the simple
case of one response variable and only one predictor and perform multivariate
linear regression where we can have multiple predictors. In this case we fit a
plane to the data, as opposed to a straight line.

To do this, we follow a very similar approach to what we did for
K-NN regression; but recall that we do not need to use cross-validation to choose any parameters,
nor do we need to standardize (i.e., center and scale) the data for linear regression.
We demonstrate how to do this below using the Sacramento real estate data with both house size
(measured in square feet) as well as number of bedrooms as our predictors, and
continue to use house sale price as our outcome/target variable that we are
trying to predict. We will start by changing the formula in the recipe to
include both the \texttt{sqft} and \texttt{beds} variables as predictors:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_recipe }\OtherTok{\textless{}{-}} \FunctionTok{recipe}\NormalTok{(price }\SpecialCharTok{\textasciitilde{}}\NormalTok{ sqft }\SpecialCharTok{+}\NormalTok{ beds, }\AttributeTok{data =}\NormalTok{ sacramento\_train)}
\end{Highlighting}
\end{Shaded}

Now we can build our workflow and fit the model:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_fit }\OtherTok{\textless{}{-}} \FunctionTok{workflow}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_recipe}\NormalTok{(lm\_recipe) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_model}\NormalTok{(lm\_spec) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{fit}\NormalTok{(}\AttributeTok{data =}\NormalTok{ sacramento\_train)}
\NormalTok{lm\_fit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## == Workflow [trained] =================================
## Preprocessor: Recipe
## Model: linear_reg()
## 
## -- Preprocessor ---------------------------------------
## 0 Recipe Steps
## 
## -- Model ----------------------------------------------
## 
## Call:
## stats::lm(formula = ..y ~ ., data = data)
## 
## Coefficients:
## (Intercept)         sqft         beds  
##       52690          155       -20209
\end{verbatim}

And finally, we predict on the test data set to assess how well our model does:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_mult\_test\_results }\OtherTok{\textless{}{-}}\NormalTok{ lm\_fit }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{predict}\NormalTok{(sacramento\_test) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{bind\_cols}\NormalTok{(sacramento\_test) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{metrics}\NormalTok{(}\AttributeTok{truth =}\NormalTok{ price, }\AttributeTok{estimate =}\NormalTok{ .pred)}
\NormalTok{lm\_mult\_test\_results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   <chr>   <chr>          <dbl>
## 1 rmse    standard   82835.   
## 2 rsq     standard       0.596
## 3 mae     standard   61008.
\end{verbatim}

In the case of two predictors, our linear regression creates a \emph{plane} of best fit, shown below:

\begin{figure}
\includegraphics[width=0.7\linewidth]{img/knn-regr_3d} \caption{Simple linear regression model’s predictions represented as a plane overlaid on top of the data using three predictors (price, house size, and the number of bedrooms)}\label{fig:08-3DlinReg}
\end{figure}

We see that the predictions from linear regression with two predictors form a
flat plane. This is the hallmark of linear regression, and differs from the
wiggly, flexible surface we get from other methods such as K-NN regression.
As discussed this can be advantageous in one aspect, which is that for each
predictor, we can get slopes/intercept from linear regression, and thus describe the
plane mathematically. We can extract those slope values from our model object
as shown below:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{coeffs }\OtherTok{\textless{}{-}} \FunctionTok{tidy}\NormalTok{(}\FunctionTok{pull\_workflow\_fit}\NormalTok{(lm\_fit))}
\NormalTok{coeffs}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 5
##   term        estimate std.error statistic  p.value
##   <chr>          <dbl>     <dbl>     <dbl>    <dbl>
## 1 (Intercept)   52690.  13745.        3.83 1.41e- 4
## 2 sqft            155.      6.72     23.0  4.46e-83
## 3 beds         -20209.   5734.       -3.52 4.59e- 4
\end{verbatim}

And then use those slopes to write a mathematical equation to describe the prediction plane:

\[\text{house price} = \beta_0 + \beta_1\cdot(\text{house size}) + \beta_2\cdot(\text{number of bedrooms}),\]
where:

\begin{itemize}
\tightlist
\item
  \(\beta_0\) is the vertical intercept of the hyperplane (the value where it cuts the vertical axis)
\item
  \(\beta_1\) is the slope for the first predictor (house size)
\item
  \(\beta_2\) is the slope for the second predictor (number of bedrooms)
\end{itemize}

Finally, we can fill in the values for \(\beta_0\), \(\beta_1\) and \(\beta_2\) from the model output above
to create the equation of the plane of best fit to the data:

\[\text{house price} = 52690 + 155\cdot (\text{house size})  -20209 \cdot (\text{number of bedrooms})\]

This model is more interpretable than the multivariate K-NN
regression model; we can write a mathematical equation that explains how
each predictor is affecting the predictions. But as always, we should look at
the test error and ask whether linear regression is doing a better job of
predicting compared to K-NN regression in this multivariate regression case. To
do that we can use this linear regression model to predict on the test data to
get our test error.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_mult\_test\_results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   <chr>   <chr>          <dbl>
## 1 rmse    standard   82835.   
## 2 rsq     standard       0.596
## 3 mae     standard   61008.
\end{verbatim}

We get that the RMSPE for the multivariate linear regression model
of 82835. This prediction error
is less than the prediction error for the multivariate K-NN regression model,
indicating that we should likely choose linear regression for predictions of
house price on this data set. But we should also ask if this more complex
model is doing a better job of predicting compared to our simple linear
regression model with only a single predictor (house size). Revisiting last
section, we see that our RMSPE for our simple linear regression model with
only a single predictor was
85161,
which is slightly more than that of our more complex model. Our model with two predictors
provided a slightly better fit on test data than our model with just one.

But should we always end up choosing a model with more predictors than fewer?
The answer is no; you never know what model will be the best until you go through the
process of comparing their performance on held-out test data. Exploratory
data analysis can give you some hints, but until you look
at the prediction errors to compare the models you don't really know.
Additionally, here we compare test errors purely for the purposes of teaching.
In practice, when you want to compare several regression models with
differing numbers of predictor variables, you should use
cross-validation on the training set only; in this case choosing the model is part
of tuning, so you cannot use the test data. There are several well known and more advanced
methods to do this that are beyond the scope of this course, and they include
backward or forward selection, and L1 or L2 regularization (also known as Lasso
and ridge regression, respectively).

\hypertarget{the-other-side-of-regression}{%
\section{The other side of regression}\label{the-other-side-of-regression}}

So far in this textbook we have used regression only in the context of
prediction. However, regression is also a powerful method to understand and/or
describe the relationship between a quantitative response variable and
one or more explanatory variables. Extending the case we have been working with
in this chapter (where we are interested in house price as the outcome/response
variable), we might also be interested in describing the
individual effects of house size and the number of bedrooms on house price,
quantifying how big each of these effects are, and assessing how accurately we
can estimate each of these effects. This side of regression is the topic of
many follow-on statistics courses and beyond the scope of this course.

\hypertarget{additional-resources-3}{%
\section{Additional resources}\label{additional-resources-3}}

\begin{itemize}
\tightlist
\item
  Pages 59-71 of \href{http://www-bcf.usc.edu/~gareth/ISL/ISLR\%20Seventh\%20Printing.pdf}{Introduction to Statistical Learning} with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani
\item
  Pages 104 - 109 of \href{https://www-bcf.usc.edu/~gareth/ISL/ISLR\%20Seventh\%20Printing.pdf}{An Introduction to Statistical Learning with Applications in R} by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani
\item
  \href{https://topepo.github.io/caret/index.html}{The \texttt{caret} Package}
\item
  Chapters 6 - 11 of \href{https://moderndive.com/}{Modern Dive} Statistical Inference via Data Science by Chester Ismay and Albert Y. Kim
\end{itemize}

\hypertarget{clustering}{%
\chapter{Clustering}\label{clustering}}

\hypertarget{overview-8}{%
\section{Overview}\label{overview-8}}

As part of exploratory data analysis, it is often helpful to see if there are
meaningful subgroups (or \emph{clusters}) in the data; this grouping can be used
for many purposes, such as generating new questions or improving predictive analyses.
This chapter provides an introduction to clustering using the \emph{K-means} algorithm,
including techniques to choose the number of clusters.

\hypertarget{chapter-learning-objectives-9}{%
\section{Chapter learning objectives}\label{chapter-learning-objectives-9}}

By the end of the chapter, students will be able to:

\begin{itemize}
\tightlist
\item
  Describe a case where clustering is appropriate, and what insight it might extract from the data.
\item
  Explain the K-means clustering algorithm.
\item
  Interpret the output of a K-means analysis.
\item
  Identify when it is necessary to scale variables before clustering, and do this using R.
\item
  Perform K-means clustering in R using \texttt{kmeans}.
\item
  Use the elbow method to choose the number of clusters for K-means.
\item
  Visualize the output of K-means clustering in R using coloured scatter plots.
\item
  Describe the advantages, limitations and assumptions of the K-means clustering algorithm.
\end{itemize}

\hypertarget{clustering-1}{%
\section{Clustering}\label{clustering-1}}

Clustering is a data analysis task involving separating a data set into
subgroups of related data. For example, we might use clustering to separate a
data set of documents into groups that correspond to topics, a data set of human
genetic information into groups that correspond to ancestral subpopulations, or
a data set of online customers into groups that correspond to purchasing
behaviours. Once the data are separated we can, for example, use the subgroups
to generate new questions about the data and follow up with a predictive
modelling exercise. In this course, clustering will be used only for exploratory
analysis, i.e., uncovering patterns in the data that we have.

Note that clustering is a fundamentally different kind of task than
classification or regression. Most notably, both classification and regression
are \emph{supervised tasks} where there is a \emph{predictive target} (a class label or
value), and we have examples of past data with labels/values that help us
predict those of future data. By contrast, clustering is an \emph{unsupervised
task}, as we are trying to understand and examine the structure of data without
any labels to help us. This approach has both advantages and disadvantages.
Clustering requires no additional annotation or input on the data; for example,
it would be nearly impossible to annotate all the articles on Wikipedia with
human-made topic labels, but we can still cluster the articles without this
information to automatically find groupings corresponding to topics.

However, because there is no predictive target, it is not as easy to evaluate
the ``quality'' of a clustering. With classification, we are able to use a test
data set to assess prediction performance. In clustering, there is not a single
good choice for evaluation. In this book, we will use visualization to ascertain
the quality of a clustering, and leave rigorous evaluation for more advanced
courses.

\begin{quote}
There are also so-called \emph{semisupervised} tasks, where only some of the data
come with labels / annotations, but the vast majority don't. The goal
is to try to uncover underlying structure in the data that allows one to
guess the missing labels. This sort of task is very useful, for example, when one
has an unlabelled data set that is too large to manually label, but one is willing to
provide a few informative example labels as a ``seed'' to guess the labels for all the data.
\end{quote}

\textbf{An illustrative example}

Here we will present an illustrative example using a simulated data set. Suppose
we have data with two variables measuring customer loyalty and
satisfaction, and we want to learn whether there are distinct ``types''
of customer. Understanding this might help us come up with better products or
promotions to improve our business in a data-driven way.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{marketing\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 19 x 2
##    loyalty  csat
##      <dbl> <dbl>
##  1     7    1   
##  2     7.5  1   
##  3     8    2   
##  4     7    2   
##  5     8    3   
##  6     1.5  1.75
##  7     1    3   
##  8     0.5  4   
##  9     2    4   
## 10     7    6   
## 11     6    6   
## 12     7    7   
## 13     6    7   
## 14     5    7   
## 15     9.5  8   
## 16     7    8   
## 17     8.3  9   
## 18     4    8   
## 19     2    3
\end{verbatim}

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/10-toy-example-plot-1.pdf}
\caption{\label{fig:10-toy-example-plot}Simulated data of customer loyalty versus satisfaction. Example derived from \citet{custexample}.}
\end{figure}

Based on this visualization, we might suspect there are a few subtypes of customer,
selected from combinations of high/low satisfaction and high/low loyalty. How
do we find this grouping automatically, and how do we pick the number of subtypes
to look for? The
way to rigorously separate the data into groups is to use a clustering algorithm.
In this chapter, we will focus on the \emph{K-means} algorithm, a widely-used
and often very effective clustering method, combined with the \emph{elbow method} for
selecting the number of clusters. This procedure will separate the data into
the following groups denoted by colour:

\includegraphics{bookdown_files/figure-latex/10-toy-example-clustering-1.pdf}

What are the labels for these groups? Unfortunately, we don't have any. K-means,
like almost all clustering algorithms, just outputs meaningless ``cluster labels''
that are typically whole numbers: 1, 2, 3, etc. But in a simple case like this,
where we can easily visualize the clusters on a scatter plot, we can give
human-made labels to the groups using their positions on
the plot:

\begin{itemize}
\tightlist
\item
  low loyalty and low satisfaction (blue cluster),
\item
  high loyalty and low satisfaction (pink cluster),
\item
  and high loyalty and high satisfaction (green cluster).
\end{itemize}

Once we have made these determinations, we can use them to inform our future business decisions,
or to ask further questions about our data. For example, here we might notice based on our clustering
that there aren't any customers with high satisfaction but low loyalty, and generate new analyses
or business strategies based on this information.

\hypertarget{k-means}{%
\section{K-means}\label{k-means}}

\hypertarget{measuring-cluster-quality}{%
\subsection{Measuring cluster quality}\label{measuring-cluster-quality}}

The K-means algorithm is a procedure that groups data into K clusters.
It starts with an initial clustering of the data, and then iteratively
improves it by making adjustments to the assignment of data
to clusters until it cannot improve any further. But how do we measure
the ``quality'' of a clustering, and what does it mean to improve it?
In K-means clustering, we measure the quality of a cluster by its
\emph{within-cluster sum-of-squared-distances} (WSSD). Computing this involves two steps.
First, we find the cluster centers by computing the mean of each variable
over data points in the cluster. For example, suppose we have a
cluster containing 3 observations, and we are using two variables, \(x\) and \(y\), to cluster the data.
Then we would compute the \(x\) and \(y\) variables, \(\mu_x\) and \(\mu_y\), of the cluster center via

\[\mu_x = \frac{1}{3}(x_1+x_2+x_3) \quad \mu_y = \frac{1}{3}(y_1+y_2+y_3).\]

In the first cluster from the customer satisfaction/loyalty example, there
are 5 data points. These are shown with their cluster center
(\texttt{csat\ =\ 1.8} and \texttt{loyalty\ =\ 7.5}) highlighted below.

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/10-toy-example-clus1-center-1.pdf}
\caption{\label{fig:10-toy-example-clus1-center}Cluster 1 from the toy example, with center highlighted.}
\end{figure}

The second step in computing the WSSD is to add up the squared distance between each point in the cluster and the cluster center.
We use the straight-line / Euclidean distance formula that we learned about in the classification chapter.
In the 3-observation cluster example above, we would compute the WSSD \(S^2\) via

\[S^2 = \left((x_1 - \mu_x)^2 + (y_1 - \mu_y)^2\right) + \left((x_2 - \mu_x)^2 + (y_2 - \mu_y)^2\right) +\left((x_3 - \mu_x)^2 + (y_3 - \mu_y)^2\right).\]

These distances are denoted by lines for the first cluster of the customer satisfaction/loyalty data example below.

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/10-toy-example-clus1-dists-1.pdf}
\caption{\label{fig:10-toy-example-clus1-dists}Cluster 1 from the toy example, with distances to the center highlighted.}
\end{figure}

The larger the value of \(S^2\), the more spread-out the cluster is, since large \(S^2\) means that points are far away from the cluster center.
Note, however, that ``large'' is relative to \emph{both} the scale of the variables for clustering \emph{and} the number of points in the cluster; a
cluster where points are very close to the center might still have a large \(S^2\) if there are many data points in the cluster.

\hypertarget{the-clustering-algorithm}{%
\subsection{The clustering algorithm}\label{the-clustering-algorithm}}

The K-means algorithm is quite simple. We begin by picking K, and uniformly randomly assigning data to the K clusters.
Then K-means consists of two major steps that attempt to minimize the
sum of WSSDs over all the clusters, i.e.~the \emph{total WSSD}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Center update:} Compute the center of each cluster.
\item
  \textbf{Label update:} Reassign each data point to the cluster with the nearest center.
\end{enumerate}

These two steps are repeated until the cluster assignments no longer change.
For example, in the customer data example from earlier, our initialization might look like this:

\includegraphics{bookdown_files/figure-latex/10-toy-kmeans-init-1.pdf}

And the first four iterations of K-means would look like (each row corresponds to an iteration,
where the left column depicts the center update,
and the right column depicts the reassignment of data to clusters):

\textbf{Center Update}                            \textbf{Label Update}
\includegraphics{bookdown_files/figure-latex/10-toy-kmeans-iter-1.pdf} \includegraphics{bookdown_files/figure-latex/10-toy-kmeans-iter-2.pdf} \includegraphics{bookdown_files/figure-latex/10-toy-kmeans-iter-3.pdf} \includegraphics{bookdown_files/figure-latex/10-toy-kmeans-iter-4.pdf}

Note that at this point we can terminate the algorithm, since none of the assignments changed
in the fourth iteration; both the centers and labels will remain the same from this point onward.

\begin{quote}
Is K-means \emph{guaranteed} to stop at some point, or could it iterate forever? As it turns out,
the answer is thankfully that K-means is guaranteed to stop after \emph{some} number of iterations. For the interested reader, the
logic for this has three steps: (1) both the label update and the center update decrease total WSSD in each iteration,
(2) the total WSSD is always greater than or equal to 0, and (3) there are only a finite number of possible
ways to assign the data to clusters. So at some point, the total WSSD must stop decreasing, which means none of the assignments
are changing and the algorithm terminates.
\end{quote}

\hypertarget{random-restarts}{%
\subsection{Random restarts}\label{random-restarts}}

K-means, unlike the classification and regression models we studied in previous chapters, can get ``stuck'' in a bad solution.
For example, if we were unlucky and initialized K-means with the following labels:

\includegraphics{bookdown_files/figure-latex/10-toy-kmeans-bad-init-1.pdf}

Then the iterations of K-means would look like:

\textbf{Center Update}                            \textbf{Label Update}
\includegraphics{bookdown_files/figure-latex/10-toy-kmeans-bad-iter-1.pdf} \includegraphics{bookdown_files/figure-latex/10-toy-kmeans-bad-iter-2.pdf} \includegraphics{bookdown_files/figure-latex/10-toy-kmeans-bad-iter-3.pdf} \includegraphics{bookdown_files/figure-latex/10-toy-kmeans-bad-iter-4.pdf} \includegraphics{bookdown_files/figure-latex/10-toy-kmeans-bad-iter-5.pdf}

This looks like a relatively bad clustering of the data, but K-means cannot improve it.
To solve this problem when clustering data using K-means, we should randomly re-initialize the labels a few times, run K-means for each initialization,
and pick the clustering that has the lowest final total WSSD.

\hypertarget{choosing-k}{%
\subsection{Choosing K}\label{choosing-k}}

In order to cluster data using K-means, we also have to pick the number of clusters, K.
But unlike in classification, we have no data labels and cannot perform
cross-validation with some measure of model prediction error.
Further, if K is chosen too small, then multiple clusters get grouped together;
if K is too large, then clusters get subdivided. In both cases, we will potentially miss
interesting structure in the data. For example, take a look below at the K-means
clustering of our customer satisfaction and loyalty data for a number of clusters
ranging from 1 to 9.

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/10-toy-kmeans-vary-k-1.pdf}
\caption{\label{fig:10-toy-kmeans-vary-k}Clustering of the customer data for \# clusters ranging from 1 to 9.}
\end{figure}

If we set K less than 3, then the clustering merges separate groups of data; this causes a large
total WSSD, since the cluster center (denoted by an ``x'') is not close to any of the data in the cluster. On
the other hand, if we set K greater than 3, the clustering subdivides subgroups of data; this does indeed still
decrease the total WSSD, but by only a \emph{diminishing amount}. If we plot the total WSSD versus the number of
clusters, we see that the decrease in total WSSD levels off (or forms an ``elbow shape'') when we reach roughly
the right number of clusters.

\includegraphics{bookdown_files/figure-latex/10-toy-kmeans-elbow-1.pdf}

\hypertarget{data-pre-processing-for-k-means}{%
\section{Data pre-processing for K-means}\label{data-pre-processing-for-k-means}}

Similar to K-nearest neighbours classification and regression, K-means
clustering uses straight-line distance to decide which points are similar to
each other. Therefore, the \emph{scale} of each of the variables in the data
will influence which cluster data points end up being assigned to.
Variables that have a large scale will have a much larger
effect on deciding cluster assignment than variables with a small scale.
To address this problem, we typically standardize our data before clustering,
which ensures that each variable has a mean of 0 and standard deviation of 1.
The \texttt{scale} function in R can be used to do this.
We show an example of how to use this function
below using the data in this chapter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scaled\_data }\OtherTok{\textless{}{-}} \FunctionTok{map\_df}\NormalTok{(unscaled\_data, scale)}
\NormalTok{scaled\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 19 x 2
##    loyalty[,1] csat[,1]
##          <dbl>    <dbl>
##  1       0.541   -1.40 
##  2       0.720   -1.40 
##  3       0.899   -1.03 
##  4       0.541   -1.03 
##  5       0.899   -0.659
##  6      -1.43    -1.12 
##  7      -1.61    -0.659
##  8      -1.79    -0.288
##  9      -1.25    -0.288
## 10       0.541    0.454
## 11       0.183    0.454
## 12       0.541    0.826
## 13       0.183    0.826
## 14      -0.175    0.826
## 15       1.44     1.20 
## 16       0.541    1.20 
## 17       1.01     1.57 
## 18      -0.533    1.20 
## 19      -1.25    -0.659
\end{verbatim}

\hypertarget{k-means-in-r}{%
\section{K-means in R}\label{k-means-in-r}}

To peform K-means clustering in R, we use the \texttt{kmeans} function. It takes at
least two arguments: the data frame containing the data you wish to cluster,
and K, the number of clusters (here we choose K = 3). Note that since the K-means
algorithm uses a random initialization of assignments, we need to set the random
seed to make the clustering reproducible.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{marketing\_clust }\OtherTok{\textless{}{-}} \FunctionTok{kmeans}\NormalTok{(marketing\_data, }\AttributeTok{centers =} \DecValTok{3}\NormalTok{)}
\NormalTok{marketing\_clust}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## K-means clustering with 3 clusters of sizes 9, 5, 5
## 
## Cluster means:
##   loyalty  csat label
## 1   6.644 7.333 1.778
## 2   7.500 1.800 3.000
## 3   1.400 3.150 3.000
## 
## Clustering vector:
##  [1] 2 2 2 2 2 3 3 3 3 1 1 1 1 1 1 1 1 1 3
## 
## Within cluster sum of squares by cluster:
## [1] 31.36  3.80  5.15
##  (between_SS / total_SS =  85.6 %)
## 
## Available components:
## 
## [1] "cluster"      "centers"      "totss"       
## [4] "withinss"     "tot.withinss" "betweenss"   
## [7] "size"         "iter"         "ifault"
\end{verbatim}

As you can see above, the clustering object returned by \texttt{kmeans} has a lot of information
that can be used to visualize the clusters, pick K, and evaluate the total WSSD.
To obtain this information in a tidy format, we will call in help
from the \texttt{broom} package. Let's start by visualizing the clustering
as a coloured scatter plot. To do that
we use the \texttt{augment} function, which takes in the model and the original data
frame, and returns a data frame with the data and the cluster assignments for
each point:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clustered\_data }\OtherTok{\textless{}{-}} \FunctionTok{augment}\NormalTok{(marketing\_clust, marketing\_data)}
\NormalTok{clustered\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 19 x 4
##    loyalty  csat label .cluster
##      <dbl> <dbl> <chr> <fct>   
##  1     7    1    3     2       
##  2     7.5  1    3     2       
##  3     8    2    3     2       
##  4     7    2    3     2       
##  5     8    3    3     2       
##  6     1.5  1.75 3     3       
##  7     1    3    3     3       
##  8     0.5  4    3     3       
##  9     2    4    3     3       
## 10     7    6    2     1       
## 11     6    6    2     1       
## 12     7    7    2     1       
## 13     6    7    2     1       
## 14     5    7    1     1       
## 15     9.5  8    2     1       
## 16     7    8    2     1       
## 17     8.3  9    2     1       
## 18     4    8    1     1       
## 19     2    3    3     3
\end{verbatim}

Now that we have this information in a tidy data frame, we can make a visualization
of the cluster assignments for each point:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cluster\_plot }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(clustered\_data, }
                       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ csat, }\AttributeTok{y =}\NormalTok{ loyalty, }\AttributeTok{colour =}\NormalTok{ .cluster), }
                       \AttributeTok{size=}\DecValTok{2}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{\textquotesingle{}Customer satisfaction\textquotesingle{}}\NormalTok{, }\AttributeTok{y =} \StringTok{\textquotesingle{}Loyalty\textquotesingle{}}\NormalTok{, }\AttributeTok{colour =} \StringTok{\textquotesingle{}Cluster\textquotesingle{}}\NormalTok{)}
\NormalTok{cluster\_plot}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/10-plot-clusters-2-1.pdf}

As mentioned above, we also need to select K by finding
where the ``elbow'' occurs in the plot of total WSSD versus number of clusters.
We can obtain the total WSSD (\texttt{tot.withinss}) from our
clustering using \texttt{broom}'s \texttt{glance} function. For example:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glance}\NormalTok{(marketing\_clust)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 4
##   totss tot.withinss betweenss  iter
##   <dbl>        <dbl>     <dbl> <int>
## 1  280.         40.3      239.     2
\end{verbatim}

To calculate the total WSSD for a variety of Ks, we will
create a data frame with a column named \texttt{k} with rows containing
each value of K we want to run K-means with (here, 1 to 9).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{marketing\_clust\_ks }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{k =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{) }
\NormalTok{marketing\_clust\_ks}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 9 x 1
##       k
##   <int>
## 1     1
## 2     2
## 3     3
## 4     4
## 5     5
## 6     6
## 7     7
## 8     8
## 9     9
\end{verbatim}

Then we use \texttt{map} to apply the \texttt{kmeans} function to each
K. However, we need to use \texttt{map} a little bit differently than we have
before. This is because we need to iterate over \texttt{k}, which is the \emph{second argument}
to the \texttt{kmeans} function. In the past, we have used \texttt{map} only to iterate over values of the
\emph{first argument} of a function. Since that is the default, we could simply
write \texttt{map(data\_frame,\ function\_name)}. This won't work here; we need to
provide our data frame as the first argument to the \texttt{kmeans} function.

The solution is to create something called an \emph{anonymous function}.
An anonymous function is a function that has no name,
unlike other functions you've seen so far (\texttt{kmeans}, \texttt{select}, etc).
To do this we will write our map statement like this:

\begin{verbatim}
map(marketing_clust_ks, function(k) kmeans(marketing_data, k))
\end{verbatim}

The anonymous function in the above call is \texttt{function(k)\ kmeans(marketing\_data,\ k)}.
This function takes a single argument (\texttt{k}) and evaluates \texttt{kmeans(marketing\_data,\ k)}.
Since \texttt{k} is the \emph{first} (and only) argument to the function, we can use \texttt{map} just
like we did before! The rest of the call above does just that -- it passes each row of
\texttt{marketing\_clust\_ks} to our anonymous function.

Below, we execute this \texttt{map} call inside of a \texttt{mutate} call on the
\texttt{marketing\_clust\_ks} data frame and get a list column that contains a K-means
clustering object for each value of K we had:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{marketing\_clust\_ks }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{k =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{marketing\_clusts =} \FunctionTok{map}\NormalTok{(k, }\ControlFlowTok{function}\NormalTok{(ks) }\FunctionTok{kmeans}\NormalTok{(marketing\_data, ks)))}
\NormalTok{marketing\_clust\_ks}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 9 x 2
##       k marketing_clusts
##   <int> <list>          
## 1     1 <kmeans>        
## 2     2 <kmeans>        
## 3     3 <kmeans>        
## 4     4 <kmeans>        
## 5     5 <kmeans>        
## 6     6 <kmeans>        
## 7     7 <kmeans>        
## 8     8 <kmeans>        
## 9     9 <kmeans>
\end{verbatim}

Next, we use \texttt{map} again to apply \texttt{glance} to each of the K-means
clustering objects to get the clustering statistics (including WSSD). The output
of glance is a data frame, and so we get another list column. This results in a
complex data frame with 3 columns, one for K, one for the
K-means clustering objects, and one for the clustering statistics:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{marketing\_clust\_ks }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{k =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{marketing\_clusts =} \FunctionTok{map}\NormalTok{(k, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{kmeans}\NormalTok{(marketing\_data, .x)),}
         \AttributeTok{glanced =} \FunctionTok{map}\NormalTok{(marketing\_clusts, glance)) }
\NormalTok{marketing\_clust\_ks}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 9 x 3
##       k marketing_clusts glanced         
##   <int> <list>           <list>          
## 1     1 <kmeans>         <tibble [1 x 4]>
## 2     2 <kmeans>         <tibble [1 x 4]>
## 3     3 <kmeans>         <tibble [1 x 4]>
## 4     4 <kmeans>         <tibble [1 x 4]>
## 5     5 <kmeans>         <tibble [1 x 4]>
## 6     6 <kmeans>         <tibble [1 x 4]>
## 7     7 <kmeans>         <tibble [1 x 4]>
## 8     8 <kmeans>         <tibble [1 x 4]>
## 9     9 <kmeans>         <tibble [1 x 4]>
\end{verbatim}

Finally we extract the total WSSD from the \texttt{glanced} column. Given that each
item in this column is a data frame, we will need to use the \texttt{unnest} function
to unpack the data frames into simpler column data types.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{clustering\_statistics }\OtherTok{\textless{}{-}}\NormalTok{ marketing\_clust\_ks }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(glanced)}

\NormalTok{clustering\_statistics}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 9 x 6
##       k marketing_clusts totss tot.withinss betweenss
##   <int> <list>           <dbl>        <dbl>     <dbl>
## 1     1 <kmeans>          280.       280.   -5.68e-14
## 2     2 <kmeans>          280.       138.    1.42e+ 2
## 3     3 <kmeans>          280.        40.3   2.39e+ 2
## 4     4 <kmeans>          280.        37.8   2.42e+ 2
## 5     5 <kmeans>          280.        15.2   2.64e+ 2
## 6     6 <kmeans>          280.        35.7   2.44e+ 2
## 7     7 <kmeans>          280.        12.0   2.68e+ 2
## 8     8 <kmeans>          280.         8.98  2.71e+ 2
## 9     9 <kmeans>          280.         8.70  2.71e+ 2
## # ... with 1 more variable: iter <int>
\end{verbatim}

Now that we have \texttt{tot.withinss} and \texttt{k} as columns in a data frame, we can make a line plot
and search for the ``elbow'' to find which value of K to use.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{elbow\_plot }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(clustering\_statistics, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ k, }\AttributeTok{y =}\NormalTok{ tot.withinss)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"K"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Total within{-}cluster sum of squares"}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{)}
\NormalTok{elbow\_plot}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/10-plot-choose-k-1.pdf}

It looks like 3 clusters is the right choice for this data.
But why is there a ``bump'' in the total WSSD plot here? Shouldn't total WSSD always
decrease as we add more clusters? Technically yes, but remember: K-means can
get ``stuck'' in a bad solution. Unfortunately, for K = 6 we had an unlucky initialization
and found a bad clustering! We can help prevent finding a bad clustering by trying a
few different random initializations via the \texttt{nstart} argument (here we use 10 restarts).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{marketing\_clust\_ks }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{k =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{marketing\_clusts =} \FunctionTok{map}\NormalTok{(k, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{kmeans}\NormalTok{(marketing\_data, }\AttributeTok{nstart =} \DecValTok{10}\NormalTok{, .x)),}
         \AttributeTok{glanced =} \FunctionTok{map}\NormalTok{(marketing\_clusts, glance)) }

\NormalTok{clustering\_statistics }\OtherTok{\textless{}{-}}\NormalTok{ marketing\_clust\_ks }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest}\NormalTok{(glanced)}

\NormalTok{elbow\_plot }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(clustering\_statistics, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ k, }\AttributeTok{y =}\NormalTok{ tot.withinss)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"K"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Total within{-}cluster sum of squares"}\NormalTok{)}\SpecialCharTok{+}
  \FunctionTok{scale\_x\_continuous}\NormalTok{(}\AttributeTok{breaks =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{9}\NormalTok{)}
\NormalTok{elbow\_plot}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/10-choose-k-nstart-1.pdf}

\hypertarget{additional-resources-4}{%
\section{Additional resources}\label{additional-resources-4}}

For more about clustering and K-means, refer to pages 385-390 and 404-405
of \href{http://www-bcf.usc.edu/~gareth/ISL/ISLR\%20Seventh\%20Printing.pdf}{Introduction to Statistical Learning with Applications in R} \citeyearpar{james2013introduction}. We have also linked a helpful companion video below:

\hypertarget{inference}{%
\chapter{Introduction to Statistical Inference}\label{inference}}

\hypertarget{overview-9}{%
\section{Overview}\label{overview-9}}

A typical data analysis task in practice is to draw conclusions about some unknown
aspect of a population of interest based on observed data sampled from that
population; we typically do not get data on the \emph{entire} population.
Data analysis questions regarding how summaries,
patterns, trends, or relationships in a data set
extend to the wider population are called \emph{inferential questions}. This chapter will start
with the fundamental ideas of sampling from populations and then introduce two common techniques in statistical inference: \emph{point estimation} and
\emph{interval estimation}.

\hypertarget{chapter-learning-objectives-10}{%
\section{Chapter learning objectives}\label{chapter-learning-objectives-10}}

By the end of the chapter, students will be able to:

\begin{itemize}
\tightlist
\item
  Describe real-world examples of questions that can be answered with the statistical inference.
\item
  Define common population parameters (e.g.~mean, proportion, standard deviation) that are often estimated using sampled data, and estimate these from a sample.
\item
  Define the following statistical sampling terms (population, sample, population parameter, point estimate, sampling distribution).
\item
  Explain the difference between a population parameter and sample point estimate.
\item
  Use R to draw random samples from a finite population.
\item
  Use R to create a sampling distribution from a finite population.
\item
  Describe how sample size influences the sampling distribution.
\item
  Define bootstrapping.
\item
  Use R to create a bootstrap distribution to approximate a sampling distribution.
\item
  Contrast the bootstrap and sampling distributions.
\end{itemize}

\hypertarget{why-do-we-need-sampling}{%
\section{Why do we need sampling?}\label{why-do-we-need-sampling}}

Statistical inference can help us decide how quantities we observe in
a subset of data relate to the same quantities in the broader
population. Suppose a retailer is considering selling iPhone accessories, and they want to estimate how big the market might be. Additionally, they want to strategize how they can market their products on North American college and university campuses. This retailer might use statistical inference to answer the question:

\emph{What proportion of all undergraduate students in North America own an iPhone?}

In the above question, we are interested in making a conclusion about \emph{all}
undergraduate students in North America; this is our \textbf{population}.
In general, the population is the complete collection of individuals or cases we are interested in studying.
Further, in the above question, we are interested in computing a quantity---the proportion
of iPhone owners---based on the entire population. This is our \textbf{population parameter}.
In general, a population parameter is a numerical characteristic
of the entire population. To compute this number in the example above, we would need to ask
every single undergraduate in North America whether or not they own an iPhone. In practice,
directly computing population parameters is often time-consuming and costly, and sometimes impossible.

A more practical approach would be to collect measurements for a \textbf{sample}: a subset of
individuals collected from the population. We can then compute a \textbf{sample estimate}---a numerical
characteristic of the sample---that estimates the population parameter. For example, suppose we randomly selected 100 undergraduate students across North America (the sample) and computed the proportion of those
students who own an iPhone (the sample estimate). In that case, we might suspect that that proportion is a reasonable estimate of the proportion of students who own an iPhone in the entire population.

\begin{figure}
\includegraphics[width=1\linewidth]{img/population_vs_sample} \caption{Population versus sample}\label{fig:11-population-vs-sample}
\end{figure}

Note that proportions are not the \emph{only} kind of population parameter we might be interested in. Suppose an undergraduate student studying at the University of British Columbia in Vancouver, British Columbia, is looking for an apartment to rent. They need to create a budget, so they want to know something about studio apartment rental prices in Vancouver, BC. This student might use statistical inference to tackle the question:

\emph{What is the average price-per-month of studio apartment rentals in Vancouver, Canada?}

The population consists of all studio apartment rentals in Vancouver, and the population parameter is the \emph{average price-per-month}. Here we used the average as a measure of center to describe the ``typical value'' of studio apartment rental prices. But even within this one example, we could also be interested in many other population parameters. For instance, we know that not every studio apartment rental in Vancouver will have the same price-per-month. The student might be interested in how much monthly prices vary and want to find a measure of the rentals' spread (or variability), such as the standard deviation. We might be interested in the fraction of studio apartment rentals that cost more than \$1000 per month. And the list of population parameters we might want to calculate goes on. The question we want to answer will help us determine the parameter we want to estimate. If we were somehow able to observe the whole population of studio apartment rental offerings in Vancouver, we could compute each of these numbers exactly; therefore, these are all population parameters. There are many kinds of observations and population parameters that you will run into in practice, but in this chapter, we will focus on two settings:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Using categorical observations to estimate the proportion of each category
\item
  Using quantitative observations to estimate the average (or mean)
\end{enumerate}

\hypertarget{sampling-distributions}{%
\section{Sampling distributions}\label{sampling-distributions}}

\hypertarget{sampling-distributions-for-proportions}{%
\subsection{Sampling distributions for proportions}\label{sampling-distributions-for-proportions}}

Let's start with an illustrative (and tasty!) example. Timbits are
bite-sized doughnuts sold at Tim Hortons, a popular Canadian-based fast-food restaurant
chain founded in Hamilton, Ontario, Canada.

\begin{figure}
\includegraphics[width=0.3\linewidth]{img/timbits} \caption{A box of Timbits}\label{fig:11-timbits-picture}
\end{figure}

Suppose we wanted to estimate the true proportion of chocolate doughnuts at Tim
Hortons restaurants. Now, of course, we (the authors!) do not have access to the true population.
So for this chapter, we created a fictitious box of 10,000 Timbits with two flavours---old-fashioned
and chocolate---as our population, and use this to illustrate
inferential concepts. Below we have a \texttt{tibble()} called \texttt{virtual\_box} with a Timbit ID and flavour as our columns. We have also loaded our necessary packages: \texttt{tidyverse} and the \texttt{infer} package, which we will need to perform sampling later in the chapter.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(infer)}
\NormalTok{virtual\_box}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10,000 x 2
##    timbit_id flavour      
##        <dbl> <fct>        
##  1         1 chocolate    
##  2         2 chocolate    
##  3         3 chocolate    
##  4         4 chocolate    
##  5         5 old fashioned
##  6         6 old fashioned
##  7         7 chocolate    
##  8         8 chocolate    
##  9         9 old fashioned
## 10        10 chocolate    
## # ... with 9,990 more rows
\end{verbatim}

From our simulated box, we can see that the proportion of chocolate Timbits is
0.63. This value, 0.63, is the \emph{population parameter}. Note that this parameter value is usually unknown in real data analysis problems.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{virtual\_box }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(flavour) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}
    \AttributeTok{n =} \FunctionTok{n}\NormalTok{(),}
    \AttributeTok{proportion =} \FunctionTok{n}\NormalTok{() }\SpecialCharTok{/} \DecValTok{10000}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##   flavour           n proportion
##   <fct>         <int>      <dbl>
## 1 old fashioned  3705      0.370
## 2 chocolate      6295      0.630
\end{verbatim}

What would happen if we were to buy a box of 40 randomly-selected Timbits and count the number of chocolate Timbits (\emph{i.e.,} take a random sample of size 40 from our Timbits population)? Let's use R to simulate this using our \texttt{virtual\_box} population. We can do this using the \texttt{rep\_sample\_n} function from the \texttt{infer} package. The arguments
of \texttt{rep\_sample\_n} are (1) the data frame (or tibble) to sample from, and (2) the size of the sample to take.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{samples\_1 }\OtherTok{\textless{}{-}} \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{tbl =}\NormalTok{ virtual\_box, }\AttributeTok{size =} \DecValTok{40}\NormalTok{)}
\NormalTok{choc\_sample\_1 }\OtherTok{\textless{}{-}} \FunctionTok{summarize}\NormalTok{(samples\_1,}
  \AttributeTok{n =} \FunctionTok{sum}\NormalTok{(flavour }\SpecialCharTok{==} \StringTok{"chocolate"}\NormalTok{),}
  \AttributeTok{prop =} \FunctionTok{sum}\NormalTok{(flavour }\SpecialCharTok{==} \StringTok{"chocolate"}\NormalTok{) }\SpecialCharTok{/} \DecValTok{40}
\NormalTok{)}
\NormalTok{choc\_sample\_1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##   replicate     n  prop
##       <int> <int> <dbl>
## 1         1    29 0.725
\end{verbatim}

Here we see that the proportion of chocolate Timbits in this random sample is
0.72. This value is our estimate --- our best guess of our population parameter using this sample. Given that it is a single
value that we are estimating, we often refer to it as a \textbf{point estimate}.

Now imagine we took another random sample of 40 Timbits from the population. Do you
think we would get the same proportion? Let's try sampling from the population
again and see what happens.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2}\NormalTok{)}
\NormalTok{samples\_2 }\OtherTok{\textless{}{-}} \FunctionTok{rep\_sample\_n}\NormalTok{(virtual\_box, }\AttributeTok{size =} \DecValTok{40}\NormalTok{)}
\NormalTok{choc\_sample\_2 }\OtherTok{\textless{}{-}} \FunctionTok{summarize}\NormalTok{(samples\_2,}
  \AttributeTok{n =} \FunctionTok{sum}\NormalTok{(flavour }\SpecialCharTok{==} \StringTok{"chocolate"}\NormalTok{),}
  \AttributeTok{prop =} \FunctionTok{sum}\NormalTok{(flavour }\SpecialCharTok{==} \StringTok{"chocolate"}\NormalTok{) }\SpecialCharTok{/} \DecValTok{40}
\NormalTok{)}
\NormalTok{choc\_sample\_2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 3
##   replicate     n  prop
##       <int> <int> <dbl>
## 1         1    27 0.675
\end{verbatim}

Notice that we get a different value for our estimate this time. The
proportion of chocolate Timbits in this sample is 0.68.
If we were to do this again, another random sample could also give a
different result. Estimates vary from sample to sample
due to \textbf{sampling variability}.

But just how much should we expect the estimates of our random
samples to vary? In order to understand this, we will simulate taking more samples
of size 40 from our population of Timbits, and calculate the
proportion of chocolate Timbits in each sample. We can then
visualize the distribution of sample proportions we calculate. The distribution
of the estimate for all possible samples of a given size (which we commonly refer to as \(n\)) from a population is
called a \textbf{sampling distribution}. The sampling distribution will help us see
how much we would expect our sample proportions from this population to vary
for samples of size 40. Below we again use the \texttt{rep\_sample\_n} to take samples
of size 40 from our population of Timbits, but we set the \texttt{reps} argument
to specify the number of samples to take, here 15,000. We will use the function \texttt{head()} to see the first few rows and \texttt{tail()} to see the last few rows of our \texttt{samples} data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples }\OtherTok{\textless{}{-}} \FunctionTok{rep\_sample\_n}\NormalTok{(virtual\_box, }\AttributeTok{size =} \DecValTok{40}\NormalTok{, }\AttributeTok{reps =} \DecValTok{15000}\NormalTok{)}
\FunctionTok{head}\NormalTok{(samples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
## # Groups:   replicate [1]
##   replicate timbit_id flavour      
##       <int>     <dbl> <fct>        
## 1         1      9054 chocolate    
## 2         1      4322 old fashioned
## 3         1      1685 chocolate    
## 4         1      3958 chocolate    
## 5         1      2765 old fashioned
## 6         1       358 old fashioned
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tail}\NormalTok{(samples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
## # Groups:   replicate [1]
##   replicate timbit_id flavour      
##       <int>     <dbl> <fct>        
## 1     15000      4633 old fashioned
## 2     15000       552 chocolate    
## 3     15000      7998 old fashioned
## 4     15000      8649 chocolate    
## 5     15000      2974 chocolate    
## 6     15000      7811 old fashioned
\end{verbatim}

Notice the column \texttt{replicate} is indicating the replicate, or sample, with which each
Timbit belongs. Since we took 15,000 samples of size 40, there are 15,000 replicates.
Now that we have taken 15,000 samples, to create a sampling distribution of sample proportions for samples of size 40, we need to calculate the proportion of chocolate Timbits for each sample, \(\hat{p}_\text{chocolate}\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_estimates }\OtherTok{\textless{}{-}}\NormalTok{ samples }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(replicate) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{sample\_proportion =} \FunctionTok{sum}\NormalTok{(flavour }\SpecialCharTok{==} \StringTok{"chocolate"}\NormalTok{) }\SpecialCharTok{/} \DecValTok{40}\NormalTok{)}
\FunctionTok{head}\NormalTok{(sample\_estimates)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   replicate sample_proportion
##       <int>             <dbl>
## 1         1             0.625
## 2         2             0.675
## 3         3             0.7  
## 4         4             0.675
## 5         5             0.45 
## 6         6             0.425
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tail}\NormalTok{(sample\_estimates)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   replicate sample_proportion
##       <int>             <dbl>
## 1     14995             0.575
## 2     14996             0.6  
## 3     14997             0.45 
## 4     14998             0.675
## 5     14999             0.7  
## 6     15000             0.525
\end{verbatim}

Now that we have calculated the proportion of chocolate Timbits for each sample, \(\hat{p}_\text{chocolate}\), we can visualize the sampling distribution of sample proportions for samples of size 40:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sampling\_distribution }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(sample\_estimates, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sample\_proportion)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{fill =} \StringTok{"dodgerblue3"}\NormalTok{, }\AttributeTok{color =} \StringTok{"lightgrey"}\NormalTok{, }\AttributeTok{bins =} \DecValTok{12}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Sample proportions"}\NormalTok{)}
\NormalTok{sampling\_distribution}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=0.6\linewidth]{bookdown_files/figure-latex/11-example-proportions7-1} \caption{Sampling distribution of the sample proportion for sample size 40}\label{fig:11-example-proportions7}
\end{figure}

The sampling distribution appears to be bell-shaped with one peak. It is
centered around 0.6 and the
sample proportions range from about 0.3 to
about 0.9. In fact, we can calculate
the mean of the sample proportions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_estimates }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(sample\_proportion))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##    mean
##   <dbl>
## 1 0.629
\end{verbatim}

We notice that the sample proportions are centred around the population proportion value, 0.63! In general, the mean of the distribution of \(\hat{p}\) should be equal to \(p\), which is good because that means the sample proportion is neither an overestimate nor an underestimate of the population proportion.

So what can we learn from this sampling distribution? This distribution tells us what we might expect from proportions from samples of size \(40\) when our population proportion is 0.63. In practice, we usually don't know the proportion of our population, but if we can use what we know about the sampling distribution, we can use it to make inferences about our population when we only have a single sample.

\hypertarget{sampling-distributions-for-means}{%
\subsection{Sampling distributions for means}\label{sampling-distributions-for-means}}

In the previous section, our variable of interest---Timbit flavour---was
\emph{categorical}, and the population parameter of interest was the proportion of chocolate
Timbits. As mentioned in the introduction to this chapter, there are many choices of population parameter for each type of observed variable. What if we wanted to infer something about a population of \emph{quantitative} variables instead? For instance, a traveller visiting Vancouver, BC may wish to know about the prices of staying somewhere using Airbnb, an online marketplace for arranging places to stay. Particularly, they might be interested in estimating the population mean price per night of Airbnb listings in Vancouver, BC. This section will study the case where we are interested in the population mean of a quantitative variable.

We will look at an example using data from \href{http://insideAirbnb.com/}{Inside Airbnb}. The data set contains Airbnb listings for Vancouver, Canada, in September 2020. Let's imagine (for learning purposes) that our data set represents the population of all Airbnb rental listings in Vancouver, and we are interested in the population mean price per night.
Our data contains an ID number, neighbourhood,
type of room, the number of people the rental accommodates, number of bathrooms, bedrooms, beds, and the price per night.

\begin{verbatim}
## # A tibble: 6 x 8
##      id neighbourhood room_type accommodates bathrooms
##   <int> <chr>         <chr>            <dbl> <chr>    
## 1     1 Downtown      Entire h~            5 2 baths  
## 2     2 Downtown Eas~ Entire h~            4 2 baths  
## 3     3 West End      Entire h~            2 1 bath   
## 4     4 Kensington-C~ Entire h~            2 1 bath   
## 5     5 Kensington-C~ Entire h~            4 1 bath   
## 6     6 Hastings-Sun~ Entire h~            4 1 bath   
## # ... with 3 more variables: bedrooms <dbl>,
## #   beds <dbl>, price <dbl>
\end{verbatim}

We can visualize the population distribution of the price per night with a histogram.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population\_distribution }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(airbnb, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{fill =} \StringTok{"dodgerblue3"}\NormalTok{, }\AttributeTok{color =} \StringTok{"lightgrey"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Price per night ($)"}\NormalTok{)}
\NormalTok{population\_distribution}
\end{Highlighting}
\end{Shaded}

\textbackslash begin\{figure\}
\includegraphics[width=0.6\linewidth]{bookdown_files/figure-latex/11-example-means2-1} \textbackslash caption\{Population distribution of price per night (\$) for all Airbnb listings in Vancouver, Canada\}\label{fig:11-example-means2}
\textbackslash end\{figure\}
We see that the distribution has one peak and is skewed---most of the listings
are less than \$250 per night, but a small proportion of listings cost more
than that, creating a long tail on the histogram's right side.

We can also calculate the population mean, the average price per night for all the Airbnb listings.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{population\_parameters }\OtherTok{\textless{}{-}}\NormalTok{ airbnb }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{pop\_mean =} \FunctionTok{mean}\NormalTok{(price))}
\NormalTok{population\_parameters}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   pop_mean
##      <dbl>
## 1     155.
\end{verbatim}

The price per night of all Airbnb rentals in Vancouver, BC is \$154.51, on average. This value is our population parameter since we are calculating it using the population data.

Suppose that we did not have access to the population data, yet we still wanted to estimate the mean price per night. We could answer this question by taking a random sample of as many Airbnb listings as we had time to, let's say we could do this for 40 listings. What would such a sample look like?

Let's take advantage of the fact that we do have access to the population data and simulate taking one random sample of 40 listings in R, again using \texttt{rep\_sample\_n}. After doing this we create a histogram to visualize the
distribution of observations in the sample,
and calculate the mean of our sample. This number is a \textbf{point estimate} for the mean of the full population.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_1 }\OtherTok{\textless{}{-}}\NormalTok{ airbnb }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\DecValTok{40}\NormalTok{)}
\FunctionTok{head}\NormalTok{(sample\_1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 9
## # Groups:   replicate [1]
##   replicate    id neighbourhood room_type accommodates
##       <int> <int> <chr>         <chr>            <dbl>
## 1         1  2317 Kensington-C~ Entire h~            2
## 2         1  1028 Riley Park    Entire h~            6
## 3         1  2487 South Cambie  Entire h~            2
## 4         1  2644 Downtown      Entire h~            2
## 5         1  3059 Downtown Eas~ Entire h~            9
## 6         1  2507 Fairview      Entire h~            4
## # ... with 4 more variables: bathrooms <chr>,
## #   bedrooms <dbl>, beds <dbl>, price <dbl>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_distribution }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(sample\_1, }\FunctionTok{aes}\NormalTok{(price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{fill =} \StringTok{"dodgerblue3"}\NormalTok{, }\AttributeTok{color =} \StringTok{"lightgrey"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Price per night ($)"}\NormalTok{)}
\NormalTok{sample\_distribution}
\end{Highlighting}
\end{Shaded}

\textbackslash begin\{figure\}
\includegraphics[width=0.6\linewidth]{bookdown_files/figure-latex/11-example-means3-1} \textbackslash caption\{Distribution of price per night (\$) for sample of 40 Airbnb listings\}\label{fig:11-example-means3}
\textbackslash end\{figure\}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{estimates }\OtherTok{\textless{}{-}}\NormalTok{ sample\_1 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{sample\_mean =} \FunctionTok{mean}\NormalTok{(price))}
\NormalTok{estimates}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 2
##   replicate sample_mean
##       <int>       <dbl>
## 1         1        127.
\end{verbatim}

Recall that the population mean
was \$154.51. We see that our point
estimate for the mean is \$127.35. So our estimate was actually quite close to the population parameter: the mean was
about 17.6\% off.
Note that in practice, we usually cannot compute the accuracy of the estimate, since we do not have access to the population
parameter; if we did, we wouldn't need to estimate it!

Also recall from the previous section that the point estimate can vary; if
we took another random sample from the population, then the value of our estimate may change.
So then did we just get lucky with our point estimate above?
How much does our estimate vary across different samples of size 40 in this example? Again, since we have access to the population,
we can take many samples and plot the \textbf{sampling distribution} of sample means for samples of size 40 to get a sense
for this variation. In this case, we'll use 15,000 samples of size 40.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samples }\OtherTok{\textless{}{-}} \FunctionTok{rep\_sample\_n}\NormalTok{(airbnb, }\AttributeTok{size =} \DecValTok{40}\NormalTok{, }\AttributeTok{reps =} \DecValTok{15000}\NormalTok{)}
\FunctionTok{head}\NormalTok{(samples)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 9
## # Groups:   replicate [1]
##   replicate    id neighbourhood room_type accommodates
##       <int> <int> <chr>         <chr>            <dbl>
## 1         1   896 Downtown      Entire h~            6
## 2         1  2723 Kensington-C~ Entire h~            4
## 3         1  3130 Fairview      Entire h~            3
## 4         1  2103 Kitsilano     Entire h~            2
## 5         1  4242 Downtown      Entire h~            2
## 6         1   610 Grandview-Wo~ Entire h~            6
## # ... with 4 more variables: bathrooms <chr>,
## #   bedrooms <dbl>, beds <dbl>, price <dbl>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_estimates }\OtherTok{\textless{}{-}}\NormalTok{ samples }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(replicate) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{sample\_mean =} \FunctionTok{mean}\NormalTok{(price))}
\FunctionTok{head}\NormalTok{(sample\_estimates)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   replicate sample_mean
##       <int>       <dbl>
## 1         1        142.
## 2         2        129.
## 3         3        114.
## 4         4        186.
## 5         5        137.
## 6         6        178.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sampling\_distribution\_40 }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(sample\_estimates, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ sample\_mean)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{fill =} \StringTok{"dodgerblue3"}\NormalTok{, }\AttributeTok{color =} \StringTok{"lightgrey"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Sample mean price per night ($)"}\NormalTok{)}
\NormalTok{sampling\_distribution\_40}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=0.6\linewidth]{bookdown_files/figure-latex/11-example-means4-1} \caption{Sampling distribution of the sample means for sample size of 40}\label{fig:11-example-means4}
\end{figure}

Here we see that the sampling distribution of the mean has one peak and is
bell-shaped. Most of the estimates are between
about \$140 and
\$170; but there are
a good fraction of cases outside this range (i.e., where the point estimate
was not close to the population parameter). So it does indeed look like we were quite lucky
when we estimated the population mean
with only 17.6\% error.
Let's visualize the population
distribution, distribution of the sample, and the sampling distribution on one
plot to compare them.

\begin{figure}
\includegraphics[width=0.7\linewidth]{bookdown_files/figure-latex/11-example-means5-1} \caption{Comparision of population distribution, sample distribution and sampling distribution}\label{fig:11-example-means5}
\end{figure}

Given that there is quite a bit of variation in the sampling distribution of the sample mean---i.e.,
the point estimate that we obtain is not very reliable---is there any way to improve the estimate?
One way to improve a point estimate is to take a \emph{larger} sample. To illustrate what effect this has,
we will take many samples of size 20, 50, 100, and 500, and plot the sampling distribution of the sample mean
below.

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/11-example-means7-1.pdf}
\caption{\label{fig:11-example-means7}Comparision of sampling distributions}
\end{figure}

Based on the visualization, two points about the sample mean become clear.
First, the
mean of the sample mean (across samples) is equal to the population mean.
Second, increasing the size of the sample
decreases the spread (i.e., the variability) in the sample mean
point estimate of the population mean. Therefore, a larger sample size results
in a more reliable point estimate of the population parameter.

\hypertarget{summary-1}{%
\subsection{Summary}\label{summary-1}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A \emph{point estimate} is a single value computed using a sample from a population (e.g.~a mean or proportion)
\item
  The \emph{sampling distribution} of an estimate is the distribution of the estimate for all possible samples of a fixed size from the same population.
\item
  The sample means and proportions calculated from samples are centered around the population mean and proportion, respectively.
\item
  The spread of the sampling distribution is related to the sample size. As the sample size increases, the spread of the sampling distribution decreases.
\item
  The shape of the sampling distribution is usually bell-shaped with one peak and centred at the population mean or proportion.
\end{enumerate}

\emph{Why all this emphasis on sampling distributions?}

Usually, we don't have access to the population data, so we cannot construct the sampling distribution as we did in this section. As we saw, our sample estimate's value will likely not equal the population parameter value exactly. We saw from the sampling distribution just how much our estimates can vary. So reporting a single \emph{point estimate} for the population parameter alone may not be enough. Using simulations, we can see patterns of the sample estimate's sampling distribution would look like for a sample of a given size. We can use these patterns to approximate the sampling distribution when we only have one sample, which is the realistic case. If we can ``predict'' what the sampling distribution would look like for a sample, we could construct a range of values we think the population parameter's value might lie. We can use our single sample and its properties that influence sampling distributions, such as the spread and sample size, to approximate the sampling distribution as best as we can. There are several methods to do this; however, in this book, we will use the bootstrap method to do this, as we will see in the next section.

\hypertarget{bootstrapping}{%
\section{Bootstrapping}\label{bootstrapping}}

\hypertarget{overview-10}{%
\subsection{Overview}\label{overview-10}}

We saw in the previous section that we could compute a \textbf{point estimate} of a population
parameter using a sample of observations from the population. And since we had access to the
population, we could evaluate how accurate the estimate was, and even get a sense for how much
the estimate would vary for different samples from the population.
But in real data analysis settings, we usually have \emph{just one sample} from our population,
and do not have access to the population itself. So how do we get a sense for how
variable our point estimate is when we only have one sample to work with?
In this section, we will discuss \textbf{interval estimation} and construct \textbf{confidence intervals} using just a single sample from a population. A confidence interval is a range of plausible values for our population parameter.

Here is the key idea. First, if you take a big enough sample, it \emph{looks like} the population. Notice the histograms' shapes for samples of different sizes taken from the population in the picture below. We see that for a large enough sample, the sample's distribution looks like that of the population.

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/11-example-bootstrapping0-1.pdf}
\caption{\label{fig:11-example-bootstrapping0}Comparision of samples of different sizes from the population}
\end{figure}

In the previous section, we took many samples of the same size \emph{from our population} to get
a sense for the variability of a sample estimate. But if our sample is big enough that it looks like our population,
we can pretend that our sample \emph{is} the population, and take more samples (with replacement) of the same size
from it instead! This very clever technique is called \textbf{the bootstrap}.
Note that by taking many samples from our single, observed sample, we do not obtain the true sampling distribution,
but rather an approximation that we call \textbf{the bootstrap distribution}.

\begin{quote}
Note that we need to sample \emph{with} replacement when using the bootstrap. Otherwise, if we had a sample of size \(n\),
and obtained a sample from it of size \(n\) \emph{without} replacement, it would just return our original sample.
\end{quote}

This section will explore how to create a bootstrap distribution from a single sample using R.
For a sample of size \(n\), the process we will go through is as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Randomly select an observation from the original sample, which was drawn from the population
\item
  Record the observation's value
\item
  Replace that observation
\item
  Repeat steps 1 - 3 (sampling \emph{with} replacement) until you have \(n\) observations, which form a bootstrap sample
\item
  Calculate the bootstrap point estimate (e.g., mean, median, proportion, slope, etc.) of the \(n\) observations in your bootstrap sample
\item
  Repeat steps (1) - (5) many times to create a distribution of point estimates (the bootstrap distribution)
\item
  Calculate the plausible range of values around our observed point estimate
\end{enumerate}

\begin{figure}
\includegraphics[width=1\linewidth]{img/intro-bootstrap} \caption{Overview of the bootstrap process}\label{fig:11-intro-bootstrap-image}
\end{figure}

\hypertarget{bootstrapping-in-r}{%
\subsection{Bootstrapping in R}\label{bootstrapping-in-r}}

Let's continue working with our Airbnb data. Once again, let's say we are interested in estimating the population mean price per night of all Airbnb listings in
Vancouver, Canada using a single sample we collected of size 40.

To simulate doing this in R, we will use \texttt{rep\_sample\_n} to take a random sample from from our population. In real life we wouldn't do this step in R, we would instead simply load the data into R, that we, or our collaborators collected.

After we have our sample, we will visualize it's distribution and calculate our point estimate, the sample mean.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_sample }\OtherTok{\textless{}{-}}\NormalTok{ airbnb }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\DecValTok{40}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ungroup}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \CommentTok{\# ungroup the data frame}
  \FunctionTok{select}\NormalTok{(price) }\CommentTok{\# drop the replicate column}
\FunctionTok{head}\NormalTok{(one\_sample)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 1
##   price
##   <dbl>
## 1   250
## 2   106
## 3   150
## 4   357
## 5    50
## 6   110
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_sample\_dist }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(one\_sample, }\FunctionTok{aes}\NormalTok{(price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{fill =} \StringTok{"dodgerblue3"}\NormalTok{, }\AttributeTok{color =} \StringTok{"lightgrey"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Price per night ($)"}\NormalTok{)}
\NormalTok{one\_sample\_dist}
\end{Highlighting}
\end{Shaded}

\textbackslash begin\{figure\}
\includegraphics[width=0.6\linewidth]{bookdown_files/figure-latex/11-bootstrapping1-1} \textbackslash caption\{Histogram of price per night (\$) for one sample of size 40\}\label{fig:11-bootstrapping1}
\textbackslash end\{figure\}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{one\_sample\_estimates }\OtherTok{\textless{}{-}}\NormalTok{ one\_sample }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{sample\_mean =} \FunctionTok{mean}\NormalTok{(price))}
\NormalTok{one\_sample\_estimates}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##   sample_mean
##         <dbl>
## 1        166.
\end{verbatim}

The sample distribution is skewed with a few observations out to the right. The
mean of the sample is \$165.62.
Remember, in practice, we usually only have one sample from the population. So
this sample and estimate are the only data we can work with.

We now perform steps (1) - (5) listed above to generate a single bootstrap sample in R using the
sample we just took, and calculate the bootstrap estimate for that sample. We
will use the \texttt{rep\_sample\_n} function as we did when we were creating our
sampling distribution. Since we want to sample with replacement, we change the
argument for \texttt{replace} from its default value of \texttt{FALSE} to \texttt{TRUE}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boot1 }\OtherTok{\textless{}{-}}\NormalTok{ one\_sample }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{40}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{reps =} \DecValTok{1}\NormalTok{)}
\FunctionTok{head}\NormalTok{(boot1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
## # Groups:   replicate [1]
##   replicate price
##       <int> <dbl>
## 1         1  201 
## 2         1  199 
## 3         1  127.
## 4         1   85 
## 5         1  169 
## 6         1   60
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boot1\_dist }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(boot1, }\FunctionTok{aes}\NormalTok{(price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{fill =} \StringTok{"dodgerblue3"}\NormalTok{, }\AttributeTok{color =} \StringTok{"lightgrey"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Price per night ($)"}\NormalTok{)}

\NormalTok{boot1\_dist}
\end{Highlighting}
\end{Shaded}

\begin{figure}
\includegraphics[width=0.6\linewidth]{bookdown_files/figure-latex/11-bootstrapping3-1} \caption{Bootstrap distribution}\label{fig:11-bootstrapping3}
\end{figure}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summarise}\NormalTok{(boot1, }\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(price))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 2
##   replicate  mean
##       <int> <dbl>
## 1         1  152.
\end{verbatim}

Notice that our bootstrap distribution has a similar shape to the original
sample distribution. Though the shapes of the distributions are similar, they
are not identical. You'll also notice that the original sample mean and the
bootstrap sample mean differ. How might that happen? Remember that we are
sampling with replacement from the original sample, so we don't end up with the
same sample values again. We are trying to mimic drawing another sample from
the population without actually having to do that.

Let's now take 15,000 bootstrap samples from the original sample we drew from the
population (\texttt{one\_sample}) using \texttt{rep\_sample\_n} and calculate the means for
each of those replicates. Recall that this assumes that \texttt{one\_sample} \emph{looks like}
our original population; but since we do not have access to the population itself,
this is often the best we can do.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boot15000 }\OtherTok{\textless{}{-}}\NormalTok{ one\_sample }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rep\_sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{40}\NormalTok{, }\AttributeTok{replace =} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{reps =} \DecValTok{15000}\NormalTok{)}
\FunctionTok{head}\NormalTok{(boot15000)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
## # Groups:   replicate [1]
##   replicate price
##       <int> <dbl>
## 1         1   200
## 2         1   176
## 3         1   105
## 4         1   105
## 5         1   105
## 6         1   132
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tail}\NormalTok{(boot15000)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
## # Groups:   replicate [1]
##   replicate price
##       <int> <dbl>
## 1     15000   357
## 2     15000    49
## 3     15000   115
## 4     15000   169
## 5     15000   145
## 6     15000   357
\end{verbatim}

Let's take a look at histograms of the first six replicates of our bootstrap samples.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{six\_bootstrap\_samples }\OtherTok{\textless{}{-}}\NormalTok{ boot15000 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(replicate }\SpecialCharTok{\textless{}=} \DecValTok{6}\NormalTok{)}
\FunctionTok{ggplot}\NormalTok{(six\_bootstrap\_samples, }\FunctionTok{aes}\NormalTok{(price)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{fill =} \StringTok{"dodgerblue3"}\NormalTok{, }\AttributeTok{color =} \StringTok{"lightgrey"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Price per night ($)"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{replicate)}
\end{Highlighting}
\end{Shaded}

\includegraphics{bookdown_files/figure-latex/11-bootstrapping-six-bootstrap-samples-1.pdf}

We see in the graph above how the bootstrap samples differ. We can also calculate the sample mean for each of these six replicates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{six\_bootstrap\_samples }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(replicate) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(price))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   replicate  mean
##       <int> <dbl>
## 1         1  154.
## 2         2  162.
## 3         3  151.
## 4         4  163.
## 5         5  158.
## 6         6  156.
\end{verbatim}

We can see that the bootstrap sample distributions and the sample means are different. This is because we are sampling with replacement. We will now calculate point estimates for our 15,000 bootstrap samples and generate a bootstrap distribution of our point estimates. The bootstrap distribution suggests how we might expect our point estimate to behave if we took another sample.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boot15000\_means }\OtherTok{\textless{}{-}}\NormalTok{ boot15000 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(replicate) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{mean =} \FunctionTok{mean}\NormalTok{(price))}
\FunctionTok{head}\NormalTok{(boot15000\_means)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   replicate  mean
##       <int> <dbl>
## 1         1  154.
## 2         2  162.
## 3         3  151.
## 4         4  163.
## 5         5  158.
## 6         6  156.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tail}\NormalTok{(boot15000\_means)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 2
##   replicate  mean
##       <int> <dbl>
## 1     14995  155.
## 2     14996  148.
## 3     14997  139.
## 4     14998  156.
## 5     14999  158.
## 6     15000  176.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{boot\_est\_dist }\OtherTok{\textless{}{-}} \FunctionTok{ggplot}\NormalTok{(boot15000\_means, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ mean)) }\SpecialCharTok{+}
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{fill =} \StringTok{"dodgerblue3"}\NormalTok{, }\AttributeTok{color =} \StringTok{"lightgrey"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\StringTok{"Sample mean price per night ($)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's compare our bootstrap distribution with the true sampling distribution (taking many samples from the population).

\begin{figure}
\includegraphics[height=0.5\textheight]{bookdown_files/figure-latex/11-bootstrapping6-1} \caption{Comparison of distribution of the bootstrap sample means and sampling distribution}\label{fig:11-bootstrapping6}
\end{figure}

There are two essential points that we can take away from these plots. First, the shape and spread of the true sampling distribution and the bootstrap distribution are similar; the bootstrap distribution lets us get a sense of the point estimate's variability. The second important point is that the means of these two distributions are different. The sampling distribution is centred at \$154.51, the population mean value. However, the bootstrap distribution is centred at the original sample's mean price per night, \$165.56. Because we are resampling from the original sample repeatedly, we see that the bootstrap distribution is centred at the original sample's mean value (unlike the sampling distribution of the sample mean, which is centred at the population parameter value).

The idea here is that we can use this distribution of bootstrap sample means to approximate the sampling distribution of the sample means when we only have one sample. Since the bootstrap distribution pretty well approximates the sampling distribution spread, we can use the bootstrap spread to help us develop a plausible range for our population parameter along with our estimate!

\begin{figure}
\centering
\includegraphics{bookdown_files/figure-latex/11-bootstrapping7-1.pdf}
\caption{\label{fig:11-bootstrapping7}Summary of bootstrapping process}
\end{figure}

\hypertarget{using-the-bootstrap-to-calculate-a-plausible-range}{%
\subsection{Using the bootstrap to calculate a plausible range}\label{using-the-bootstrap-to-calculate-a-plausible-range}}

Now that we have constructed our bootstrap distribution let's use it to create an approximate bootstrap confidence interval, a range of plausible values for the population mean. We will build a 95\% percentile bootstrap confidence interval and find the range of values that cover the middle 95\% of the bootstrap distribution. A 95\% confidence interval means that if we were to repeat the sampling process and calculate 95\% confidence intervals each time and repeat this process many times, then 95\% of the intervals would capture the population parameter's value. Note that there's nothing particularly special about 95\%, we could have used other confidence levels, such as 90\% or 99\%. There is a balance between our level of confidence and precision. A higher confidence level corresponds to a wider range of the interval, and a lower confidence level corresponds to a narrower range. Therefore the level we choose is based on what chance we are willing to take of being wrong based on the implications of being wrong for our application. In general, we choose confidence levels to be comfortable with our level of uncertainty, but not so strict that the interval is unhelpful. For instance, if our decision impacts human life and the implications of being wrong are deadly, we may want to be very confident and choose a higher confidence level.

To calculate our 95\% percentile bootstrap confidence interval, we will do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Arrange the observations in the bootstrap distribution in ascending order
\item
  Find the value such that 2.5\% of observations fall below it (the 2.5\% percentile). Use that value as the lower bound of the interval
\item
  Find the value such that 97.5\% of observations fall below it (the 97.5\% percentile). Use that value as the upper bound of the interval
\end{enumerate}

To do this in R, we can use the \texttt{quantile()} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bounds }\OtherTok{\textless{}{-}}\NormalTok{ boot15000\_means }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(mean) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{pull}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{quantile}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{0.025}\NormalTok{, }\FloatTok{0.975}\NormalTok{))}
\NormalTok{bounds}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  2.5% 97.5% 
## 134.1 200.3
\end{verbatim}

Our interval, \$134.08 to \$200.28, captures the middle 95\% of the sample mean prices in the bootstrap distribution. We can visualize the interval on our distribution in the picture below.

\begin{figure}
\includegraphics[width=0.7\linewidth]{bookdown_files/figure-latex/11-bootstrapping9-1} \caption{Distribution of the bootstrap sample means with percentile lower and upper bounds}\label{fig:11-bootstrapping9}
\end{figure}

To finish our estimation of the population parameter, we would report the point estimate and our confidence interval's lower and upper bounds. Here the sample mean price-per-night of 40 Airbnb listings was \$127.35, and we are 95\% ``confident'' that the true population mean price-per-night for all Airbnb listings in Vancouver is between \$(134.08, 200.28).

Notice that our interval does indeed contain the true
population mean value, \$154.51! However, in
practice, we would not know whether our interval captured the population parameter or not because we usually only have a single sample, not the entire population. However, this is the best we can do when we only have one sample!

This chapter is only the beginning of the journey into statistical inference. We can extend the concepts learned here to do much more than report point estimates and confidence intervals, such as hypothesis testing for differences between populations, tests for associations between variables, and so much more! We have just scratched the surface of statistical inference; however, the material presented here will serve as the foundation for more advanced statistical techniques you may learn about in the future!

\hypertarget{additional-resources-5}{%
\section{Additional resources}\label{additional-resources-5}}

For more about statistical inference and bootstrapping, refer to

\begin{itemize}
\tightlist
\item
  Chapters 7 - 8 of \href{https://moderndive.com/}{Modern Dive: Statistical
  Inference via Data Science} by Chester Ismay and Albert Y. Kim
\item
  Chapters 4 - 7 of \href{https://www.openintro.org/}{OpenIntro Statistics - Fourth Edition} by David M. Diez, Christopher D. Barr and Mine Cetinkaya-Rundel
\end{itemize}

\hypertarget{move-to-your-own-machine}{%
\chapter{Moving to your own machine}\label{move-to-your-own-machine}}

\hypertarget{overview-11}{%
\section{Overview}\label{overview-11}}

Throughout this book, we have assumed that you are working on a web-based platform
(e.g., JupyterHub) that already has Jupyter, R, a number of R packages, and Git set up and ready to use.
In this chapter, you'll learn how to install all of that software on your own computer in case
you don't have a preconfigured JupyterHub available to you.

\hypertarget{chapter-learning-objectives-11}{%
\section{Chapter learning objectives}\label{chapter-learning-objectives-11}}

By the end of the chapter, students will be able to:

\begin{itemize}
\tightlist
\item
  install Git and the miniconda Python distribution
\item
  install and launch a local instance of JupyterLab with the R kernel
\item
  download files from a JupyterHub for later local use
\end{itemize}

\hypertarget{installing-software-on-your-own-computer}{%
\section{Installing software on your own computer}\label{installing-software-on-your-own-computer}}

In this section we will provide instructions for installing the software
required by this book on our own computer.
Given that installation instructions can vary widely based on the computer setup
we have created instructions for multiple operating systems.
In particular, the installation instructions below have been verified to work
on a computer that:

\begin{itemize}
\tightlist
\item
  runs one of the following operating systems: MacOS 10.15.X (Catalina); Ubuntu 20.04; Windows 10, version 2004.
\item
  can connect to networks via a wireless connection
\item
  uses a 64-bit CPU
\item
  uses English as the default language
\end{itemize}

\textbf{For macOS users only:} Apple recently changed the default shell in the terminal to Zsh.
However, the programs we need work better with the Bash shell. Thus, we recomend you change
the default shell to Bash by opening the terminal
(\href{https://youtu.be/5AJbWEWwnbY}{how to video}) and typing:

\begin{verbatim}
chsh -s /bin/bash
\end{verbatim}

You will have to quit all instances of open terminals and then restart the
terminal for this to take effect.

\hypertarget{git}{%
\subsection{Git}\label{git}}

As shown in the version control chapter, Git is a very useful tool for version
controlling your projects, as well as sharing your work with others.

\textbf{Windows:} To install
Git on Windows go to \url{https://git-scm.com/download/win} and download the windows
version of git. Once the download has finished, run the installer and accept
the default configuration for all pages.

\textbf{MacOS:} To install Git on Mac OS open the terminal and type the following command:

\begin{verbatim}
xcode-select --install
\end{verbatim}

\textbf{Ubuntu:} To install Git on Ubuntu open the terminal and type the following commands:

\begin{verbatim}
sudo apt update
sudo apt install git
\end{verbatim}

\hypertarget{miniconda}{%
\subsection{Miniconda}\label{miniconda}}

To run Jupyter notebooks on our computers we will need to install a program
similar to the one we used as our web-based platform. One such program is
JupyterLab. But JupyterLab relies on Python; we can install this via
the \href{https://docs.conda.io/en/latest/miniconda.html}{miniconda Python package distribution}.

\textbf{Windows:} To install miniconda on Windows, download
the \href{https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe}{Python 3.8 64-bit version from here}.
Once the download has finished, run the installer and accept the default
configuration for all pages. After installation, you can open the Anaconda Prompt
by opening the Start Menu and searching for the program called
``Anaconda Prompt (miniconda3)''. When this opens you will see a prompt similar to
\texttt{(base)\ C:\textbackslash{}Users\textbackslash{}your\_name}.

\textbf{MacOS:} To install miniconda on MacOS, download
the \href{https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.pkg}{Python 3.8 64-bit version from here}.
After the download has finished, run the installer and accept the default
configuration for all pages.

\textbf{Ubuntu:} To install miniconda on Ubuntu, we first download
the \href{https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh}{Python 3.8 64-bit version from here}.
After the download has finished, open the terminal and execute the following
commands:

\begin{verbatim}
bash path/to/Miniconda3-latest-Linux-x86_64.sh
\end{verbatim}

\begin{quote}
Note: most often this file is downloaded to the Downloads directory, and thus the command will look like this:

\begin{verbatim}
bash Downloads/Miniconda3-latest-Linux-x86_64.sh
\end{verbatim}
\end{quote}

The instructions for the installation will then appear:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\tightlist
\item
  Press Enter.
\item
  Once the licence agreement shows, you can press space scroll down, or press \texttt{q} to skip reading it.
\item
  Type \texttt{yes} and press enter to accept the licence agreement.
\item
  Press enter to accept the default installation location.
\item
  Type \texttt{yes} and press enter to instruct the installer to run \texttt{conda\ init}, which makes \texttt{conda} available from the terminal/shell.
\end{enumerate}

\hypertarget{jupyterlab}{%
\subsection{JupyterLab}\label{jupyterlab}}

With miniconda set up, we can now install JupyterLab and the Jupyter Git extension.
Type the following into the Anaconda Prompt (Windows) or the terminal (MacOS and Ubuntu) and press enter:

\begin{verbatim}
conda install -c conda-forge -y jupyterlab
conda install -y nodejs=10.*
pip install --upgrade jupyterlab-git
jupyter lab build
\end{verbatim}

To test that your JupyterLab installation is functional, you can type
\texttt{jupyter\ lab} into the Anaconda Prompt (Windows) or terminal (MacOS and Ubuntu) and press enter. This should open a new
tab in your default browser with the JupyterLab interface. To exit out of
JupyterLab you can click \texttt{File\ -\textgreater{}\ Shutdown}, or go to the terminal from which
you launched JupyterLab, hold \texttt{Ctrl}, and press \texttt{c} twice.

\hypertarget{r-and-the-irkernel}{%
\subsection{R and the IRkernel}\label{r-and-the-irkernel}}

To have R available to you in JupyterLab, you will need to install the R programming language and the IRkernel.
To install these, type the following into
the Anaconda Prompt (Windows) or terminal (MacOS and Ubuntu):

\begin{verbatim}
conda install -c conda-forge -y r-base
conda install -c conda-forge -y r-irkernel
\end{verbatim}

To improve the experience of using R in JupyterLab, we will add an extension
that allows us to setup keyboard shortcuts for inserting text.
By default, this extension creates shortcuts for inserting two of the most common R
operators: \texttt{\textless{}-} and \texttt{\%\textgreater{}\%}. Type the following in the Anaconda Prompt (Windows)
or terminal (MacOS and Ubuntu) and press enter:

\begin{verbatim}
jupyter labextension install @techrah/text-shortcuts
jupyter lab build
\end{verbatim}

\hypertarget{r-packages}{%
\subsection{R packages}\label{r-packages}}

To install the packages used in this book, type the following in the Anaconda Prompt (Windows) or terminal (MacOS and Ubuntu) and press enter:

\begin{verbatim}
conda install -c conda-forge -y \
  r-cowplot \
  r-ggally \
  r-gridextra \
  r-infer \
  r-kknn \
  r-rodbc \
  r-rpostgres \
  r-rsqlite \
  r-testthat \
  r-tidymodels \
  r-tinytex \
  unixodbc
\end{verbatim}

\hypertarget{latex}{%
\subsection{LaTeX}\label{latex}}

To be able to render \texttt{.ipynb} files to \texttt{.pdf} you need to install a LaTeX
distribution. These can be quite large, so we will opt to use \texttt{tinytex}, a
light-weight cross-platform, portable, and easy-to-maintain LaTeX distribution
based on TeX Live. To install it open JupyterLab by typing \texttt{jupyter\ lab}
in the Anaconda Prompt (Windows) or terminal (MacOS and Ubuntu) and press enter.
Then from JupyterLab open an R console and type the commands listed below and
press Shift + enter to install \texttt{tinytex}:

\begin{verbatim}
tinytex::install_tinytex()
tinytex::tlmgr_install(c("eurosym", 
                         "adjustbox",
                         "caption",
                         "collectbox",
                         "enumitem",
                         "environ",
                         "fp",
                         "jknapltx",
                         "ms",
                         "oberdiek",
                         "parskip",
                         "pgf",
                         "rsfs",
                         "tcolorbox",
                         "titling",
                         "trimspaces",
                         "ucs",
                         "ulem",
                         "upquote"))
\end{verbatim}

\hypertarget{moving-files-to-your-computer}{%
\section{Moving files to your computer}\label{moving-files-to-your-computer}}

In the course that uses this textbook, students work on a web-based platform
(a JupyterHub) to do their course work. This section is to help students
save their work from this platform at the end of the course.

First in JupyterHub, open a terminal by clicking ``terminal'' in the Launcher tab.
Next, type the following in the terminal to create a
compressed \texttt{.zip} archive for the course work you are interested in downloading:

\begin{verbatim}
zip -r course_folder.zip your_course_folder
\end{verbatim}

After the compressing process is complete, right-click on \texttt{course\_folder.zip}
in the JupyterHub file browser
and click ``Download''. You should be able to use your computer's software to unzip
the compressed folder by double-clicking on it.

  \bibliography{book.bib,packages.bib,references.bib}

\backmatter
\printindex

\end{document}
